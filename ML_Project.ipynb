{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 - Decision Theory"
      ],
      "metadata": {
        "id": "QI9CD9BCoh5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One successful use of probabilistic models is for building spam filters, which take in an email and take different actions depending on the likelihood that it’s spam.\n",
        "\n",
        "Imagine you are running an email service. You have a well-calibrated spam classifier that tells you the probability that a particular email is spam: $p(spam|email)$. You have four options for what to do with each email: You can list it as important email, show it to the user, put it in the spam folder, or delete it entirely.\n",
        "\n",
        "Depending on whether or not the email really is spam, the user will suffer a different amount  of wasted time for the different actions we can take, $L$(action, spam):\n",
        "\n",
        "Action   | Spam        | Not spam\n",
        "-------- | ----------- | -----------\n",
        "Important| 15         | 0\n",
        "Show     | 5          | 1\n",
        "Folder   | 1           | 40\n",
        "Delete   | 0           | 150"
      ],
      "metadata": {
        "id": "lqGwGkzkokTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MrQFeFh6eq1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.1\n",
        "[3pts] Plot the expected wasted user time for each of the four possible actions, as a function of the probability of spam: $p(spam|email)$."
      ],
      "metadata": {
        "id": "2V_k3L8ByUs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rAwFc8cVSXtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = [[15, 0],[5, 1], [1, 40],[0, 150]]\n",
        "actions_names = ['Important','Show', 'Folder', 'Delete']\n",
        "num_actions = len(losses)\n",
        "def expected_loss_of_action(prob_spam, action):\n",
        "    #TODO: Return expected loss over a Bernoulli random variable\n",
        "    # with mean prob_spam.\n",
        "    # Losses are given by the table above.\n",
        "    \n",
        "prob_range = np.linspace(0., 1., num=600) \n",
        "\n",
        "# Make plot\n",
        "for action in range(num_actions):\n",
        "    plt.plot(prob_range, expected_loss_of_action(prob_range, action), label=actions_names[action])\n",
        "\n",
        "plt.xlabel('$p(spam|email)$')\n",
        "plt.ylabel('Expected loss of action')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "CwqF1QStyUI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.2\n",
        "[2pts] Write a function that computes the optimal action given the probability of spam."
      ],
      "metadata": {
        "id": "oqtMD5X4NZ1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_action(prob_spam):\n",
        "    #TODO: return best action given the probability of spam. \n",
        "    #Hint: np.argmin might be helpful.\n",
        "    "
      ],
      "metadata": {
        "id": "7gLS3sO1NiPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.3\n",
        "[4pts] Plot the expected loss of the optimal action as a function of the probability of spam.\n",
        "\n",
        "\n",
        "Color the line according to the optimal action for that probability of spam.\n"
      ],
      "metadata": {
        "id": "TaIhABveNinu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_range = np.linspace(0., 1., num=600) \n",
        "optimal_losses = []\n",
        "optimal_actions = []\n",
        "for p in prob_range:\n",
        "    # TODO: Compute the optimal action and its expected loss for\n",
        "    # probability of spam given by p.\n",
        "\n",
        "plt.xlabel('p(spam|email)')\n",
        "plt.ylabel('Expected loss of optimal action')\n",
        "plt.plot(prob_range, optimal_losses)"
      ],
      "metadata": {
        "id": "Gdk3OQLONo-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.4\n",
        "[4pts] For exactly which range of the probabilities of an email being spam should we delete an email?\n",
        "\n",
        "Find the exact answer by hand using algebra."
      ],
      "metadata": {
        "id": "M0eRJyGdNpXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "fr_ghgvoUz5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2 - Naïve Bayes, A Generative Model"
      ],
      "metadata": {
        "id": "H1kRdfM6ol0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true)\n",
        "\n",
        "\n",
        "In this question, we'll fit a Naïve Bayes model to the fashion MNIST dataset, and use this model for making predictions and generating new images from the same distribution. MNIST is a dataset of 28x28 black-and-white images of items of clothing. We represent each image by a vector $x^{(i)} \\in \\{0,1\\}^{784}$, where 0 and 1 represent white and black pixels respectively. Each class label $c^{(i)}$ is a different item of clothing, which in the code is represented by a 10-dimensional one-hot vector.\n",
        "\n",
        "The Naïve Bayes model parameterized by $\\theta$ and $\\pi$ defines the following joint probability of $x$ and $c$,\n",
        "$$p(x,c|\\theta,\\pi) = p(c|\\pi)p(x|c,\\theta) = p(c|\\pi)\\prod_{j=1}^{784}p(x_j|c,\\theta),$$\n",
        "where $x_j | c,\\theta \\sim \\operatorname{Bernoulli}(\\theta_{jc})$ or in other words $p(x_j | c,\\theta) = \\theta_{jc}^{x_j}(1-\\theta_{jc})^{1-x_j}$, and $c|\\pi$ follows a simple categorical distribution, i.e. $p(c|\\pi) = \\pi_c$.\n",
        "\n",
        "We begin by learning the parameters $\\theta$ and $\\pi$. The following code will download and prepare the training and test sets."
      ],
      "metadata": {
        "id": "Je6H8FAKpqmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import struct\n",
        "import array\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def download(url, filename):\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "    out_file = os.path.join('data', filename)\n",
        "    if not os.path.isfile(out_file):\n",
        "        urlretrieve(url, out_file)\n",
        "\n",
        "\n",
        "def fashion_mnist():\n",
        "    base_url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n",
        "\n",
        "    def parse_labels(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "    def parse_images(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "    for filename in ['train-images-idx3-ubyte.gz',\n",
        "                     'train-labels-idx1-ubyte.gz',\n",
        "                     't10k-images-idx3-ubyte.gz',\n",
        "                     't10k-labels-idx1-ubyte.gz']:\n",
        "        download(base_url + filename, filename)\n",
        "\n",
        "    train_images = parse_images('data/train-images-idx3-ubyte.gz')\n",
        "    train_labels = parse_labels('data/train-labels-idx1-ubyte.gz')\n",
        "    test_images = parse_images('data/t10k-images-idx3-ubyte.gz')\n",
        "    test_labels = parse_labels('data/t10k-labels-idx1-ubyte.gz')\n",
        "    # Remove the data point that cause log(0)\n",
        "    remove = (14926, 20348, 36487, 45128, 50945, 51163, 55023)\n",
        "    train_images = np.delete(train_images,remove, axis=0)\n",
        "    train_labels = np.delete(train_labels, remove, axis=0)\n",
        "    return train_images, train_labels, test_images[:1000], test_labels[:1000]\n",
        "\n",
        "\n",
        "def load_fashion_mnist():\n",
        "    partial_flatten = lambda x: np.reshape(x, (x.shape[0], np.prod(x.shape[1:])))\n",
        "    one_hot = lambda x, k: np.array(x[:, None] == np.arange(k)[None, :], dtype=int)\n",
        "    train_images, train_labels, test_images, test_labels =  fashion_mnist()\n",
        "    train_images = (partial_flatten(train_images) / 255.0 > .5).astype(float)\n",
        "    test_images = (partial_flatten(test_images) / 255.0 > .5).astype(float)\n",
        "    train_labels = one_hot(train_labels, 10)\n",
        "    test_labels = one_hot(test_labels, 10)\n",
        "    N_data = train_images.shape[0]\n",
        "\n",
        "    return N_data, train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "k587bbiSvhB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.1\n",
        "[2pts] Derive the expression for the Maximum Likelihood Estimator (MLE) of $\\theta$ and $\\pi$."
      ],
      "metadata": {
        "id": "qgGhDuEBvuMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "cISpi3BUOdEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.2\n",
        "[4pts] Using the MLE for this data, many entries of $\\theta$ will be estimated to be 0, which seems extreme. So we look for another estimation method.\n",
        "\n",
        "Assume the prior distribution of $\\theta$ is such that the entries are i.i.d. and drawn from $\\operatorname{Beta}(2,2)$. Derive the Maximum A Posteriori (MAP) estimator for $\\theta$ (it has a simple final form). You can return the MLE for $\\pi$ in your implementation. From now on, we will work with this estimator."
      ],
      "metadata": {
        "id": "gTMSP01Sw-F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "uI3hcFf1Of82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_map_estimator(train_images, train_labels):\n",
        "    \"\"\" Inputs: train_images (N_samples x N_features), train_labels (N_samples x N_classes)\n",
        "        Returns the MAP estimator theta_est (N_features x N_classes) and the MLE\n",
        "        estimator pi_est (N_classes)\"\"\"\n",
        "    \n",
        "    # YOU NEED TO WRITE THIS PART\n"
      ],
      "metadata": {
        "id": "v49Abi0uxeII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.3\n",
        "[5pts] Derive an expression for the class log-likelihood $\\log p(c|x,\\theta,\\pi)$ for a single image. Then, complete the implementation of the following functions. Recall that our prediction rule is to choose the class that maximizes the above log-likelihood, and accuracy is defined as the fraction of samples that are correctly predicted.\n",
        "\n",
        "Report the average log-likelihood $\\frac{1}{N}\\sum_{i=1}^{N}\\log p(c^{(i)}|x^{(i)},\\hat{\\theta},\\hat{\\pi})$ (where $N$ is the number of samples) on the training test, as well the training and test errors."
      ],
      "metadata": {
        "id": "7yO5yq0dyus4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "36FW8dZpOhb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(images, theta, pi):\n",
        "    \"\"\" Inputs: images (N_samples x N_features), theta, pi\n",
        "        Returns the matrix 'log_like' of loglikehoods over the input images where\n",
        "        log_like[i,c] = log p (c |x^(i), theta, pi) using the estimators theta and pi.\n",
        "        log_like is a matrix of (N_samples x N_classes)\n",
        "    Note that log likelihood is not only for c^(i), it is for all possible c's.\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        " \n",
        "\n",
        "\n",
        "def accuracy(log_like, labels):\n",
        "    \"\"\" Inputs: matrix of log likelihoods and 1-of-K labels (N_samples x N_classes)\n",
        "    Returns the accuracy based on predictions from log likelihood values\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "\n",
        "N_data, train_images, train_labels, test_images, test_labels = load_fashion_mnist()\n",
        "theta_est, pi_est = train_map_estimator(train_images, train_labels)\n",
        "\n",
        "loglike_train = log_likelihood(train_images, theta_est, pi_est)\n",
        "avg_loglike = np.sum(loglike_train * train_labels) / N_data\n",
        "train_accuracy = accuracy(loglike_train, train_labels)\n",
        "loglike_test = log_likelihood(test_images, theta_est, pi_est)\n",
        "test_accuracy = accuracy(loglike_test, test_labels)\n",
        "\n",
        "print(f\"Average log-likelihood for MAP is {avg_loglike:.3f}\")\n",
        "print(f\"Training accuracy for MAP is {train_accuracy:.3f}\")\n",
        "print(f\"Test accuracy for MAP is {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "6RZwBnVh0Zoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.4\n",
        "[2pts] Given this model's assumptions, is it always true that any two pixels $x_i$ and $x_j$ with $i \\neq j$ are indenepdent given $c$? How about after marginalizing over $c$? Explain your answer."
      ],
      "metadata": {
        "id": "qFJkXeMK2mwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your answer here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "ImntAmpWOjYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.5 \n",
        "[4pts] Since we have a generative model for our data, we can do more than just prediction. Randomly sample and plot 10 images from the learned distribution using the MAP estimates. (Hint: You first need to sample the class $c$, and then sample pixels conditioned on $c$.)"
      ],
      "metadata": {
        "id": "_P4Y1x_G28QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_sampler(theta, pi, num_images):\n",
        "    \"\"\" Inputs: parameters theta and pi, and number of images to sample\n",
        "    Returns the sampled images (N_images x N_features)\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "\n",
        "\n",
        "def plot_images(images, ims_per_row=5, padding=5, image_dimensions=(28, 28),\n",
        "                cmap=matplotlib.cm.binary, vmin=0., vmax=1.):\n",
        "    \"\"\"Images should be a (N_images x pixels) matrix.\"\"\"\n",
        "    fig = plt.figure(1)\n",
        "    fig.clf()\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    N_images = images.shape[0]\n",
        "    N_rows = np.int32(np.ceil(float(N_images) / ims_per_row))\n",
        "    pad_value = vmin\n",
        "    concat_images = np.full(((image_dimensions[0] + padding) * N_rows + padding,\n",
        "                             (image_dimensions[1] + padding) * ims_per_row + padding), pad_value)\n",
        "    for i in range(N_images):\n",
        "        cur_image = np.reshape(images[i, :], image_dimensions)\n",
        "        row_ix = i // ims_per_row\n",
        "        col_ix = i % ims_per_row\n",
        "        row_start = padding + (padding + image_dimensions[0]) * row_ix\n",
        "        col_start = padding + (padding + image_dimensions[1]) * col_ix\n",
        "        concat_images[row_start: row_start + image_dimensions[0],\n",
        "                      col_start: col_start + image_dimensions[1]] = cur_image\n",
        "        cax = ax.matshow(concat_images, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "    \n",
        "    plt.plot()\n",
        "\n",
        "\n",
        "sampled_images = image_sampler(theta_est, pi_est, 10)\n",
        "plot_images(sampled_images)"
      ],
      "metadata": {
        "id": "amV0qFMC3myy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.6\n",
        "[4pts] One of the advantages of generative models is that they can handle missing data, or be used to answer different sorts of questions about the model. Assume we have only observed some pixels of the image. Let $x_E = \\{x_p : \\text{pixel $p$ is observed}\\}$. Derive an expression for $p(x_j|x_E,\\theta,\\pi)$, the conditional probability of an unobserved pixel $j$ given the observed pixels and distribution parameters. (Hint: You have to marginalize over $c$.)"
      ],
      "metadata": {
        "id": "qCYQ6VVx5YI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "GGT2yAtLOmJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.7\n",
        "[4pts] We assume that only 30% of the pixels are observed. For the first 20 images in the training set, plot the images when the unobserved pixels are left as white, as well as the same images when the unobserved pixels are filled with the marginal probability of the pixel being 1 given the observed pixels, i.e. the value of the unobserved pixel $j$ is $p(x_j = 1|x_E,\\theta,\\pi)$."
      ],
      "metadata": {
        "id": "5qGf4Rwx6ZPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probabilistic_imputer(theta, pi, original_images, is_observed):\n",
        "    \"\"\"Inputs: parameters theta and pi, original_images (N_images x N_features), \n",
        "        and is_observed which has the same shape as original_images, with a value\n",
        "        1. in every observed entry and 0. in every unobserved entry.\n",
        "    Returns the new images where unobserved pixels are replaced by their \n",
        "    conditional probability\"\"\"\n",
        "    \n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "\n",
        "    #\n",
        "    \n",
        "num_features = train_images.shape[1]\n",
        "is_observed = np.random.binomial(1, p=0.3, size=(20, num_features))\n",
        "plot_images(train_images[:20] * is_observed)"
      ],
      "metadata": {
        "id": "k8g40hvw6pE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_images = probabilistic_imputer(theta_est, pi_est, train_images[:20], is_observed)\n",
        "plot_images(imputed_images)"
      ],
      "metadata": {
        "id": "RtOM8Ba4uVQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}