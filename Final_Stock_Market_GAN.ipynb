{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvezmosaraf/Machine-Learning-Project/blob/main/Final_Stock_Market_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrismZF1nVfA"
      },
      "source": [
        "# Google Drive Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY5SlcPumq8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9f8dce-72d4-44dd-c926-a7e5f59fbf89"
      },
      "source": [
        "#### GOOGLE DRIVE SPECIFIC ##########################\n",
        "# Make sure that you have GPU selected in the Runtime\n",
        "# If you do, will print Found GPU at: /device:GPU:0\n",
        "# Else go to Runtime -> Change Runtime Type\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # keep using v1 syntax \n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T-TJQGSyfs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e3eaca-86eb-415d-dbd8-4190c3331aa0"
      },
      "source": [
        "# CODE SNIPPET TO ACCESS THE FILES IN GOOGLE DRIVE (GO TO BROWSER AND VERIFY)\n",
        "# THEN YOU CAN ACCESS THE FILES ON LEFT SIDEBAR (copy path)\n",
        "# (https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q#scrollTo=H4SJ-tGNkOeY)\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "# drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "# !ls \"/content/drive/My Drive\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEgacIZfy5B7"
      },
      "source": [
        "# Note:\n",
        "# 1. Googlepath exists for googledrive finding the files. This is just a location\n",
        "# within google drive (a folder) that you want to collect info in.\n",
        "# Can be set to empty string if running locally. \n",
        "#\n",
        "# 2. Have companyName.csv in the googlepath that you specify. A stock_data folder\n",
        "# will be created and the stock info will be downloaded into it.\n",
        "googlepath = \"/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/\"\n",
        "ALPHA_VANTAGE_KEY = \"YFUTWYHB7TY1CD74\" # This is a backup key. Get your own from Alphavanatge so you can make API calls without blocks.\n",
        "\n",
        "# Setting the Training Amount\n",
        "TRAINING_AMOUNT = 50000 # Number of steps to train the models\n",
        "SAVE_STEPS_AMOUNT = 10000 # Frequency of which to save the models\n",
        "PCT_CHANGE_AMOUNT = 5 # Percent Change Amount\n",
        "HISTORICAL_DAYS_AMOUNT = 20 # Window to use for historical dayss\n",
        "DAYS_AHEAD = 5 # How many days ahead to look"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu5MzfoPhgKv"
      },
      "source": [
        "# Getting the Data\n",
        "Currently set to just download stock on first row of companylist.csv (aka AAPL) and saves it to the stock_data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aIFfn9cfLyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5e8aa4-a76e-4d3d-e7ec-9c0e435d79f3"
      },
      "source": [
        "'''\n",
        "Downloads stock data from alphavantage\n",
        "'''\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import urllib\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "import io\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "#Note should have companylist.csv in the directory with this file.\n",
        "\n",
        "# Add stock_data folder\n",
        "if not os.path.exists(f'{googlepath}stock_data'):\n",
        "    os.makedirs(f'{googlepath}stock_data')\n",
        "\n",
        "'''\n",
        "Saves data to a file\n",
        "'''\n",
        "def save(googlepath, stock_csv, output_dir, filename):\n",
        "    filepath = os.path.join(googlepath, output_dir, filename)\n",
        "    try:\n",
        "        df = stock_csv\n",
        "        df.to_csv(filepath, index=False)\n",
        "    except Exception as ex:\n",
        "        print('Could not open file {} to write data'.format(filepath))\n",
        "        print(ex)\n",
        "\n",
        "\n",
        "def try_download(symbol):\n",
        "    try:\n",
        "        # Keep call frequency below threshold \n",
        "        time.sleep(15) # (5 per minute allowed for free service)\n",
        "        url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&apikey={}&datatype=csv&outputsize=full'.format(symbol, ALPHA_VANTAGE_KEY)\n",
        "        df = pd.read_csv(url)\n",
        "        df = df.drop(['split_coefficient', 'dividend_amount', 'adjusted_close'], axis=1)\n",
        "        return df, True\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "#Given a stock symbol (aka 'tsla') will download and save the data to the\n",
        "#output dir as a csv \n",
        "def download_symbol(symbol, output_dir, retry_count=4):\n",
        "\n",
        "    stock_csv, didPass = try_download(symbol)\n",
        "    if didPass:\n",
        "        save(googlepath, stock_csv, output_dir, '{}.csv'.format(symbol))\n",
        "    else:\n",
        "        print('Failed to download {}'.format(symbol))\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/companylist.csv\")\n",
        "for symbol in df.Symbol:\n",
        "    my_file = Path(f\"{googlepath}stock_data/{symbol}.csv\")  # check if already downloaded\n",
        "    if not my_file.exists():\n",
        "        print('Downloading {}'.format(symbol))\n",
        "        download_symbol(symbol, 'stock_data')\n",
        "    else:\n",
        "        print(f\"Already downloaded {symbol}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already downloaded MMM\n",
            "Already downloaded AOS\n",
            "Already downloaded ABT\n",
            "Already downloaded ABBV\n",
            "Already downloaded ACN\n",
            "Already downloaded ATVI\n",
            "Already downloaded AYI\n",
            "Already downloaded ADBE\n",
            "Already downloaded AAP\n",
            "Already downloaded AMD\n",
            "Already downloaded AES\n",
            "Already downloaded AET\n",
            "Already downloaded AMG\n",
            "Already downloaded AFL\n",
            "Already downloaded A\n",
            "Already downloaded APD\n",
            "Already downloaded AKAM\n",
            "Already downloaded ALK\n",
            "Already downloaded ALB\n",
            "Already downloaded ARE\n",
            "Already downloaded ALXN\n",
            "Already downloaded ALGN\n",
            "Already downloaded ALLE\n",
            "Downloading AGN\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download AGN\n",
            "Downloading ADS\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download ADS\n",
            "Already downloaded LNT\n",
            "Already downloaded ALL\n",
            "Already downloaded GOOGL\n",
            "Already downloaded GOOG\n",
            "Already downloaded MO\n",
            "Already downloaded AMZN\n",
            "Already downloaded AEE\n",
            "Already downloaded AAL\n",
            "Already downloaded AEP\n",
            "Already downloaded AXP\n",
            "Already downloaded AIG\n",
            "Already downloaded AMT\n",
            "Already downloaded AWK\n",
            "Already downloaded AMP\n",
            "Already downloaded ABC\n",
            "Already downloaded AME\n",
            "Already downloaded AMGN\n",
            "Already downloaded APH\n",
            "Downloading APC\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download APC\n",
            "Already downloaded ADI\n",
            "Already downloaded ANDV\n",
            "Already downloaded ANSS\n",
            "Downloading ANTM\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download ANTM\n",
            "Already downloaded AON\n",
            "Already downloaded APA\n",
            "Already downloaded AIV\n",
            "Already downloaded AAPL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6f-Ie82y2yf"
      },
      "source": [
        "# Helper Functions to Plot: Define the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4c-ooOFw1OJ"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXdCIHYjwcs_"
      },
      "source": [
        "# Initializing the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixb_ot3CwcN2"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "\n",
        "class GAN():\n",
        "\n",
        "    def sample_Z(self, batch_size, n):\n",
        "        return np.random.uniform(-1., 1., size=(batch_size, n))\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, generator_input_size=200, is_train=True):\n",
        "        def get_batch_norm_with_global_normalization_vars(size):\n",
        "            v = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            m = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            beta = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            gamma = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            return v, m, beta, gamma\n",
        "\n",
        "        self.X = tf.compat.v1.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Z = tf.compat.v1.placeholder(tf.float32, shape=[None, generator_input_size])\n",
        "\n",
        "        generator_output_size = num_features*num_historical_days\n",
        "        with tf.variable_scope(\"generator\"):\n",
        "            W1 = tf.Variable(tf.truncated_normal([generator_input_size, generator_output_size*10]))\n",
        "            b1 = tf.Variable(tf.truncated_normal([generator_output_size*10]))\n",
        "\n",
        "            h1 = tf.nn.sigmoid(tf.matmul(self.Z, W1) + b1)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([generator_output_size*10, generator_output_size*5]))\n",
        "            b2 = tf.Variable(tf.truncated_normal([generator_output_size*5]))\n",
        "\n",
        "            h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)\n",
        "\n",
        "            W3 = tf.Variable(tf.truncated_normal([generator_output_size*5, generator_output_size]))\n",
        "            b3 = tf.Variable(tf.truncated_normal([generator_output_size]))\n",
        "\n",
        "            g_log_prob = tf.matmul(h2, W3) + b3\n",
        "            g_log_prob = tf.reshape(g_log_prob, [-1, num_historical_days, 1, num_features])\n",
        "            self.gen_data = tf.reshape(g_log_prob, [-1, num_historical_days, num_features])\n",
        "\n",
        "            theta_G = [W1, b1, W2, b2, W3, b3]\n",
        "\n",
        "\n",
        "\n",
        "        with tf.variable_scope(\"discriminator\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "\n",
        "            v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(32)\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "\n",
        "            v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(64)\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 64, 128],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([128], dtype=tf.float32))\n",
        "\n",
        "            v3, m3, beta3, gamma3 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*128, 128]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([128]))\n",
        "\n",
        "            v4, m4, beta4, gamma4 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([128, 1]))\n",
        "\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]\n",
        "\n",
        "        def discriminator(X):\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            pool = relu\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            pool = relu\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=0.8)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=0.8)\n",
        "\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "            D_logit = tf.matmul(h1, W2)\n",
        "            D_prob = tf.nn.sigmoid(D_logit)\n",
        "            return D_prob, D_logit, features\n",
        "\n",
        "        D_real, D_logit_real, self.features = discriminator(X)\n",
        "        D_fake, D_logit_fake, _ = discriminator(g_log_prob)\n",
        "\n",
        "\n",
        "        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
        "        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
        "        self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n",
        "        self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n",
        "        self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n",
        "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n",
        "\n",
        "\n",
        "        self.D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.D_loss, var_list=theta_D)\n",
        "        self.G_solver = tf.train.AdamOptimizer(learning_rate=0.000055).minimize(self.G_loss, var_list=theta_G)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31leQzt9wmOJ"
      },
      "source": [
        "# Training the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N3ZkCzmwmCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "091d4339-0749-4718-cd90-b3b997a78d29"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "random.seed(42)\n",
        "class TrainGan:\n",
        "\n",
        "    def __init__(self, num_historical_days, batch_size=128):\n",
        "        self.batch_size = batch_size\n",
        "        self.data = []\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n",
        "      \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            #Read in file -- note that parse_dates will be need later\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df[['open','high','low','close','volume']]\n",
        "\n",
        "            #Normilize using a of size num_historical_days\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "\n",
        "            #Drop days that we don't have data for\n",
        "            df = df.dropna()\n",
        "            \n",
        "            #Hold out 500 days for testing\n",
        "            df = df[500:]\n",
        "\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                self.data.append(df.values[i-num_historical_days:i])\n",
        "\n",
        "        self.gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200)\n",
        "\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        while True:\n",
        "            batch.append(random.choice(self.data))\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "    def train(self, print_steps=100, display_data=100, save_steps=SAVE_STEPS_AMOUNT):\n",
        "        if not os.path.exists(f'{googlepath}models'):\n",
        "            os.makedirs(f'{googlepath}models')\n",
        "        sess = tf.Session()\n",
        "        \n",
        "        G_loss = 0\n",
        "        D_loss = 0\n",
        "        G_l2_loss = 0\n",
        "        D_l2_loss = 0\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        g_loss_array = []\n",
        "        d_loss_array = []\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}models/checkpoint'):\n",
        "                with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, X in enumerate(self.random_batch(self.batch_size)):\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "            if i % 1 == 0:\n",
        "                _, D_loss_curr, D_l2_loss_curr = sess.run([self.gan.D_solver, self.gan.D_loss, self.gan.D_l2_loss], feed_dict=\n",
        "                        {self.gan.X:X, self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                D_loss += D_loss_curr\n",
        "                D_l2_loss += D_l2_loss_curr\n",
        "            if i % 1 == 0:\n",
        "                _, G_loss_curr, G_l2_loss_curr = sess.run([self.gan.G_solver, self.gan.G_loss, self.gan.G_l2_loss],\n",
        "                        feed_dict={self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                G_loss += G_loss_curr\n",
        "                G_l2_loss += G_l2_loss_curr\n",
        "                \n",
        "            g_loss_array.append(G_loss_curr - G_l2_loss)\n",
        "            d_loss_array.append(D_loss_curr - D_l2_loss)\n",
        "            \n",
        "            \n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} D_loss={}, G_loss={}'.format(i + int(currentStep), D_loss/print_steps - D_l2_loss/print_steps, G_loss/print_steps - G_l2_loss/print_steps))\n",
        "                G_loss = 0\n",
        "                D_loss = 0\n",
        "                G_l2_loss = 0\n",
        "                D_l2_loss = 0\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess, f'{googlepath}/models/gan.ckpt', i + int(currentStep))\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if ((i + int(currentStep)) > TRAINING_AMOUNT):\n",
        "                \n",
        "                print(\"Reached {} epochs for GAN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                \n",
        "                axisX = np.arange(0,len(g_loss_array),1)\n",
        "                plt.plot(axisX, g_loss_array, label='generator loss')\n",
        "                plt.plot(axisX, d_loss_array, label='discriminator loss')\n",
        "                plt.legend()\n",
        "                plt.title('generator and discriminator loss')\n",
        "                plt.show()\n",
        "                \n",
        "                break\n",
        "\n",
        "            # if (i+1) % display_data == 0:\n",
        "            #     print('Generated Data')\n",
        "            #     print(sess.run(self.gan.gen_data, feed_dict={self.gan.Z:self.gan.sample_Z(1, 200)}))\n",
        "            #     print('Real Data')\n",
        "            #     print(X[0])\n",
        "\n",
        "\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "gan = TrainGan(HISTORICAL_DAYS_AMOUNT, 128)\n",
        "gan.train()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/MMM.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AOS.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABBV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ACN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ATVI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AYI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ADBE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMD.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AES.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AET.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AFL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/A.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APD.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AKAM.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALK.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALB.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ARE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALXN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALLE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/LNT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/GOOGL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/GOOG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/MO.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMZN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AEE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AEP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AXP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AIG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AWK.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABC.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AME.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APH.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ADI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ANDV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ANSS.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AON.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APA.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AIV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAPL.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-33e3bb907886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainGan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHISTORICAL_DAYS_AMOUNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-33e3bb907886>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, print_steps, display_data, save_steps)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}models/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgooglepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mcurrentStep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mnew_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}.meta'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mnew_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[0;32m-> 1582\u001b[0;31m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0m\u001b[1;32m   1583\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                                                  **kwargs)[0]\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1597\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1599\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File does not exist. Received: {filename}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File does not exist. Received: /content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/models//content/drive/MyDrive/M.Tech Project/Code/23rdDec2022//models/gan.ckpt-49999.meta."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL6fVkWpx0ok"
      },
      "source": [
        "# Initializing the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV1Jr8-x0on"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "\n",
        "class CNN():\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, is_train=True):\n",
        "      \n",
        "        self.X = tf.compat.v1.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Y = tf.compat.v1.placeholder(tf.int32, shape=[None, 2])\n",
        "        self.keep_prob = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
        "\n",
        "        with tf.variable_scope(\"cnn\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 16],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([16], dtype=tf.float32))\n",
        "\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 16, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "            conv = tf.nn.conv2d(relu, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "            conv = tf.nn.conv2d(relu, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            print(flattened_convolution_size)\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=self.keep_prob)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*64, 32]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([32]))\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([32, 2]))\n",
        "            logits = tf.matmul(h1, W2)\n",
        "\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.Y, 1), tf.argmax(logits, 1)), tf.float32))\n",
        "            self.confusion_matrix = tf.confusion_matrix(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\n",
        "            tf.summary.scalar('accuracy', self.accuracy)\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]           \n",
        "\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=logits))\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.loss)\n",
        "        self.summary = tf.summary.merge_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lqp4vhRx7pz"
      },
      "source": [
        "## Training the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2ZFkj9Gx7cX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ea064cd-394c-4554-db13-d8fdf235a443"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import xgboost as xgb\n",
        "#from sklearn.externals import joblib\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "class TrainCNN:\n",
        "\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "        self.cnn = CNN(num_features=5, num_historical_days=num_historical_days, is_train=False)\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n",
        "    \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df[['open','high','low','close','volume']]\n",
        "            # data for new column labels that will use the pct_change of the closing data.\n",
        "            # pct_change measure change between current and prior element. Map these into a 1x2\n",
        "            # array to show if the pct_change > (our desired threshold) or less than.\n",
        "            labels = df.close.pct_change(days).map(lambda x: [int(x > pct_change/100.0), int(x <= pct_change/100.0)])\n",
        "            \n",
        "            # rolling normalization. (df - df.mean) / (df.max - df.min)\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "            df['labels'] = labels\n",
        "\n",
        "            # doing pct_change will give some rows (like first row) a NaN value. Drop that.\n",
        "            df = df.dropna()\n",
        "\n",
        "            # Do the testing data split\n",
        "            test_df = df[:500]\n",
        "            df = df[500:]\n",
        "\n",
        "            # get the predictors of the dataframe\n",
        "            data = df[['open','high','low','close','volume']].values\n",
        "\n",
        "            # the response value\n",
        "            labels = df['labels'].values\n",
        "\n",
        "            # start at num_historical_days and iterate the full length of the training\n",
        "            # data at intervals of num_historical_days\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                # split the df into arrays of length num_historical_days and append\n",
        "                # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\n",
        "                self.data.append(data[i-num_historical_days:i])\n",
        "\n",
        "                # appending if price went up or down in curr day of \"i\" we are looking\n",
        "                # at\n",
        "                self.labels.append(labels[i-1])\n",
        "            \n",
        "            # do same for test data\n",
        "            data = test_df[['open','high','low','close','volume']].values\n",
        "            labels = test_df['labels'].values\n",
        "            for i in range(num_historical_days, len(test_df), 1):\n",
        "                self.test_data.append(data[i-num_historical_days:i])\n",
        "                self.test_labels.append(labels[i-1])\n",
        "\n",
        "    # a function to get a random_batch of data.\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        labels = []\n",
        "        # zip concatenates each array index of both arrays together\n",
        "        data = list(zip(self.data, self.labels))\n",
        "        i = 0\n",
        "        while True:\n",
        "            i+= 1\n",
        "            while True:\n",
        "                # pick a random array, i.e. range of days, from data\n",
        "                d = random.choice(data)\n",
        "                # balance the data with equal number of positive pct_change\n",
        "                # and negative pct_change\n",
        "                if(d[1][0]== int(i%2)):\n",
        "                    break\n",
        "            batch.append(d[0])  # append the range of days we got to batch\n",
        "            labels.append(d[1])  # append the label of that range of data we got\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch, labels\n",
        "                batch = []\n",
        "                labels = []\n",
        "\n",
        "    def train(self, print_steps=100, display_steps=100, save_steps=SAVE_STEPS_AMOUNT, batch_size=128, keep_prob=0.6):\n",
        "        if not os.path.exists(f'{googlepath}cnn_models'):\n",
        "            os.makedirs(f'{googlepath}cnn_models')\n",
        "        if not os.path.exists(f'{googlepath}logs'):\n",
        "            os.makedirs(f'{googlepath}logs')\n",
        "        if os.path.exists(f'{googlepath}logs/train'):\n",
        "            for file in [os.path.join(f'{googlepath}logs/train/', f) for f in os.listdir(f'{googlepath}logs/train/')]:\n",
        "                os.remove(file)\n",
        "        if os.path.exists(f'{googlepath}logs/test'):\n",
        "            for file in [os.path.join(f'{googlepath}logs/test/', f) for f in os.listdir(f'{googlepath}logs/test')]:\n",
        "                os.remove(file)\n",
        "\n",
        "        sess = tf.Session()\n",
        "        loss = 0\n",
        "        l2_loss = 0\n",
        "        accuracy = 0\n",
        "        saver = tf.train.Saver()\n",
        "        train_writer = tf.summary.FileWriter(f'{googlepath}/logs/train')\n",
        "        test_writer = tf.summary.FileWriter(f'{googlepath}/logs/test')\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        test_loss_array = []\n",
        "        test_accuracy_array = []\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}cnn_models/checkpoint'):\n",
        "                with open(f'{googlepath}cnn_models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}cnn_models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, [X, y] in enumerate(self.random_batch(batch_size)):\n",
        "\n",
        "          \n",
        "            _, loss_curr, accuracy_curr = sess.run([self.cnn.optimizer, self.cnn.loss, self.cnn.accuracy], feed_dict=\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n",
        "            loss += loss_curr\n",
        "            accuracy += accuracy_curr\n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} loss={}, accuracy={}'.format(i + int(currentStep), loss/print_steps, accuracy/print_steps))\n",
        "                loss = 0\n",
        "                l2_loss = 0\n",
        "                accuracy = 0\n",
        "                test_loss, test_accuracy, confusion_matrix = sess.run([self.cnn.loss, self.cnn.accuracy, self.cnn.confusion_matrix], feed_dict={self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n",
        "                test_loss_array.append(test_loss)\n",
        "                test_accuracy_array.append(test_accuracy)\n",
        "                print(\"Test loss = {}, Test accuracy = {}\".format(test_loss, test_accuracy))\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess,  f'{googlepath}cnn_models/cnn.ckpt', i)\n",
        "\n",
        "            if (i+1) % display_steps == 0:\n",
        "                summary = sess.run(self.cnn.summary, feed_dict=\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n",
        "                train_writer.add_summary(summary, i)\n",
        "                summary = sess.run(self.cnn.summary, feed_dict={\n",
        "                    self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n",
        "                test_writer.add_summary(summary, i)\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if (i + int(currentStep)) > TRAINING_AMOUNT:\n",
        "                print(\"Reached {} epochs for CNN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                print(confusion_matrix)\n",
        "                plot_confusion_matrix(confusion_matrix, ['Down', 'Up'], normalize=True, title=\"CNN Confusion Matrix\")\n",
        "                \n",
        "                axisX = np.arange(0,len(test_loss_array),1)\n",
        "                plt.plot(axisX, test_loss_array, label='test loss')\n",
        "                plt.plot(axisX, test_accuracy_array, label='test accuracy')\n",
        "                plt.legend()\n",
        "                plt.title('test loss and accuracy')\n",
        "                plt.show()\n",
        "\n",
        "                break\n",
        "\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "cnn = TrainCNN(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\n",
        "cnn.train()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"cnn/Relu:0\", shape=(?, 20, 1, 16), dtype=float32)\n",
            "Tensor(\"cnn/Relu_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"cnn/Relu_2:0\", shape=(?, 18, 1, 64), dtype=float32)\n",
            "1152\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/MMM.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AOS.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABBV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ACN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ATVI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AYI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ADBE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMD.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AES.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AET.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AFL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/A.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APD.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AKAM.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALK.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALB.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ARE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALXN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALLE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/LNT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/GOOGL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/GOOG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/MO.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMZN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AEE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AEP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AXP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AIG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AWK.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABC.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AME.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APH.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ADI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ANDV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ANSS.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AON.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APA.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AIV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAPL.csv\n",
            "Step=99 loss=0.7578561636805534, accuracy=0.674609375\n",
            "Test loss = 0.5335126519203186, Test accuracy = 0.7516493201255798\n",
            "Step=199 loss=0.42605629831552505, accuracy=0.80015625\n",
            "Test loss = 0.4289622902870178, Test accuracy = 0.8016927242279053\n",
            "Step=299 loss=0.37789449632167815, accuracy=0.823984375\n",
            "Test loss = 0.3954828083515167, Test accuracy = 0.8177083134651184\n",
            "Step=399 loss=0.3308290076255798, accuracy=0.851484375\n",
            "Test loss = 0.40169334411621094, Test accuracy = 0.8158420324325562\n",
            "Step=499 loss=0.31922841757535936, accuracy=0.858984375\n",
            "Test loss = 0.35468167066574097, Test accuracy = 0.8403645753860474\n",
            "Step=599 loss=0.3069778724014759, accuracy=0.86515625\n",
            "Test loss = 0.3600696921348572, Test accuracy = 0.8384982347488403\n",
            "Step=699 loss=0.29511707812547683, accuracy=0.87171875\n",
            "Test loss = 0.3457561433315277, Test accuracy = 0.8458333611488342\n",
            "Step=799 loss=0.2864453490078449, accuracy=0.87734375\n",
            "Test loss = 0.3599240481853485, Test accuracy = 0.8399739861488342\n",
            "Step=899 loss=0.2830862468481064, accuracy=0.880703125\n",
            "Test loss = 0.3317723870277405, Test accuracy = 0.8521701097488403\n",
            "Step=999 loss=0.2659665659070015, accuracy=0.888046875\n",
            "Test loss = 0.329182893037796, Test accuracy = 0.8546441197395325\n",
            "Step=1099 loss=0.27115474686026575, accuracy=0.889375\n",
            "Test loss = 0.3055155575275421, Test accuracy = 0.8636718988418579\n",
            "Step=1199 loss=0.26639442533254626, accuracy=0.890078125\n",
            "Test loss = 0.33490559458732605, Test accuracy = 0.8512153029441833\n",
            "Step=1299 loss=0.26618474423885347, accuracy=0.890625\n",
            "Test loss = 0.3141959011554718, Test accuracy = 0.8595920205116272\n",
            "Step=1399 loss=0.2568631732463837, accuracy=0.89734375\n",
            "Test loss = 0.3252260088920593, Test accuracy = 0.8541666865348816\n",
            "Step=1499 loss=0.25043305069208144, accuracy=0.896640625\n",
            "Test loss = 0.33878329396247864, Test accuracy = 0.8501735925674438\n",
            "Step=1599 loss=0.2526818335056305, accuracy=0.900703125\n",
            "Test loss = 0.2993157207965851, Test accuracy = 0.8664930462837219\n",
            "Step=1699 loss=0.24354360446333886, accuracy=0.9015625\n",
            "Test loss = 0.32000744342803955, Test accuracy = 0.8594183921813965\n",
            "Step=1799 loss=0.2395111219584942, accuracy=0.90359375\n",
            "Test loss = 0.32305994629859924, Test accuracy = 0.8583767414093018\n",
            "Step=1899 loss=0.23445616863667965, accuracy=0.908984375\n",
            "Test loss = 0.2912091910839081, Test accuracy = 0.8711371421813965\n",
            "Step=1999 loss=0.22789417415857316, accuracy=0.913203125\n",
            "Test loss = 0.3159841299057007, Test accuracy = 0.8620659708976746\n",
            "Step=2099 loss=0.2271753704547882, accuracy=0.91234375\n",
            "Test loss = 0.3348865807056427, Test accuracy = 0.8542534708976746\n",
            "Step=2199 loss=0.22265011474490165, accuracy=0.915078125\n",
            "Test loss = 0.2886718809604645, Test accuracy = 0.8721787929534912\n",
            "Step=2299 loss=0.21991876006126404, accuracy=0.91671875\n",
            "Test loss = 0.30709308385849, Test accuracy = 0.8661024570465088\n",
            "Step=2399 loss=0.21656184136867523, accuracy=0.92\n",
            "Test loss = 0.30467626452445984, Test accuracy = 0.8654513955116272\n",
            "Step=2499 loss=0.2074089901149273, accuracy=0.923671875\n",
            "Test loss = 0.3066156804561615, Test accuracy = 0.8655815720558167\n",
            "Step=2599 loss=0.212065649330616, accuracy=0.92046875\n",
            "Test loss = 0.3322964906692505, Test accuracy = 0.8573784828186035\n",
            "Step=2699 loss=0.20936593309044838, accuracy=0.921796875\n",
            "Test loss = 0.28992125391960144, Test accuracy = 0.8717882037162781\n",
            "Step=2799 loss=0.1974481725692749, accuracy=0.927421875\n",
            "Test loss = 0.2992304265499115, Test accuracy = 0.8694444298744202\n",
            "Step=2899 loss=0.20246468380093574, accuracy=0.928984375\n",
            "Test loss = 0.2921885550022125, Test accuracy = 0.8716145753860474\n",
            "Step=2999 loss=0.1998268010467291, accuracy=0.927890625\n",
            "Test loss = 0.29609960317611694, Test accuracy = 0.8693576455116272\n",
            "Step=3099 loss=0.1948680854588747, accuracy=0.93125\n",
            "Test loss = 0.2800987660884857, Test accuracy = 0.8760416507720947\n",
            "Step=3199 loss=0.1974914425611496, accuracy=0.930390625\n",
            "Test loss = 0.2930058240890503, Test accuracy = 0.8719183802604675\n",
            "Step=3299 loss=0.18779939018189906, accuracy=0.936484375\n",
            "Test loss = 0.29535025358200073, Test accuracy = 0.8720920085906982\n",
            "Step=3399 loss=0.18404638037085533, accuracy=0.934296875\n",
            "Test loss = 0.29012441635131836, Test accuracy = 0.8751736283302307\n",
            "Step=3499 loss=0.18379369512200355, accuracy=0.93828125\n",
            "Test loss = 0.2773609459400177, Test accuracy = 0.878819465637207\n",
            "Step=3599 loss=0.17799557596445084, accuracy=0.939296875\n",
            "Test loss = 0.2811872363090515, Test accuracy = 0.8781684041023254\n",
            "Step=3699 loss=0.18050075754523276, accuracy=0.93671875\n",
            "Test loss = 0.26400890946388245, Test accuracy = 0.8845919966697693\n",
            "Step=3799 loss=0.18186419360339642, accuracy=0.9359375\n",
            "Test loss = 0.2726682126522064, Test accuracy = 0.8824218511581421\n",
            "Step=3899 loss=0.17288909263908864, accuracy=0.94328125\n",
            "Test loss = 0.29458746314048767, Test accuracy = 0.8730902671813965\n",
            "Step=3999 loss=0.17309427656233312, accuracy=0.942421875\n",
            "Test loss = 0.2603679299354553, Test accuracy = 0.8861545324325562\n",
            "Step=4099 loss=0.16538278445601462, accuracy=0.9475\n",
            "Test loss = 0.2688552141189575, Test accuracy = 0.8842881917953491\n",
            "Step=4199 loss=0.1720602325350046, accuracy=0.943515625\n",
            "Test loss = 0.2654728889465332, Test accuracy = 0.8844618201255798\n",
            "Step=4299 loss=0.1685545701533556, accuracy=0.9465625\n",
            "Test loss = 0.27969008684158325, Test accuracy = 0.880772590637207\n",
            "Step=4399 loss=0.16435173839330675, accuracy=0.947890625\n",
            "Test loss = 0.28826090693473816, Test accuracy = 0.8775173425674438\n",
            "Step=4499 loss=0.16396578185260297, accuracy=0.950078125\n",
            "Test loss = 0.3096220791339874, Test accuracy = 0.8707465529441833\n",
            "Step=4599 loss=0.16429960377514363, accuracy=0.949765625\n",
            "Test loss = 0.2772359549999237, Test accuracy = 0.8829861283302307\n",
            "Step=4699 loss=0.15524689860641958, accuracy=0.950703125\n",
            "Test loss = 0.2891252338886261, Test accuracy = 0.8796007037162781\n",
            "Step=4799 loss=0.15556834682822226, accuracy=0.953203125\n",
            "Test loss = 0.2694648206233978, Test accuracy = 0.88671875\n",
            "Step=4899 loss=0.15589927673339843, accuracy=0.95265625\n",
            "Test loss = 0.2647341787815094, Test accuracy = 0.8876302242279053\n",
            "Step=4999 loss=0.1571268005669117, accuracy=0.951484375\n",
            "Test loss = 0.2779015004634857, Test accuracy = 0.8825086951255798\n",
            "Step=5099 loss=0.14974960923194885, accuracy=0.955234375\n",
            "Test loss = 0.2572980523109436, Test accuracy = 0.8929687738418579\n",
            "Step=5199 loss=0.15219583481550217, accuracy=0.95484375\n",
            "Test loss = 0.27794355154037476, Test accuracy = 0.8836371302604675\n",
            "Step=5299 loss=0.1479874962568283, accuracy=0.9571875\n",
            "Test loss = 0.2667907774448395, Test accuracy = 0.8896267414093018\n",
            "Step=5399 loss=0.1426078600436449, accuracy=0.958515625\n",
            "Test loss = 0.2897131145000458, Test accuracy = 0.8807291388511658\n",
            "Step=5499 loss=0.1412239669263363, accuracy=0.958046875\n",
            "Test loss = 0.2651803493499756, Test accuracy = 0.8904079794883728\n",
            "Step=5599 loss=0.13819207593798638, accuracy=0.959453125\n",
            "Test loss = 0.27304890751838684, Test accuracy = 0.8865451216697693\n",
            "Step=5699 loss=0.13711640924215318, accuracy=0.958203125\n",
            "Test loss = 0.24860581755638123, Test accuracy = 0.8958333134651184\n",
            "Step=5799 loss=0.14033616967499257, accuracy=0.9590625\n",
            "Test loss = 0.23882947862148285, Test accuracy = 0.8999131917953491\n",
            "Step=5899 loss=0.13822231471538543, accuracy=0.960703125\n",
            "Test loss = 0.27077415585517883, Test accuracy = 0.8881510496139526\n",
            "Step=5999 loss=0.13187394957989454, accuracy=0.9628125\n",
            "Test loss = 0.2699619233608246, Test accuracy = 0.8880208134651184\n",
            "Step=6099 loss=0.12635394632816316, accuracy=0.963828125\n",
            "Test loss = 0.2509812116622925, Test accuracy = 0.8953993320465088\n",
            "Step=6199 loss=0.13162392050027846, accuracy=0.9609375\n",
            "Test loss = 0.26909032464027405, Test accuracy = 0.8903211951255798\n",
            "Step=6299 loss=0.13404583353549243, accuracy=0.961953125\n",
            "Test loss = 0.26934072375297546, Test accuracy = 0.8901475667953491\n",
            "Step=6399 loss=0.1251700746640563, accuracy=0.965\n",
            "Test loss = 0.26236778497695923, Test accuracy = 0.8935763835906982\n",
            "Step=6499 loss=0.1295567563921213, accuracy=0.9628125\n",
            "Test loss = 0.26661455631256104, Test accuracy = 0.8929253220558167\n",
            "Step=6599 loss=0.11499443531036377, accuracy=0.9678125\n",
            "Test loss = 0.29145294427871704, Test accuracy = 0.8834635615348816\n",
            "Step=6699 loss=0.11615334238857031, accuracy=0.968515625\n",
            "Test loss = 0.25777000188827515, Test accuracy = 0.8959635496139526\n",
            "Step=6799 loss=0.1182278485968709, accuracy=0.967109375\n",
            "Test loss = 0.26992374658584595, Test accuracy = 0.8932291865348816\n",
            "Step=6899 loss=0.12189942233264446, accuracy=0.96484375\n",
            "Test loss = 0.24524100124835968, Test accuracy = 0.9014322757720947\n",
            "Step=6999 loss=0.11692599914968013, accuracy=0.969140625\n",
            "Test loss = 0.25100573897361755, Test accuracy = 0.8991753458976746\n",
            "Step=7099 loss=0.10954753823578357, accuracy=0.969921875\n",
            "Test loss = 0.27710607647895813, Test accuracy = 0.8901475667953491\n",
            "Step=7199 loss=0.11650154452770949, accuracy=0.96796875\n",
            "Test loss = 0.27032172679901123, Test accuracy = 0.8934462070465088\n",
            "Step=7299 loss=0.1149314222484827, accuracy=0.96859375\n",
            "Test loss = 0.2452153116464615, Test accuracy = 0.9015191197395325\n",
            "Step=7399 loss=0.11471374206244946, accuracy=0.9678125\n",
            "Test loss = 0.2539333403110504, Test accuracy = 0.899609386920929\n",
            "Step=7499 loss=0.10795920159667731, accuracy=0.969921875\n",
            "Test loss = 0.2571132481098175, Test accuracy = 0.8987413048744202\n",
            "Step=7599 loss=0.10456954229623079, accuracy=0.97078125\n",
            "Test loss = 0.25746577978134155, Test accuracy = 0.898350715637207\n",
            "Step=7699 loss=0.11022616006433963, accuracy=0.969453125\n",
            "Test loss = 0.25709259510040283, Test accuracy = 0.8995659947395325\n",
            "Step=7799 loss=0.10787996541708708, accuracy=0.97\n",
            "Test loss = 0.25752270221710205, Test accuracy = 0.8991753458976746\n",
            "Step=7899 loss=0.10240560408681632, accuracy=0.972421875\n",
            "Test loss = 0.25178173184394836, Test accuracy = 0.9029513597488403\n",
            "Step=7999 loss=0.10301620360463858, accuracy=0.972421875\n",
            "Test loss = 0.2789955139160156, Test accuracy = 0.8933159708976746\n",
            "Step=8099 loss=0.10052328240126371, accuracy=0.97296875\n",
            "Test loss = 0.27618691325187683, Test accuracy = 0.8944878578186035\n",
            "Step=8199 loss=0.10008037764579057, accuracy=0.973359375\n",
            "Test loss = 0.2639188766479492, Test accuracy = 0.8990017175674438\n",
            "Step=8299 loss=0.09654862232506276, accuracy=0.974453125\n",
            "Test loss = 0.26083406805992126, Test accuracy = 0.9003472328186035\n",
            "Step=8399 loss=0.0981499083340168, accuracy=0.973046875\n",
            "Test loss = 0.28825366497039795, Test accuracy = 0.8901909589767456\n",
            "Step=8499 loss=0.09268745001405478, accuracy=0.973828125\n",
            "Test loss = 0.2544184923171997, Test accuracy = 0.903124988079071\n",
            "Step=8599 loss=0.09972600787878036, accuracy=0.972265625\n",
            "Test loss = 0.2564261853694916, Test accuracy = 0.9017361402511597\n",
            "Step=8699 loss=0.08989454355090856, accuracy=0.9759375\n",
            "Test loss = 0.27414461970329285, Test accuracy = 0.8971354365348816\n",
            "Step=8799 loss=0.09205968726426363, accuracy=0.975390625\n",
            "Test loss = 0.26015520095825195, Test accuracy = 0.9025173783302307\n",
            "Step=8899 loss=0.09152755200862885, accuracy=0.9759375\n",
            "Test loss = 0.2851201891899109, Test accuracy = 0.8932725787162781\n",
            "Step=8999 loss=0.0923529390245676, accuracy=0.9740625\n",
            "Test loss = 0.2663877606391907, Test accuracy = 0.9008680582046509\n",
            "Step=9099 loss=0.0854723509401083, accuracy=0.9765625\n",
            "Test loss = 0.2679602801799774, Test accuracy = 0.9012587070465088\n",
            "Step=9199 loss=0.08790054228156804, accuracy=0.97546875\n",
            "Test loss = 0.2585272789001465, Test accuracy = 0.9052517414093018\n",
            "Step=9299 loss=0.08383022099733353, accuracy=0.9784375\n",
            "Test loss = 0.27189263701438904, Test accuracy = 0.9006510376930237\n",
            "Step=9399 loss=0.08550123032182455, accuracy=0.97734375\n",
            "Test loss = 0.28333529829978943, Test accuracy = 0.8963541388511658\n",
            "Step=9499 loss=0.08484159026294946, accuracy=0.977421875\n",
            "Test loss = 0.2685815691947937, Test accuracy = 0.9015191197395325\n",
            "Step=9599 loss=0.0840062765404582, accuracy=0.976796875\n",
            "Test loss = 0.2828359305858612, Test accuracy = 0.8972222208976746\n",
            "Step=9699 loss=0.07974016234278679, accuracy=0.978203125\n",
            "Test loss = 0.2718850076198578, Test accuracy = 0.9032552242279053\n",
            "Step=9799 loss=0.07760076384991407, accuracy=0.97953125\n",
            "Test loss = 0.2669016420841217, Test accuracy = 0.9039496779441833\n",
            "Step=9899 loss=0.08141987681388856, accuracy=0.977734375\n",
            "Test loss = 0.27085718512535095, Test accuracy = 0.9027777910232544\n",
            "Step=9999 loss=0.08079579658806324, accuracy=0.97828125\n",
            "Test loss = 0.2794959247112274, Test accuracy = 0.900390625\n",
            "Step=10099 loss=0.07402310606092215, accuracy=0.98046875\n",
            "Test loss = 0.285413920879364, Test accuracy = 0.8982204794883728\n",
            "Step=10199 loss=0.0764747242256999, accuracy=0.97859375\n",
            "Test loss = 0.2785501182079315, Test accuracy = 0.901562511920929\n",
            "Step=10299 loss=0.07438615288585425, accuracy=0.980859375\n",
            "Test loss = 0.28001636266708374, Test accuracy = 0.9013020992279053\n",
            "Step=10399 loss=0.07569414906203747, accuracy=0.978671875\n",
            "Test loss = 0.2759171426296234, Test accuracy = 0.9020399451255798\n",
            "Step=10499 loss=0.07630536768585444, accuracy=0.98\n",
            "Test loss = 0.285817950963974, Test accuracy = 0.8997830152511597\n",
            "Step=10599 loss=0.07224236659705639, accuracy=0.98140625\n",
            "Test loss = 0.27728304266929626, Test accuracy = 0.9025607705116272\n",
            "Step=10699 loss=0.06857851399108768, accuracy=0.981796875\n",
            "Test loss = 0.2964607775211334, Test accuracy = 0.8972222208976746\n",
            "Step=10799 loss=0.07212361572310329, accuracy=0.9803125\n",
            "Test loss = 0.28098487854003906, Test accuracy = 0.9018229246139526\n",
            "Step=10899 loss=0.06909016665071249, accuracy=0.981796875\n",
            "Test loss = 0.27952975034713745, Test accuracy = 0.905078113079071\n",
            "Step=10999 loss=0.06483886536210776, accuracy=0.983359375\n",
            "Test loss = 0.28853553533554077, Test accuracy = 0.9020399451255798\n",
            "Step=11099 loss=0.06548976425081492, accuracy=0.982734375\n",
            "Test loss = 0.2820151746273041, Test accuracy = 0.9033420085906982\n",
            "Step=11199 loss=0.0665795974060893, accuracy=0.983046875\n",
            "Test loss = 0.2983570098876953, Test accuracy = 0.8993055820465088\n",
            "Step=11299 loss=0.06980577709153295, accuracy=0.981953125\n",
            "Test loss = 0.3006264567375183, Test accuracy = 0.8995225429534912\n",
            "Step=11399 loss=0.0634742146730423, accuracy=0.98375\n",
            "Test loss = 0.2769809365272522, Test accuracy = 0.9083767533302307\n",
            "Step=11499 loss=0.06223983347415924, accuracy=0.984296875\n",
            "Test loss = 0.3144347369670868, Test accuracy = 0.8950520753860474\n",
            "Step=11599 loss=0.06388251520693303, accuracy=0.9840625\n",
            "Test loss = 0.3007129728794098, Test accuracy = 0.9006944298744202\n",
            "Step=11699 loss=0.06155964395031333, accuracy=0.983046875\n",
            "Test loss = 0.27526387572288513, Test accuracy = 0.9097222089767456\n",
            "Step=11799 loss=0.06033246358856559, accuracy=0.98578125\n",
            "Test loss = 0.29659295082092285, Test accuracy = 0.901562511920929\n",
            "Step=11899 loss=0.05901298280805349, accuracy=0.984765625\n",
            "Test loss = 0.30052438378334045, Test accuracy = 0.9036892652511597\n",
            "Step=11999 loss=0.05767896449193358, accuracy=0.9846875\n",
            "Test loss = 0.2880173921585083, Test accuracy = 0.9075086712837219\n",
            "Step=12099 loss=0.0570347074046731, accuracy=0.985625\n",
            "Test loss = 0.28806960582733154, Test accuracy = 0.9064670205116272\n",
            "Step=12199 loss=0.05715706206858158, accuracy=0.986015625\n",
            "Test loss = 0.30518391728401184, Test accuracy = 0.9009114503860474\n",
            "Step=12299 loss=0.057721367795020344, accuracy=0.985\n",
            "Test loss = 0.307577908039093, Test accuracy = 0.9008246660232544\n",
            "Step=12399 loss=0.05660871885716915, accuracy=0.98546875\n",
            "Test loss = 0.3090256452560425, Test accuracy = 0.9024739861488342\n",
            "Step=12499 loss=0.05077549988403916, accuracy=0.988046875\n",
            "Test loss = 0.3059365153312683, Test accuracy = 0.9028645753860474\n",
            "Step=12599 loss=0.05307013554498553, accuracy=0.986953125\n",
            "Test loss = 0.2947811186313629, Test accuracy = 0.9065538048744202\n",
            "Step=12699 loss=0.053790613859891895, accuracy=0.986171875\n",
            "Test loss = 0.29657259583473206, Test accuracy = 0.9076822996139526\n",
            "Step=12799 loss=0.05140206793323159, accuracy=0.986640625\n",
            "Test loss = 0.3020544648170471, Test accuracy = 0.9053819179534912\n",
            "Step=12899 loss=0.048536101467907426, accuracy=0.988671875\n",
            "Test loss = 0.3089989125728607, Test accuracy = 0.9046875238418579\n",
            "Step=12999 loss=0.052082742489874365, accuracy=0.9875\n",
            "Test loss = 0.300615131855011, Test accuracy = 0.9069878458976746\n",
            "Step=13099 loss=0.04612771647050977, accuracy=0.988671875\n",
            "Test loss = 0.30085262656211853, Test accuracy = 0.9069444537162781\n",
            "Step=13199 loss=0.04497456626035273, accuracy=0.98921875\n",
            "Test loss = 0.31817594170570374, Test accuracy = 0.9019097089767456\n",
            "Step=13299 loss=0.046948208808898925, accuracy=0.98953125\n",
            "Test loss = 0.3088696300983429, Test accuracy = 0.9051215052604675\n",
            "Step=13399 loss=0.04847304362803698, accuracy=0.9878125\n",
            "Test loss = 0.31207647919654846, Test accuracy = 0.9075520634651184\n",
            "Step=13499 loss=0.047027008589357135, accuracy=0.98921875\n",
            "Test loss = 0.32994717359542847, Test accuracy = 0.8993489742279053\n",
            "Step=13599 loss=0.044641206078231334, accuracy=0.989453125\n",
            "Test loss = 0.3193446695804596, Test accuracy = 0.9032986164093018\n",
            "Step=13699 loss=0.04645826984196901, accuracy=0.98921875\n",
            "Test loss = 0.31340014934539795, Test accuracy = 0.9059895873069763\n",
            "Step=13799 loss=0.0420240687020123, accuracy=0.99015625\n",
            "Test loss = 0.3163083493709564, Test accuracy = 0.9060329794883728\n",
            "Step=13899 loss=0.04749931339174509, accuracy=0.98875\n",
            "Test loss = 0.32785236835479736, Test accuracy = 0.9029513597488403\n",
            "Step=13999 loss=0.04152811549603939, accuracy=0.990703125\n",
            "Test loss = 0.3229782283306122, Test accuracy = 0.9062066078186035\n",
            "Step=14099 loss=0.039836126985028385, accuracy=0.99109375\n",
            "Test loss = 0.33273687958717346, Test accuracy = 0.9034287929534912\n",
            "Step=14199 loss=0.04022977674379945, accuracy=0.990703125\n",
            "Test loss = 0.3320537209510803, Test accuracy = 0.9030815958976746\n",
            "Step=14299 loss=0.038764448575675486, accuracy=0.991875\n",
            "Test loss = 0.32459571957588196, Test accuracy = 0.9051215052604675\n",
            "Step=14399 loss=0.036582221686840055, accuracy=0.992734375\n",
            "Test loss = 0.31268197298049927, Test accuracy = 0.9112847447395325\n",
            "Step=14499 loss=0.038993329396471384, accuracy=0.990390625\n",
            "Test loss = 0.33210477232933044, Test accuracy = 0.9036892652511597\n",
            "Step=14599 loss=0.039262255178764464, accuracy=0.9915625\n",
            "Test loss = 0.32789725065231323, Test accuracy = 0.9059895873069763\n",
            "Step=14699 loss=0.0362256421521306, accuracy=0.991875\n",
            "Test loss = 0.33836236596107483, Test accuracy = 0.9029079675674438\n",
            "Step=14799 loss=0.03625071263872087, accuracy=0.99203125\n",
            "Test loss = 0.3239617347717285, Test accuracy = 0.9096788167953491\n",
            "Step=14899 loss=0.034503001887351274, accuracy=0.993515625\n",
            "Test loss = 0.34154748916625977, Test accuracy = 0.9044705033302307\n",
            "Step=14999 loss=0.033567777695134283, accuracy=0.992578125\n",
            "Test loss = 0.34534329175949097, Test accuracy = 0.902430534362793\n",
            "Step=15099 loss=0.03709450056776405, accuracy=0.991796875\n",
            "Test loss = 0.34084734320640564, Test accuracy = 0.9051215052604675\n",
            "Step=15199 loss=0.035296544656157496, accuracy=0.99234375\n",
            "Test loss = 0.338107168674469, Test accuracy = 0.9064236283302307\n",
            "Step=15299 loss=0.03664646002463996, accuracy=0.992578125\n",
            "Test loss = 0.34031379222869873, Test accuracy = 0.9041232466697693\n",
            "Step=15399 loss=0.03042318639345467, accuracy=0.993984375\n",
            "Test loss = 0.34944775700569153, Test accuracy = 0.9036024212837219\n",
            "Step=15499 loss=0.029937140857800842, accuracy=0.99359375\n",
            "Test loss = 0.3501223921775818, Test accuracy = 0.904210090637207\n",
            "Step=15599 loss=0.029868663884699345, accuracy=0.994375\n",
            "Test loss = 0.3421608507633209, Test accuracy = 0.9073784947395325\n",
            "Step=15699 loss=0.031272116154432296, accuracy=0.993515625\n",
            "Test loss = 0.34631145000457764, Test accuracy = 0.9062933921813965\n",
            "Step=15799 loss=0.030528271198272706, accuracy=0.9934375\n",
            "Test loss = 0.3521595895290375, Test accuracy = 0.9057291746139526\n",
            "Step=15899 loss=0.03230242772959173, accuracy=0.993203125\n",
            "Test loss = 0.34641233086586, Test accuracy = 0.908984363079071\n",
            "Step=15999 loss=0.030970379607751966, accuracy=0.99421875\n",
            "Test loss = 0.35031941533088684, Test accuracy = 0.9055555462837219\n",
            "Step=16099 loss=0.02796685087494552, accuracy=0.993984375\n",
            "Test loss = 0.35581308603286743, Test accuracy = 0.9040364623069763\n",
            "Step=16199 loss=0.028183127138763666, accuracy=0.994375\n",
            "Test loss = 0.35365602374076843, Test accuracy = 0.9077690839767456\n",
            "Step=16299 loss=0.027134584384039043, accuracy=0.995546875\n",
            "Test loss = 0.36003202199935913, Test accuracy = 0.906336784362793\n",
            "Step=16399 loss=0.025390610815957188, accuracy=0.99546875\n",
            "Test loss = 0.3712088167667389, Test accuracy = 0.9030815958976746\n",
            "Step=16499 loss=0.028343688156455757, accuracy=0.994296875\n",
            "Test loss = 0.3569357991218567, Test accuracy = 0.9077256917953491\n",
            "Step=16599 loss=0.027112793792039156, accuracy=0.995234375\n",
            "Test loss = 0.36731746792793274, Test accuracy = 0.9033854007720947\n",
            "Step=16699 loss=0.027391399275511503, accuracy=0.995546875\n",
            "Test loss = 0.3662717044353485, Test accuracy = 0.9053385257720947\n",
            "Step=16799 loss=0.02499091299250722, accuracy=0.9953125\n",
            "Test loss = 0.3696301281452179, Test accuracy = 0.9026909470558167\n",
            "Step=16899 loss=0.023709578486159445, accuracy=0.9953125\n",
            "Test loss = 0.3700580298900604, Test accuracy = 0.9037760496139526\n",
            "Step=16999 loss=0.025962917180731893, accuracy=0.995390625\n",
            "Test loss = 0.3741019368171692, Test accuracy = 0.9040364623069763\n",
            "Step=17099 loss=0.022045251419767736, accuracy=0.996015625\n",
            "Test loss = 0.36180177330970764, Test accuracy = 0.908116340637207\n",
            "Step=17199 loss=0.024555150186643004, accuracy=0.995703125\n",
            "Test loss = 0.3744508624076843, Test accuracy = 0.9052517414093018\n",
            "Step=17299 loss=0.02433373925741762, accuracy=0.995703125\n",
            "Test loss = 0.38181135058403015, Test accuracy = 0.9039496779441833\n",
            "Step=17399 loss=0.022695828573778273, accuracy=0.995703125\n",
            "Test loss = 0.3707795739173889, Test accuracy = 0.9073784947395325\n",
            "Step=17499 loss=0.020660717664286496, accuracy=0.99703125\n",
            "Test loss = 0.37357738614082336, Test accuracy = 0.9095486402511597\n",
            "Step=17599 loss=0.023581685810349882, accuracy=0.995625\n",
            "Test loss = 0.3728593587875366, Test accuracy = 0.9105468988418579\n",
            "Step=17699 loss=0.0224420852586627, accuracy=0.99546875\n",
            "Test loss = 0.38170453906059265, Test accuracy = 0.9056423902511597\n",
            "Step=17799 loss=0.02073879899457097, accuracy=0.996640625\n",
            "Test loss = 0.38946178555488586, Test accuracy = 0.904210090637207\n",
            "Step=17899 loss=0.020931641813367607, accuracy=0.996640625\n",
            "Test loss = 0.4070276916027069, Test accuracy = 0.8999565839767456\n",
            "Step=17999 loss=0.02022903800942004, accuracy=0.99625\n",
            "Test loss = 0.3970333933830261, Test accuracy = 0.9050347208976746\n",
            "Step=18099 loss=0.019337981729768215, accuracy=0.9971875\n",
            "Test loss = 0.39696019887924194, Test accuracy = 0.9034287929534912\n",
            "Step=18199 loss=0.018047362472862004, accuracy=0.9971875\n",
            "Test loss = 0.39496302604675293, Test accuracy = 0.9063802361488342\n",
            "Step=18299 loss=0.018123525930568575, accuracy=0.997109375\n",
            "Test loss = 0.40046241879463196, Test accuracy = 0.904296875\n",
            "Step=18399 loss=0.02082423212006688, accuracy=0.99640625\n",
            "Test loss = 0.38949695229530334, Test accuracy = 0.9104166626930237\n",
            "Step=18499 loss=0.017195090022869407, accuracy=0.9971875\n",
            "Test loss = 0.40830400586128235, Test accuracy = 0.9038194417953491\n",
            "Step=18599 loss=0.01760541656985879, accuracy=0.997265625\n",
            "Test loss = 0.3903805613517761, Test accuracy = 0.9095486402511597\n",
            "Step=18699 loss=0.01829201606567949, accuracy=0.99703125\n",
            "Test loss = 0.402587890625, Test accuracy = 0.9069010615348816\n",
            "Step=18799 loss=0.0173287552036345, accuracy=0.9971875\n",
            "Test loss = 0.41780924797058105, Test accuracy = 0.9027777910232544\n",
            "Step=18899 loss=0.01742214917205274, accuracy=0.997421875\n",
            "Test loss = 0.4030971825122833, Test accuracy = 0.9064670205116272\n",
            "Step=18999 loss=0.01804268218576908, accuracy=0.997265625\n",
            "Test loss = 0.4100680947303772, Test accuracy = 0.9048610925674438\n",
            "Step=19099 loss=0.01690502915531397, accuracy=0.997421875\n",
            "Test loss = 0.40312647819519043, Test accuracy = 0.9085069298744202\n",
            "Step=19199 loss=0.01655536304228008, accuracy=0.997578125\n",
            "Test loss = 0.40679502487182617, Test accuracy = 0.9078993201255798\n",
            "Step=19299 loss=0.01641496335156262, accuracy=0.99765625\n",
            "Test loss = 0.409457266330719, Test accuracy = 0.907421886920929\n",
            "Step=19399 loss=0.015591060635633766, accuracy=0.997890625\n",
            "Test loss = 0.41086187958717346, Test accuracy = 0.9069010615348816\n",
            "Step=19499 loss=0.01592190981376916, accuracy=0.997421875\n",
            "Test loss = 0.41991347074508667, Test accuracy = 0.9055121541023254\n",
            "Step=19599 loss=0.014794649030081928, accuracy=0.998203125\n",
            "Test loss = 0.4194900393486023, Test accuracy = 0.9055989384651184\n",
            "Step=19699 loss=0.013592271502129733, accuracy=0.998125\n",
            "Test loss = 0.41533151268959045, Test accuracy = 0.9087239503860474\n",
            "Step=19799 loss=0.014588979966938496, accuracy=0.998125\n",
            "Test loss = 0.4225689470767975, Test accuracy = 0.9063802361488342\n",
            "Step=19899 loss=0.014345677671954035, accuracy=0.998203125\n",
            "Test loss = 0.4240263104438782, Test accuracy = 0.9084635376930237\n",
            "Step=19999 loss=0.015161592736840248, accuracy=0.997578125\n",
            "Test loss = 0.43128883838653564, Test accuracy = 0.9051649570465088\n",
            "Step=20099 loss=0.013207378145307303, accuracy=0.998203125\n",
            "Test loss = 0.4306280314922333, Test accuracy = 0.9037760496139526\n",
            "Step=20199 loss=0.013553635240532458, accuracy=0.998203125\n",
            "Test loss = 0.4320610761642456, Test accuracy = 0.9052083492279053\n",
            "Step=20299 loss=0.011612507393583655, accuracy=0.99875\n",
            "Test loss = 0.4304804503917694, Test accuracy = 0.9067274332046509\n",
            "Step=20399 loss=0.010727847605012357, accuracy=0.99875\n",
            "Test loss = 0.42923256754875183, Test accuracy = 0.9079861044883728\n",
            "Step=20499 loss=0.012664446921553463, accuracy=0.99859375\n",
            "Test loss = 0.4314573407173157, Test accuracy = 0.9065538048744202\n",
            "Step=20599 loss=0.012222027936950326, accuracy=0.998671875\n",
            "Test loss = 0.4402882754802704, Test accuracy = 0.9052517414093018\n",
            "Step=20699 loss=0.01075523391366005, accuracy=0.99875\n",
            "Test loss = 0.44733425974845886, Test accuracy = 0.9056857824325562\n",
            "Step=20799 loss=0.010820180168375373, accuracy=0.99890625\n",
            "Test loss = 0.4418492615222931, Test accuracy = 0.9063802361488342\n",
            "Step=20899 loss=0.009306208877824247, accuracy=0.99890625\n",
            "Test loss = 0.4448573589324951, Test accuracy = 0.906336784362793\n",
            "Step=20999 loss=0.00977454345440492, accuracy=0.99921875\n",
            "Test loss = 0.4420052766799927, Test accuracy = 0.9079861044883728\n",
            "Step=21099 loss=0.009690101309679448, accuracy=0.998671875\n",
            "Test loss = 0.44805142283439636, Test accuracy = 0.9065971970558167\n",
            "Step=21199 loss=0.01138161051319912, accuracy=0.9984375\n",
            "Test loss = 0.4529709815979004, Test accuracy = 0.9072482585906982\n",
            "Step=21299 loss=0.01019850245444104, accuracy=0.998828125\n",
            "Test loss = 0.4535803198814392, Test accuracy = 0.9083333611488342\n",
            "Step=21399 loss=0.010897042178548873, accuracy=0.99875\n",
            "Test loss = 0.45412686467170715, Test accuracy = 0.9066406488418579\n",
            "Step=21499 loss=0.010185202022548766, accuracy=0.9984375\n",
            "Test loss = 0.45457330346107483, Test accuracy = 0.9092013835906982\n",
            "Step=21599 loss=0.009533823907840997, accuracy=0.999296875\n",
            "Test loss = 0.45795831084251404, Test accuracy = 0.9065104126930237\n",
            "Step=21699 loss=0.009527713814750314, accuracy=0.998671875\n",
            "Test loss = 0.4555343687534332, Test accuracy = 0.9090712070465088\n",
            "Step=21799 loss=0.009648824364412577, accuracy=0.99890625\n",
            "Test loss = 0.4577207863330841, Test accuracy = 0.9091579914093018\n",
            "Step=21899 loss=0.0094542221352458, accuracy=0.99921875\n",
            "Test loss = 0.4604927599430084, Test accuracy = 0.9091145992279053\n",
            "Step=21999 loss=0.008999264598824084, accuracy=0.9990625\n",
            "Test loss = 0.46442896127700806, Test accuracy = 0.9095920324325562\n",
            "Step=22099 loss=0.008017201144248247, accuracy=0.999296875\n",
            "Test loss = 0.4639692008495331, Test accuracy = 0.9092447757720947\n",
            "Step=22199 loss=0.008358222187962384, accuracy=0.9990625\n",
            "Test loss = 0.4735608398914337, Test accuracy = 0.9084635376930237\n",
            "Step=22299 loss=0.00834385321009904, accuracy=0.999140625\n",
            "Test loss = 0.4721945822238922, Test accuracy = 0.9093316197395325\n",
            "Step=22399 loss=0.006767226504161954, accuracy=0.99953125\n",
            "Test loss = 0.47535282373428345, Test accuracy = 0.9077690839767456\n",
            "Step=22499 loss=0.008030765347648412, accuracy=0.998828125\n",
            "Test loss = 0.47727152705192566, Test accuracy = 0.9090277552604675\n",
            "Step=22599 loss=0.006666947125922889, accuracy=0.999609375\n",
            "Test loss = 0.48358628153800964, Test accuracy = 0.9050347208976746\n",
            "Step=22699 loss=0.007603315562009811, accuracy=0.999140625\n",
            "Test loss = 0.47685110569000244, Test accuracy = 0.9097222089767456\n",
            "Step=22799 loss=0.007293409218546003, accuracy=0.999453125\n",
            "Test loss = 0.48987045884132385, Test accuracy = 0.9051215052604675\n",
            "Step=22899 loss=0.006960642384365201, accuracy=0.999453125\n",
            "Test loss = 0.4914507567882538, Test accuracy = 0.906163215637207\n",
            "Step=22999 loss=0.006550340686226263, accuracy=0.99953125\n",
            "Test loss = 0.4921514689922333, Test accuracy = 0.9074652791023254\n",
            "Step=23099 loss=0.006797974745277316, accuracy=0.999375\n",
            "Test loss = 0.492044061422348, Test accuracy = 0.9093316197395325\n",
            "Step=23199 loss=0.006067467271350324, accuracy=0.9996875\n",
            "Test loss = 0.49530383944511414, Test accuracy = 0.9079861044883728\n",
            "Step=23299 loss=0.007430275622755289, accuracy=0.999140625\n",
            "Test loss = 0.5009839534759521, Test accuracy = 0.9065971970558167\n",
            "Step=23399 loss=0.006018561787204817, accuracy=0.999609375\n",
            "Test loss = 0.508063554763794, Test accuracy = 0.9047309160232544\n",
            "Step=23499 loss=0.006304056477965787, accuracy=0.999375\n",
            "Test loss = 0.4969978332519531, Test accuracy = 0.9085069298744202\n",
            "Step=23599 loss=0.006043876246549189, accuracy=0.999453125\n",
            "Test loss = 0.5040102601051331, Test accuracy = 0.9076822996139526\n",
            "Step=23699 loss=0.006334127691807225, accuracy=0.99953125\n",
            "Test loss = 0.5028470158576965, Test accuracy = 0.9098524451255798\n",
            "Step=23799 loss=0.005695945464540273, accuracy=0.999609375\n",
            "Test loss = 0.5065423846244812, Test accuracy = 0.908984363079071\n",
            "Step=23899 loss=0.004869198983069509, accuracy=0.99984375\n",
            "Test loss = 0.5081948637962341, Test accuracy = 0.9082465171813965\n",
            "Step=23999 loss=0.005582070371601731, accuracy=0.99953125\n",
            "Test loss = 0.5064367651939392, Test accuracy = 0.9090277552604675\n",
            "Step=24099 loss=0.0046136272023431955, accuracy=0.999921875\n",
            "Test loss = 0.512069582939148, Test accuracy = 0.9088107347488403\n",
            "Step=24199 loss=0.004358249867800623, accuracy=1.0\n",
            "Test loss = 0.5135217308998108, Test accuracy = 0.9092881679534912\n",
            "Step=24299 loss=0.004344051966909319, accuracy=0.99984375\n",
            "Test loss = 0.5230122804641724, Test accuracy = 0.9077256917953491\n",
            "Step=24399 loss=0.005369048184948042, accuracy=0.999609375\n",
            "Test loss = 0.5293468832969666, Test accuracy = 0.9059461951255798\n",
            "Step=24499 loss=0.003942752794828266, accuracy=0.999921875\n",
            "Test loss = 0.5298689007759094, Test accuracy = 0.9060329794883728\n",
            "Step=24599 loss=0.004466859388630837, accuracy=0.99953125\n",
            "Test loss = 0.5265903472900391, Test accuracy = 0.9085069298744202\n",
            "Step=24699 loss=0.004654716413933784, accuracy=0.999765625\n",
            "Test loss = 0.5247192978858948, Test accuracy = 0.910069465637207\n",
            "Step=24799 loss=0.004635571893304586, accuracy=0.999453125\n",
            "Test loss = 0.529050886631012, Test accuracy = 0.9097222089767456\n",
            "Step=24899 loss=0.004042289385106415, accuracy=0.999921875\n",
            "Test loss = 0.5350649356842041, Test accuracy = 0.9077256917953491\n",
            "Step=24999 loss=0.005903422432020307, accuracy=0.99953125\n",
            "Test loss = 0.5496284365653992, Test accuracy = 0.9043402671813965\n",
            "Step=25099 loss=0.004401736246654764, accuracy=0.9996875\n",
            "Test loss = 0.5320200324058533, Test accuracy = 0.910069465637207\n",
            "Step=25199 loss=0.004340668694349006, accuracy=0.999609375\n",
            "Test loss = 0.5356300473213196, Test accuracy = 0.9085069298744202\n",
            "Step=25299 loss=0.004068455037195235, accuracy=0.999765625\n",
            "Test loss = 0.5374231338500977, Test accuracy = 0.9075520634651184\n",
            "Step=25399 loss=0.003534704678459093, accuracy=0.999921875\n",
            "Test loss = 0.5417972803115845, Test accuracy = 0.9075520634651184\n",
            "Step=25499 loss=0.004107954186620191, accuracy=0.9996875\n",
            "Test loss = 0.5492587685585022, Test accuracy = 0.9064670205116272\n",
            "Step=25599 loss=0.004134532586904243, accuracy=0.999765625\n",
            "Test loss = 0.5600407719612122, Test accuracy = 0.9029513597488403\n",
            "Step=25699 loss=0.0037340914690867067, accuracy=0.9996875\n",
            "Test loss = 0.5490491986274719, Test accuracy = 0.9076389074325562\n",
            "Step=25799 loss=0.004136127951787785, accuracy=0.99953125\n",
            "Test loss = 0.5526134371757507, Test accuracy = 0.907421886920929\n",
            "Step=25899 loss=0.003707242503296584, accuracy=0.99984375\n",
            "Test loss = 0.5620190501213074, Test accuracy = 0.9066840410232544\n",
            "Step=25999 loss=0.0032745811878703533, accuracy=0.99984375\n",
            "Test loss = 0.5536724925041199, Test accuracy = 0.9087673425674438\n",
            "Step=26099 loss=0.0032420643977820874, accuracy=0.999765625\n",
            "Test loss = 0.563409686088562, Test accuracy = 0.9065538048744202\n",
            "Step=26199 loss=0.0031754320982145144, accuracy=0.999921875\n",
            "Test loss = 0.5666284561157227, Test accuracy = 0.9053819179534912\n",
            "Step=26299 loss=0.0027891862025717275, accuracy=1.0\n",
            "Test loss = 0.5634850859642029, Test accuracy = 0.9083767533302307\n",
            "Step=26399 loss=0.003261558320373297, accuracy=0.999921875\n",
            "Test loss = 0.568908154964447, Test accuracy = 0.9068576097488403\n",
            "Step=26499 loss=0.002566437447676435, accuracy=0.999921875\n",
            "Test loss = 0.5613119602203369, Test accuracy = 0.9088975787162781\n",
            "Step=26599 loss=0.002904328605509363, accuracy=0.999765625\n",
            "Test loss = 0.5736798644065857, Test accuracy = 0.9067708253860474\n",
            "Step=26699 loss=0.002771816088352352, accuracy=0.99984375\n",
            "Test loss = 0.5683784484863281, Test accuracy = 0.9095486402511597\n",
            "Step=26799 loss=0.0029231153841828925, accuracy=0.99984375\n",
            "Test loss = 0.5763387680053711, Test accuracy = 0.9069010615348816\n",
            "Step=26899 loss=0.002529863000381738, accuracy=0.999921875\n",
            "Test loss = 0.5747572779655457, Test accuracy = 0.9088541865348816\n",
            "Step=26999 loss=0.0027142806840129196, accuracy=0.999921875\n",
            "Test loss = 0.5771855711936951, Test accuracy = 0.9091145992279053\n",
            "Step=27099 loss=0.00278217455081176, accuracy=0.99984375\n",
            "Test loss = 0.5820234417915344, Test accuracy = 0.9071180820465088\n",
            "Step=27199 loss=0.002306074424413964, accuracy=0.999921875\n",
            "Test loss = 0.5878781080245972, Test accuracy = 0.9072916507720947\n",
            "Step=27299 loss=0.0024041660578222944, accuracy=0.999921875\n",
            "Test loss = 0.5887680053710938, Test accuracy = 0.9065538048744202\n",
            "Step=27399 loss=0.0023344422288937494, accuracy=1.0\n",
            "Test loss = 0.5910460352897644, Test accuracy = 0.9071180820465088\n",
            "Step=27499 loss=0.0019299154257168994, accuracy=1.0\n",
            "Test loss = 0.5947479605674744, Test accuracy = 0.9071614742279053\n",
            "Step=27599 loss=0.002515553059638478, accuracy=0.99984375\n",
            "Test loss = 0.5916504859924316, Test accuracy = 0.908203125\n",
            "Step=27699 loss=0.002091261353343725, accuracy=1.0\n",
            "Test loss = 0.5978280305862427, Test accuracy = 0.9080728888511658\n",
            "Step=27799 loss=0.0020052281604148446, accuracy=1.0\n",
            "Test loss = 0.6025153994560242, Test accuracy = 0.9073350429534912\n",
            "Step=27899 loss=0.001815098681836389, accuracy=1.0\n",
            "Test loss = 0.6047719717025757, Test accuracy = 0.9075520634651184\n",
            "Step=27999 loss=0.0019136974308639765, accuracy=1.0\n",
            "Test loss = 0.6060853004455566, Test accuracy = 0.9084635376930237\n",
            "Step=28099 loss=0.0019240723369875923, accuracy=1.0\n",
            "Test loss = 0.6046778559684753, Test accuracy = 0.9075086712837219\n",
            "Step=28199 loss=0.0016184565797448158, accuracy=1.0\n",
            "Test loss = 0.6089509725570679, Test accuracy = 0.9072482585906982\n",
            "Step=28299 loss=0.002310055225389078, accuracy=0.999765625\n",
            "Test loss = 0.6121128797531128, Test accuracy = 0.909375011920929\n",
            "Step=28399 loss=0.0016414474847260862, accuracy=1.0\n",
            "Test loss = 0.6165440082550049, Test accuracy = 0.9072916507720947\n",
            "Step=28499 loss=0.0015373509097844362, accuracy=1.0\n",
            "Test loss = 0.6196996569633484, Test accuracy = 0.9088541865348816\n",
            "Step=28599 loss=0.001891132885357365, accuracy=0.999921875\n",
            "Test loss = 0.6183286905288696, Test accuracy = 0.9095920324325562\n",
            "Step=28699 loss=0.001636334454815369, accuracy=1.0\n",
            "Test loss = 0.6253108382225037, Test accuracy = 0.9080294966697693\n",
            "Step=28799 loss=0.0015340268006548286, accuracy=1.0\n",
            "Test loss = 0.6263872981071472, Test accuracy = 0.9068576097488403\n",
            "Step=28899 loss=0.0014022276393370703, accuracy=1.0\n",
            "Test loss = 0.6255183219909668, Test accuracy = 0.9061197638511658\n",
            "Step=28999 loss=0.001534673978749197, accuracy=1.0\n",
            "Test loss = 0.6331896185874939, Test accuracy = 0.9073784947395325\n",
            "Step=29099 loss=0.001488160528242588, accuracy=1.0\n",
            "Test loss = 0.63129723072052, Test accuracy = 0.9091579914093018\n",
            "Step=29199 loss=0.0014315676718251779, accuracy=1.0\n",
            "Test loss = 0.6366549134254456, Test accuracy = 0.9092881679534912\n",
            "Step=29299 loss=0.001390135077526793, accuracy=1.0\n",
            "Test loss = 0.6424295902252197, Test accuracy = 0.9083333611488342\n",
            "Step=29399 loss=0.0013252481713425367, accuracy=1.0\n",
            "Test loss = 0.6392015218734741, Test accuracy = 0.9077690839767456\n",
            "Step=29499 loss=0.001185957394191064, accuracy=1.0\n",
            "Test loss = 0.6406493782997131, Test accuracy = 0.9091579914093018\n",
            "Step=29599 loss=0.0012092794588534162, accuracy=1.0\n",
            "Test loss = 0.6460543870925903, Test accuracy = 0.9092013835906982\n",
            "Step=29699 loss=0.0012385812657885254, accuracy=1.0\n",
            "Test loss = 0.6467342376708984, Test accuracy = 0.9090712070465088\n",
            "Step=29799 loss=0.001140451910032425, accuracy=1.0\n",
            "Test loss = 0.6561594605445862, Test accuracy = 0.9077690839767456\n",
            "Step=29899 loss=0.0010942475902265868, accuracy=1.0\n",
            "Test loss = 0.65769362449646, Test accuracy = 0.9077256917953491\n",
            "Step=29999 loss=0.001188771232846193, accuracy=1.0\n",
            "Test loss = 0.654435396194458, Test accuracy = 0.9084635376930237\n",
            "Step=30099 loss=0.0010689617577008903, accuracy=1.0\n",
            "Test loss = 0.6594716310501099, Test accuracy = 0.9074652791023254\n",
            "Step=30199 loss=0.0010489644270273858, accuracy=1.0\n",
            "Test loss = 0.6595638990402222, Test accuracy = 0.9079861044883728\n",
            "Step=30299 loss=0.0010090834635775535, accuracy=1.0\n",
            "Test loss = 0.661547064781189, Test accuracy = 0.9104600548744202\n",
            "Step=30399 loss=0.000878794530290179, accuracy=1.0\n",
            "Test loss = 0.665686845779419, Test accuracy = 0.908289909362793\n",
            "Step=30499 loss=0.000950051863619592, accuracy=1.0\n",
            "Test loss = 0.6753950119018555, Test accuracy = 0.9069878458976746\n",
            "Step=30599 loss=0.0009604344272520393, accuracy=1.0\n",
            "Test loss = 0.6741399168968201, Test accuracy = 0.910069465637207\n",
            "Step=30699 loss=0.001199276192637626, accuracy=0.999921875\n",
            "Test loss = 0.6718188524246216, Test accuracy = 0.9083333611488342\n",
            "Step=30799 loss=0.0008615343381825368, accuracy=1.0\n",
            "Test loss = 0.6800878047943115, Test accuracy = 0.9111545085906982\n",
            "Step=30899 loss=0.0012263266369700431, accuracy=0.999921875\n",
            "Test loss = 0.6892441511154175, Test accuracy = 0.9069878458976746\n",
            "Step=30999 loss=0.0009458363108569756, accuracy=1.0\n",
            "Test loss = 0.6854856610298157, Test accuracy = 0.908289909362793\n",
            "Step=31099 loss=0.0009426001443353016, accuracy=1.0\n",
            "Test loss = 0.6851221919059753, Test accuracy = 0.9089409708976746\n",
            "Step=31199 loss=0.0008842974199797026, accuracy=1.0\n",
            "Test loss = 0.7022163271903992, Test accuracy = 0.9080728888511658\n",
            "Step=31299 loss=0.0009068494732491672, accuracy=1.0\n",
            "Test loss = 0.6911455988883972, Test accuracy = 0.908289909362793\n",
            "Step=31399 loss=0.000944753738003783, accuracy=1.0\n",
            "Test loss = 0.6961718201637268, Test accuracy = 0.9088541865348816\n",
            "Step=31499 loss=0.0007440097023209091, accuracy=1.0\n",
            "Test loss = 0.6972209215164185, Test accuracy = 0.9085937738418579\n",
            "Step=31599 loss=0.000639188471250236, accuracy=1.0\n",
            "Test loss = 0.7013787031173706, Test accuracy = 0.908289909362793\n",
            "Step=31699 loss=0.0006464156000583898, accuracy=1.0\n",
            "Test loss = 0.7022631764411926, Test accuracy = 0.9100260138511658\n",
            "Step=31799 loss=0.0005774595311959274, accuracy=1.0\n",
            "Test loss = 0.7043665647506714, Test accuracy = 0.908984363079071\n",
            "Step=31899 loss=0.0005840601019735914, accuracy=1.0\n",
            "Test loss = 0.7072840929031372, Test accuracy = 0.9098524451255798\n",
            "Step=31999 loss=0.0005811883579008282, accuracy=1.0\n",
            "Test loss = 0.7081934213638306, Test accuracy = 0.9090712070465088\n",
            "Step=32099 loss=0.0006189136567991227, accuracy=1.0\n",
            "Test loss = 0.71314936876297, Test accuracy = 0.9088975787162781\n",
            "Step=32199 loss=0.0005188737949356437, accuracy=1.0\n",
            "Test loss = 0.7184242010116577, Test accuracy = 0.9065971970558167\n",
            "Step=32299 loss=0.0005269064677122515, accuracy=1.0\n",
            "Test loss = 0.7178537845611572, Test accuracy = 0.9095051884651184\n",
            "Step=32399 loss=0.0005305272803525441, accuracy=1.0\n",
            "Test loss = 0.7160666584968567, Test accuracy = 0.9094617962837219\n",
            "Step=32499 loss=0.0005652937936247327, accuracy=1.0\n",
            "Test loss = 0.7263658046722412, Test accuracy = 0.9072482585906982\n",
            "Step=32599 loss=0.0005226302666414995, accuracy=1.0\n",
            "Test loss = 0.7239400148391724, Test accuracy = 0.908203125\n",
            "Step=32699 loss=0.00046818325194180943, accuracy=1.0\n",
            "Test loss = 0.7319448590278625, Test accuracy = 0.9077690839767456\n",
            "Step=32799 loss=0.0004643361251510214, accuracy=1.0\n",
            "Test loss = 0.7285581231117249, Test accuracy = 0.9081597328186035\n",
            "Step=32899 loss=0.0004441459971712902, accuracy=1.0\n",
            "Test loss = 0.7344767451286316, Test accuracy = 0.9086371660232544\n",
            "Step=32999 loss=0.000465524470055243, accuracy=1.0\n",
            "Test loss = 0.7346847653388977, Test accuracy = 0.9089409708976746\n",
            "Step=33099 loss=0.00040328907620278186, accuracy=1.0\n",
            "Test loss = 0.7393222451210022, Test accuracy = 0.9078993201255798\n",
            "Step=33199 loss=0.0005360492215550039, accuracy=1.0\n",
            "Test loss = 0.7429283261299133, Test accuracy = 0.9080728888511658\n",
            "Step=33299 loss=0.0004474059610220138, accuracy=1.0\n",
            "Test loss = 0.742486834526062, Test accuracy = 0.9077256917953491\n",
            "Step=33399 loss=0.00042687578010372815, accuracy=1.0\n",
            "Test loss = 0.7420899868011475, Test accuracy = 0.9098524451255798\n",
            "Step=33499 loss=0.0003516571329964791, accuracy=1.0\n",
            "Test loss = 0.7484487891197205, Test accuracy = 0.9090712070465088\n",
            "Step=33599 loss=0.00034092304354999215, accuracy=1.0\n",
            "Test loss = 0.7528380751609802, Test accuracy = 0.9089409708976746\n",
            "Step=33699 loss=0.0004004796739172889, accuracy=1.0\n",
            "Test loss = 0.7591235637664795, Test accuracy = 0.9076389074325562\n",
            "Step=33799 loss=0.0003732888177910354, accuracy=1.0\n",
            "Test loss = 0.7573188543319702, Test accuracy = 0.9092447757720947\n",
            "Step=33899 loss=0.00032693043882318305, accuracy=1.0\n",
            "Test loss = 0.7592727541923523, Test accuracy = 0.9098090529441833\n",
            "Step=33999 loss=0.0003060612610715907, accuracy=1.0\n",
            "Test loss = 0.7742122411727905, Test accuracy = 0.9049913287162781\n",
            "Step=34099 loss=0.00035544331025448626, accuracy=1.0\n",
            "Test loss = 0.7678674459457397, Test accuracy = 0.9077256917953491\n",
            "Step=34199 loss=0.0003155355859780684, accuracy=1.0\n",
            "Test loss = 0.7726423740386963, Test accuracy = 0.907421886920929\n",
            "Step=34299 loss=0.00031500989498454144, accuracy=1.0\n",
            "Test loss = 0.7718214988708496, Test accuracy = 0.9081597328186035\n",
            "Step=34399 loss=0.00029610861362016293, accuracy=1.0\n",
            "Test loss = 0.7720991373062134, Test accuracy = 0.910243034362793\n",
            "Step=34499 loss=0.00032928178821748586, accuracy=1.0\n",
            "Test loss = 0.7761921286582947, Test accuracy = 0.9090277552604675\n",
            "Step=34599 loss=0.00028202224697452036, accuracy=1.0\n",
            "Test loss = 0.7799836993217468, Test accuracy = 0.9088107347488403\n",
            "Step=34699 loss=0.0002830831600294914, accuracy=1.0\n",
            "Test loss = 0.7835471034049988, Test accuracy = 0.9084201455116272\n",
            "Step=34799 loss=0.00026083222197485154, accuracy=1.0\n",
            "Test loss = 0.787066638469696, Test accuracy = 0.9086805582046509\n",
            "Step=34899 loss=0.0003037980876251822, accuracy=1.0\n",
            "Test loss = 0.7975435256958008, Test accuracy = 0.9078559279441833\n",
            "Step=34999 loss=0.00023730963388516102, accuracy=1.0\n",
            "Test loss = 0.7942273616790771, Test accuracy = 0.9086805582046509\n",
            "Step=35099 loss=0.00022515222619404085, accuracy=1.0\n",
            "Test loss = 0.8029311299324036, Test accuracy = 0.9064670205116272\n",
            "Step=35199 loss=0.00022477836384496186, accuracy=1.0\n",
            "Test loss = 0.8050891160964966, Test accuracy = 0.9086805582046509\n",
            "Step=35299 loss=0.0003165092330891639, accuracy=1.0\n",
            "Test loss = 0.799996018409729, Test accuracy = 0.9078559279441833\n",
            "Step=35399 loss=0.00019896358615369535, accuracy=1.0\n",
            "Test loss = 0.803098738193512, Test accuracy = 0.908116340637207\n",
            "Step=35499 loss=0.00019924207728763576, accuracy=1.0\n",
            "Test loss = 0.8077179193496704, Test accuracy = 0.9094617962837219\n",
            "Step=35599 loss=0.00018615106913784984, accuracy=1.0\n",
            "Test loss = 0.8078635931015015, Test accuracy = 0.9092013835906982\n",
            "Step=35699 loss=0.0001854330990317976, accuracy=1.0\n",
            "Test loss = 0.8139176964759827, Test accuracy = 0.9085503220558167\n",
            "Step=35799 loss=0.00017575010628206655, accuracy=1.0\n",
            "Test loss = 0.818149983882904, Test accuracy = 0.9085503220558167\n",
            "Step=35899 loss=0.00019773456544498912, accuracy=1.0\n",
            "Test loss = 0.8347764611244202, Test accuracy = 0.9036458134651184\n",
            "Step=35999 loss=0.013256185878417455, accuracy=0.997421875\n",
            "Test loss = 0.8800904750823975, Test accuracy = 0.8968750238418579\n",
            "Step=36099 loss=0.003417188692110358, accuracy=0.999375\n",
            "Test loss = 0.8190249800682068, Test accuracy = 0.9049479365348816\n",
            "Step=36199 loss=0.0005190436544944533, accuracy=1.0\n",
            "Test loss = 0.816927433013916, Test accuracy = 0.9062933921813965\n",
            "Step=36299 loss=0.00038434833462815734, accuracy=1.0\n",
            "Test loss = 0.8194451928138733, Test accuracy = 0.9055555462837219\n",
            "Step=36399 loss=0.0003794914239551872, accuracy=1.0\n",
            "Test loss = 0.817091703414917, Test accuracy = 0.9064236283302307\n",
            "Step=36499 loss=0.0002742966382356826, accuracy=1.0\n",
            "Test loss = 0.8172575235366821, Test accuracy = 0.9059028029441833\n",
            "Step=36599 loss=0.0002632671613537241, accuracy=1.0\n",
            "Test loss = 0.8180719017982483, Test accuracy = 0.906163215637207\n",
            "Step=36699 loss=0.000267091825735406, accuracy=1.0\n",
            "Test loss = 0.8183560967445374, Test accuracy = 0.9056857824325562\n",
            "Step=36799 loss=0.00021931175229838117, accuracy=1.0\n",
            "Test loss = 0.8194954991340637, Test accuracy = 0.9059461951255798\n",
            "Step=36899 loss=0.0002309631927346345, accuracy=1.0\n",
            "Test loss = 0.8176030516624451, Test accuracy = 0.9065104126930237\n",
            "Step=36999 loss=0.00022463756184151862, accuracy=1.0\n",
            "Test loss = 0.8180475831031799, Test accuracy = 0.907031238079071\n",
            "Step=37099 loss=0.0002033947329618968, accuracy=1.0\n",
            "Test loss = 0.8192459940910339, Test accuracy = 0.9068576097488403\n",
            "Step=37199 loss=0.00020973603313905186, accuracy=1.0\n",
            "Test loss = 0.818997859954834, Test accuracy = 0.9073784947395325\n",
            "Step=37299 loss=0.0001942890200007241, accuracy=1.0\n",
            "Test loss = 0.8187011480331421, Test accuracy = 0.9071614742279053\n",
            "Step=37399 loss=0.00019571573575376532, accuracy=1.0\n",
            "Test loss = 0.8190293908119202, Test accuracy = 0.9075086712837219\n",
            "Step=37499 loss=0.00018118721760401968, accuracy=1.0\n",
            "Test loss = 0.8196769952774048, Test accuracy = 0.9071180820465088\n",
            "Step=37599 loss=0.00018337552952289116, accuracy=1.0\n",
            "Test loss = 0.819973349571228, Test accuracy = 0.9078124761581421\n",
            "Step=37699 loss=0.0001998828650539508, accuracy=1.0\n",
            "Test loss = 0.8201218247413635, Test accuracy = 0.9078559279441833\n",
            "Step=37799 loss=0.00017758767564373556, accuracy=1.0\n",
            "Test loss = 0.820554792881012, Test accuracy = 0.9069878458976746\n",
            "Step=37899 loss=0.00018048580270260572, accuracy=1.0\n",
            "Test loss = 0.8200421929359436, Test accuracy = 0.9079427123069763\n",
            "Step=37999 loss=0.00017146965437859764, accuracy=1.0\n",
            "Test loss = 0.8220936059951782, Test accuracy = 0.9073350429534912\n",
            "Step=38099 loss=0.0001699768068283447, accuracy=1.0\n",
            "Test loss = 0.8228484988212585, Test accuracy = 0.907421886920929\n",
            "Step=38199 loss=0.00016032662610086844, accuracy=1.0\n",
            "Test loss = 0.8233787417411804, Test accuracy = 0.9075086712837219\n",
            "Step=38299 loss=0.00017089761451643425, accuracy=1.0\n",
            "Test loss = 0.8227109313011169, Test accuracy = 0.9076389074325562\n",
            "Step=38399 loss=0.0001561934450728586, accuracy=1.0\n",
            "Test loss = 0.8240310549736023, Test accuracy = 0.9076389074325562\n",
            "Step=38499 loss=0.00015947229814628372, accuracy=1.0\n",
            "Test loss = 0.825115978717804, Test accuracy = 0.9071614742279053\n",
            "Step=38599 loss=0.00014743350340722827, accuracy=1.0\n",
            "Test loss = 0.8258996605873108, Test accuracy = 0.9073784947395325\n",
            "Step=38699 loss=0.00016056705884693657, accuracy=1.0\n",
            "Test loss = 0.8257466554641724, Test accuracy = 0.9078124761581421\n",
            "Step=38799 loss=0.00015490980869799386, accuracy=1.0\n",
            "Test loss = 0.8278127312660217, Test accuracy = 0.9072048664093018\n",
            "Step=38899 loss=0.0001582951129967114, accuracy=1.0\n",
            "Test loss = 0.8272660374641418, Test accuracy = 0.9073350429534912\n",
            "Step=38999 loss=0.00014150448369036893, accuracy=1.0\n",
            "Test loss = 0.8281898498535156, Test accuracy = 0.9075955152511597\n",
            "Step=39099 loss=0.00014884672680636867, accuracy=1.0\n",
            "Test loss = 0.8289754390716553, Test accuracy = 0.9076389074325562\n",
            "Step=39199 loss=0.00014824394940660568, accuracy=1.0\n",
            "Test loss = 0.8291417360305786, Test accuracy = 0.907421886920929\n",
            "Step=39299 loss=0.0001498542795161484, accuracy=1.0\n",
            "Test loss = 0.8295282125473022, Test accuracy = 0.9077256917953491\n",
            "Step=39399 loss=0.00013398933955613757, accuracy=1.0\n",
            "Test loss = 0.8305056095123291, Test accuracy = 0.9073784947395325\n",
            "Step=39499 loss=0.00013267720052681398, accuracy=1.0\n",
            "Test loss = 0.8304852247238159, Test accuracy = 0.9087673425674438\n",
            "Step=39599 loss=0.00013941667202743702, accuracy=1.0\n",
            "Test loss = 0.8313122391700745, Test accuracy = 0.9083767533302307\n",
            "Step=39699 loss=0.0001448406232520938, accuracy=1.0\n",
            "Test loss = 0.8331461548805237, Test accuracy = 0.9085069298744202\n",
            "Step=39799 loss=0.00013756981381447985, accuracy=1.0\n",
            "Test loss = 0.8313345909118652, Test accuracy = 0.9085503220558167\n",
            "Step=39899 loss=0.0001354819311382016, accuracy=1.0\n",
            "Test loss = 0.8347318768501282, Test accuracy = 0.9073784947395325\n",
            "Step=39999 loss=0.00012967501417733728, accuracy=1.0\n",
            "Test loss = 0.8361300230026245, Test accuracy = 0.9077256917953491\n",
            "Step=40099 loss=0.0001359398354543373, accuracy=1.0\n",
            "Test loss = 0.8388254642486572, Test accuracy = 0.9070746302604675\n",
            "Step=40199 loss=0.00012450004778656875, accuracy=1.0\n",
            "Test loss = 0.8390001058578491, Test accuracy = 0.9073350429534912\n",
            "Step=40299 loss=0.0001282080043165479, accuracy=1.0\n",
            "Test loss = 0.8403340578079224, Test accuracy = 0.9080294966697693\n",
            "Step=40399 loss=0.00011900341851287522, accuracy=1.0\n",
            "Test loss = 0.8405063152313232, Test accuracy = 0.9067708253860474\n",
            "Step=40499 loss=0.00012981939093151596, accuracy=1.0\n",
            "Test loss = 0.8398120403289795, Test accuracy = 0.908116340637207\n",
            "Step=40599 loss=0.00011583693612919888, accuracy=1.0\n",
            "Test loss = 0.8410666584968567, Test accuracy = 0.9083767533302307\n",
            "Step=40699 loss=0.00011761508547351695, accuracy=1.0\n",
            "Test loss = 0.8428170084953308, Test accuracy = 0.9078124761581421\n",
            "Step=40799 loss=0.00012509717937064124, accuracy=1.0\n",
            "Test loss = 0.8443064093589783, Test accuracy = 0.9081597328186035\n",
            "Step=40899 loss=0.00012732397528452565, accuracy=1.0\n",
            "Test loss = 0.84511399269104, Test accuracy = 0.9073350429534912\n",
            "Step=40999 loss=0.00011902908172487515, accuracy=1.0\n",
            "Test loss = 0.8479664921760559, Test accuracy = 0.9085503220558167\n",
            "Step=41099 loss=0.00011565694403543602, accuracy=1.0\n",
            "Test loss = 0.8474869728088379, Test accuracy = 0.908984363079071\n",
            "Step=41199 loss=0.00011773354886827291, accuracy=1.0\n",
            "Test loss = 0.8499761819839478, Test accuracy = 0.9079427123069763\n",
            "Step=41299 loss=0.00011402629177609925, accuracy=1.0\n",
            "Test loss = 0.8517327308654785, Test accuracy = 0.9086805582046509\n",
            "Step=41399 loss=0.0001177639584420831, accuracy=1.0\n",
            "Test loss = 0.8534354567527771, Test accuracy = 0.9080728888511658\n",
            "Step=41499 loss=0.00011456292657385347, accuracy=1.0\n",
            "Test loss = 0.8522862792015076, Test accuracy = 0.9092013835906982\n",
            "Step=41599 loss=0.00010491124932741513, accuracy=1.0\n",
            "Test loss = 0.8552674055099487, Test accuracy = 0.9078559279441833\n",
            "Step=41699 loss=0.00010336075287341373, accuracy=1.0\n",
            "Test loss = 0.8586866855621338, Test accuracy = 0.908116340637207\n",
            "Step=41799 loss=0.00010356647717344459, accuracy=1.0\n",
            "Test loss = 0.8572078347206116, Test accuracy = 0.9084635376930237\n",
            "Step=41899 loss=0.00010208931482338812, accuracy=1.0\n",
            "Test loss = 0.8637360334396362, Test accuracy = 0.9087239503860474\n",
            "Step=41999 loss=0.00011060950298997341, accuracy=1.0\n",
            "Test loss = 0.8617852926254272, Test accuracy = 0.9079427123069763\n",
            "Step=42099 loss=9.652827902755235e-05, accuracy=1.0\n",
            "Test loss = 0.8632386326789856, Test accuracy = 0.9084201455116272\n",
            "Step=42199 loss=9.491749540757155e-05, accuracy=1.0\n",
            "Test loss = 0.8695940375328064, Test accuracy = 0.9075520634651184\n",
            "Step=42299 loss=0.0001086746838336694, accuracy=1.0\n",
            "Test loss = 0.8677254319190979, Test accuracy = 0.907421886920929\n",
            "Step=42399 loss=9.14274837487028e-05, accuracy=1.0\n",
            "Test loss = 0.8686264157295227, Test accuracy = 0.9092881679534912\n",
            "Step=42499 loss=8.887873595085694e-05, accuracy=1.0\n",
            "Test loss = 0.87064528465271, Test accuracy = 0.9080728888511658\n",
            "Step=42599 loss=9.193314926960738e-05, accuracy=1.0\n",
            "Test loss = 0.8788413405418396, Test accuracy = 0.9064236283302307\n",
            "Step=42699 loss=0.00010884192262892611, accuracy=1.0\n",
            "Test loss = 0.878160834312439, Test accuracy = 0.9077256917953491\n",
            "Step=42799 loss=8.670410712511511e-05, accuracy=1.0\n",
            "Test loss = 0.8794909119606018, Test accuracy = 0.9080294966697693\n",
            "Step=42899 loss=9.166108317003819e-05, accuracy=1.0\n",
            "Test loss = 0.8781746029853821, Test accuracy = 0.9084635376930237\n",
            "Step=42999 loss=8.4386312028073e-05, accuracy=1.0\n",
            "Test loss = 0.8832242488861084, Test accuracy = 0.9095486402511597\n",
            "Step=43099 loss=8.103860574919963e-05, accuracy=1.0\n",
            "Test loss = 0.8976545333862305, Test accuracy = 0.9048177003860474\n",
            "Step=43199 loss=0.00014130370247585232, accuracy=1.0\n",
            "Test loss = 0.889129638671875, Test accuracy = 0.9100260138511658\n",
            "Step=43299 loss=0.010187394596068771, accuracy=0.996640625\n",
            "Test loss = 0.9353892207145691, Test accuracy = 0.9032118320465088\n",
            "Step=43399 loss=0.0030091669400280807, accuracy=0.999296875\n",
            "Test loss = 0.8944212794303894, Test accuracy = 0.9070746302604675\n",
            "Step=43499 loss=0.0009673146541899769, accuracy=0.999921875\n",
            "Test loss = 0.8914095759391785, Test accuracy = 0.9071614742279053\n",
            "Step=43599 loss=0.00024978240668133367, accuracy=1.0\n",
            "Test loss = 0.8920023441314697, Test accuracy = 0.9082465171813965\n",
            "Step=43699 loss=0.00019219245412386955, accuracy=1.0\n",
            "Test loss = 0.8918370008468628, Test accuracy = 0.908116340637207\n",
            "Step=43799 loss=0.0001512370195632684, accuracy=1.0\n",
            "Test loss = 0.891819953918457, Test accuracy = 0.9075086712837219\n",
            "Step=43899 loss=0.00015501118683459937, accuracy=1.0\n",
            "Test loss = 0.8932839035987854, Test accuracy = 0.9078124761581421\n",
            "Step=43999 loss=0.00013727211986406474, accuracy=1.0\n",
            "Test loss = 0.8939605951309204, Test accuracy = 0.907421886920929\n",
            "Step=44099 loss=0.00013103328670695193, accuracy=1.0\n",
            "Test loss = 0.8928071856498718, Test accuracy = 0.908116340637207\n",
            "Step=44199 loss=0.0001234567338906345, accuracy=1.0\n",
            "Test loss = 0.8936108350753784, Test accuracy = 0.9076822996139526\n",
            "Step=44299 loss=0.010225183756156185, accuracy=0.99984375\n",
            "Test loss = 0.9269212484359741, Test accuracy = 0.9120659828186035\n",
            "Step=44399 loss=0.005467251193440461, accuracy=0.9996875\n",
            "Test loss = 0.8932266235351562, Test accuracy = 0.9086371660232544\n",
            "Step=44499 loss=0.0003020173851109576, accuracy=0.999921875\n",
            "Test loss = 0.892507791519165, Test accuracy = 0.908289909362793\n",
            "Step=44599 loss=0.000178006674250355, accuracy=1.0\n",
            "Test loss = 0.892285168170929, Test accuracy = 0.907421886920929\n",
            "Step=44699 loss=0.00022034520821762271, accuracy=1.0\n",
            "Test loss = 0.8924538493156433, Test accuracy = 0.9094184041023254\n",
            "Step=44799 loss=0.00017569814575836063, accuracy=1.0\n",
            "Test loss = 0.8930178880691528, Test accuracy = 0.9088107347488403\n",
            "Step=44899 loss=0.00014825573100097245, accuracy=1.0\n",
            "Test loss = 0.8916293382644653, Test accuracy = 0.908203125\n",
            "Step=44999 loss=0.00012239936342666623, accuracy=1.0\n",
            "Test loss = 0.8933778405189514, Test accuracy = 0.908116340637207\n",
            "Step=45099 loss=0.00012958040122612146, accuracy=1.0\n",
            "Test loss = 0.8927473425865173, Test accuracy = 0.9083767533302307\n",
            "Step=45199 loss=0.00012114587356336415, accuracy=1.0\n",
            "Test loss = 0.8931856155395508, Test accuracy = 0.908289909362793\n",
            "Step=45299 loss=0.0001182466504178592, accuracy=1.0\n",
            "Test loss = 0.8945370316505432, Test accuracy = 0.9079861044883728\n",
            "Step=45399 loss=0.00012011861690552905, accuracy=1.0\n",
            "Test loss = 0.8936313986778259, Test accuracy = 0.9080294966697693\n",
            "Step=45499 loss=0.00011448202581959777, accuracy=1.0\n",
            "Test loss = 0.8944228291511536, Test accuracy = 0.9078124761581421\n",
            "Step=45599 loss=9.85443727040547e-05, accuracy=1.0\n",
            "Test loss = 0.8954386115074158, Test accuracy = 0.9079427123069763\n",
            "Step=45699 loss=0.0001079028235108126, accuracy=1.0\n",
            "Test loss = 0.8961117267608643, Test accuracy = 0.9081597328186035\n",
            "Step=45799 loss=9.703012423415203e-05, accuracy=1.0\n",
            "Test loss = 0.8972901105880737, Test accuracy = 0.9077690839767456\n",
            "Step=45899 loss=9.26866542067728e-05, accuracy=1.0\n",
            "Test loss = 0.8974283933639526, Test accuracy = 0.9078559279441833\n",
            "Step=45999 loss=9.792463180929189e-05, accuracy=1.0\n",
            "Test loss = 0.8981294631958008, Test accuracy = 0.9080728888511658\n",
            "Step=46099 loss=9.501802884187782e-05, accuracy=1.0\n",
            "Test loss = 0.8982821702957153, Test accuracy = 0.9077256917953491\n",
            "Step=46199 loss=8.895743856555782e-05, accuracy=1.0\n",
            "Test loss = 0.8997526168823242, Test accuracy = 0.9077256917953491\n",
            "Step=46299 loss=9.769577598490287e-05, accuracy=1.0\n",
            "Test loss = 0.8983471393585205, Test accuracy = 0.9079861044883728\n",
            "Step=46399 loss=8.290400957776e-05, accuracy=1.0\n",
            "Test loss = 0.9000739455223083, Test accuracy = 0.9076389074325562\n",
            "Step=46499 loss=8.918212563003181e-05, accuracy=1.0\n",
            "Test loss = 0.8991906642913818, Test accuracy = 0.9077690839767456\n",
            "Step=46599 loss=9.219239305821247e-05, accuracy=1.0\n",
            "Test loss = 0.900928258895874, Test accuracy = 0.9080728888511658\n",
            "Step=46699 loss=8.392629417357966e-05, accuracy=1.0\n",
            "Test loss = 0.9019421339035034, Test accuracy = 0.9080294966697693\n",
            "Step=46799 loss=8.350934367626906e-05, accuracy=1.0\n",
            "Test loss = 0.9008532762527466, Test accuracy = 0.9076822996139526\n",
            "Step=46899 loss=8.505419669745607e-05, accuracy=1.0\n",
            "Test loss = 0.9014914631843567, Test accuracy = 0.9080728888511658\n",
            "Step=46999 loss=8.102399555355077e-05, accuracy=1.0\n",
            "Test loss = 0.9038561582565308, Test accuracy = 0.907421886920929\n",
            "Step=47099 loss=7.40539842445287e-05, accuracy=1.0\n",
            "Test loss = 0.9037361741065979, Test accuracy = 0.908116340637207\n",
            "Step=47199 loss=7.751526674837806e-05, accuracy=1.0\n",
            "Test loss = 0.9044896364212036, Test accuracy = 0.9080728888511658\n",
            "Step=47299 loss=7.917461665783776e-05, accuracy=1.0\n",
            "Test loss = 0.9049174189567566, Test accuracy = 0.9076822996139526\n",
            "Step=47399 loss=7.62517162002041e-05, accuracy=1.0\n",
            "Test loss = 0.9066489338874817, Test accuracy = 0.907421886920929\n",
            "Step=47499 loss=7.263655272254254e-05, accuracy=1.0\n",
            "Test loss = 0.9049462080001831, Test accuracy = 0.9083767533302307\n",
            "Step=47599 loss=7.621797529282048e-05, accuracy=1.0\n",
            "Test loss = 0.9066611528396606, Test accuracy = 0.9079427123069763\n",
            "Step=47699 loss=7.085349559929455e-05, accuracy=1.0\n",
            "Test loss = 0.9077282547950745, Test accuracy = 0.9079861044883728\n",
            "Step=47799 loss=6.678980180367944e-05, accuracy=1.0\n",
            "Test loss = 0.9077037572860718, Test accuracy = 0.908116340637207\n",
            "Step=47899 loss=7.105998420229299e-05, accuracy=1.0\n",
            "Test loss = 0.9092764854431152, Test accuracy = 0.9077256917953491\n",
            "Step=47999 loss=7.104977998096728e-05, accuracy=1.0\n",
            "Test loss = 0.9084233641624451, Test accuracy = 0.9080294966697693\n",
            "Step=48099 loss=6.816214918217156e-05, accuracy=1.0\n",
            "Test loss = 0.9095445275306702, Test accuracy = 0.9077256917953491\n",
            "Step=48199 loss=7.178985668360838e-05, accuracy=1.0\n",
            "Test loss = 0.9086446166038513, Test accuracy = 0.908289909362793\n",
            "Step=48299 loss=7.054678389977198e-05, accuracy=1.0\n",
            "Test loss = 0.911820113658905, Test accuracy = 0.908289909362793\n",
            "Step=48399 loss=6.658106332906754e-05, accuracy=1.0\n",
            "Test loss = 0.9115815162658691, Test accuracy = 0.9083767533302307\n",
            "Step=48499 loss=6.735090912115993e-05, accuracy=1.0\n",
            "Test loss = 0.9150875806808472, Test accuracy = 0.9075086712837219\n",
            "Step=48599 loss=6.923605787960696e-05, accuracy=1.0\n",
            "Test loss = 0.9148408770561218, Test accuracy = 0.9085937738418579\n",
            "Step=48699 loss=6.572449432496796e-05, accuracy=1.0\n",
            "Test loss = 0.913974940776825, Test accuracy = 0.9080294966697693\n",
            "Step=48799 loss=6.31360721308738e-05, accuracy=1.0\n",
            "Test loss = 0.9159849286079407, Test accuracy = 0.908116340637207\n",
            "Step=48899 loss=6.379374157404527e-05, accuracy=1.0\n",
            "Test loss = 0.9151633977890015, Test accuracy = 0.9095051884651184\n",
            "Step=48999 loss=6.321174794720718e-05, accuracy=1.0\n",
            "Test loss = 0.9175878167152405, Test accuracy = 0.9081597328186035\n",
            "Step=49099 loss=6.093152325775009e-05, accuracy=1.0\n",
            "Test loss = 0.9176400303840637, Test accuracy = 0.9093316197395325\n",
            "Step=49199 loss=6.099150981754065e-05, accuracy=1.0\n",
            "Test loss = 0.9198790788650513, Test accuracy = 0.9088107347488403\n",
            "Step=49299 loss=6.159075903269695e-05, accuracy=1.0\n",
            "Test loss = 0.920593798160553, Test accuracy = 0.9080728888511658\n",
            "Step=49399 loss=5.5857934876257785e-05, accuracy=1.0\n",
            "Test loss = 0.9213657975196838, Test accuracy = 0.908289909362793\n",
            "Step=49499 loss=5.627998398267664e-05, accuracy=1.0\n",
            "Test loss = 0.9215826988220215, Test accuracy = 0.908203125\n",
            "Step=49599 loss=5.901346570681199e-05, accuracy=1.0\n",
            "Test loss = 0.9248974919319153, Test accuracy = 0.9078559279441833\n",
            "Step=49699 loss=5.435945589852054e-05, accuracy=1.0\n",
            "Test loss = 0.9272972941398621, Test accuracy = 0.9079427123069763\n",
            "Step=49799 loss=5.0663373458519344e-05, accuracy=1.0\n",
            "Test loss = 0.9252046942710876, Test accuracy = 0.9090277552604675\n",
            "Step=49899 loss=5.1445049612084406e-05, accuracy=1.0\n",
            "Test loss = 0.9267018437385559, Test accuracy = 0.9083767533302307\n",
            "Step=49999 loss=5.4732325443183074e-05, accuracy=1.0\n",
            "Test loss = 0.9257659912109375, Test accuracy = 0.908984363079071\n",
            "Reached 50001 epochs for CNN\n",
            "[[ 1500  1053]\n",
            " [ 1044 19443]]\n",
            "Normalized confusion matrix\n",
            "[[0.58754407 0.41245593]\n",
            " [0.05095914 0.94904086]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9W2wtvdwuJV"
      },
      "source": [
        "\n",
        "# Train XGB Boost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ddo_A9wt9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf39bc0f-c376-4b0a-ee7c-af3a378a938e"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import xgboost as xgb\n",
        "#from sklearn.externals import joblib\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class TrainXGBBoost:\n",
        "\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "        \n",
        "        assert os.path.exists(f'{googlepath}models/checkpoint')\n",
        "        gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200, is_train=False)\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver = tf.train.Saver()\n",
        "            if os.path.exists(f'{googlepath}models/checkpoint'):\n",
        "                    \n",
        "                    with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "                        model_name = next(f).split('\"'.encode())[1]\n",
        "                    model_name_string = model_name.decode()\n",
        "                    model_name_string = model_name_string.replace(\"//\", \"/\")\n",
        "                    #filename = \"{}models/{}\".format(googlepath, model_name_string )\n",
        "                    currentStep = model_name_string.split(\"-\")[1]\n",
        "                    new_saver = tf.train.import_meta_graph('{}.meta'.format(model_name_string))\n",
        "                    new_saver.restore(sess, \"{}\".format(model_name_string))\n",
        "            files = [os.path.join(f'{googlepath}stock_data', f) for f in os.listdir(f'{googlepath}/stock_data')]\n",
        "            for file in files:\n",
        "                print(file)\n",
        "                #Read in file -- note that parse_dates will be need later\n",
        "                df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "                df = df[['open','high','low','close','volume']]\n",
        "\n",
        "                #Normilize using a of size num_historical_days\n",
        "                labels = df.close.pct_change(days).map(lambda x: int(x > pct_change/100.0))\n",
        "                df = ((df -\n",
        "                df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "                /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "                -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "                df['labels'] = labels\n",
        "\n",
        "                df = df.dropna()\n",
        "\n",
        "                #Hold out the testing data\n",
        "                test_df = df[:500]\n",
        "                df = df[500:]\n",
        "\n",
        "                data = df[['open','high','low','close','volume']].values\n",
        "                labels = df['labels'].values\n",
        "                for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.data.append(features[0])\n",
        "#                     print(features[0])\n",
        "                    self.labels.append(labels[i-1])\n",
        "                data = test_df[['open','high','low','close','volume']].values\n",
        "                labels = test_df['labels'].values\n",
        "                for i in range(num_historical_days, len(test_df), 1):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.test_data.append(features[0])\n",
        "                    self.test_labels.append(labels[i-1])\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        params = {}\n",
        "        params['objective'] = 'multi:softprob'\n",
        "        params['eta'] = 0.01\n",
        "        params['num_class'] = 2\n",
        "        params['max_depth'] = 20\n",
        "        params['subsample'] = 0.05\n",
        "        params['colsample_bytree'] = 0.05\n",
        "        params['eval_metric'] = 'mlogloss'\n",
        "\n",
        "        train = xgb.DMatrix(self.data, self.labels)\n",
        "        test = xgb.DMatrix(self.test_data, self.test_labels)\n",
        "\n",
        "        watchlist = [(train, 'train'), (test, 'test')]\n",
        "        clf = xgb.train(params, train, 1000, evals=watchlist, early_stopping_rounds=100)\n",
        "        joblib.dump(clf, f'{googlepath}models/clf.pkl')\n",
        "        cm = confusion_matrix(self.test_labels, list(map(lambda x: int(x[1] > .5), clf.predict(test))))\n",
        "        print(cm)\n",
        "        Accuracy = accuracy_score(self.test_labels, list(map(lambda x: int(x[1] > .5), clf.predict(test))))\n",
        "        Precision = precision_score(self.test_labels, list(map(lambda x: int(x[1] > .5), clf.predict(test))))\n",
        "        Sensitivity_recall = recall_score(self.test_labels, list(map(lambda x: int(x[1] > .5), clf.predict(test))))\n",
        "        Specificity = recall_score(self.test_labels, list(map(lambda x: int(x[1] > .5), clf.predict(test))), pos_label=0)\n",
        "        F1_score = f1_score(self.test_labels, list(map(lambda x: int(x[1] > .5), clf.predict(test))))\n",
        "        print({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})\n",
        "        plot_confusion_matrix(cm, ['Down', 'Up'], normalize=True, title=\"Confusion Matrix\")\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "boost_model = TrainXGBBoost(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\n",
        "boost_model.train()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/MMM.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AOS.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABBV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ACN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ATVI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AYI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ADBE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMD.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AES.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AET.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AFL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/A.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APD.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AKAM.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALK.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALB.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ARE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALXN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALLE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/LNT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ALL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/GOOGL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/GOOG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/MO.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMZN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AEE.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAL.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AEP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AXP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AIG.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMT.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AWK.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMP.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ABC.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AME.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AMGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APH.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ADI.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ANDV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/ANSS.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AON.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/APA.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AIV.csv\n",
            "/content/drive/MyDrive/M.Tech Project/Code/23rdDec2022/stock_data/AAPL.csv\n",
            "[0]\ttrain-mlogloss:0.686378\ttest-mlogloss:0.686564\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 100 rounds.\n",
            "[1]\ttrain-mlogloss:0.679796\ttest-mlogloss:0.680082\n",
            "[2]\ttrain-mlogloss:0.673537\ttest-mlogloss:0.673999\n",
            "[3]\ttrain-mlogloss:0.667147\ttest-mlogloss:0.667762\n",
            "[4]\ttrain-mlogloss:0.660966\ttest-mlogloss:0.661736\n",
            "[5]\ttrain-mlogloss:0.654819\ttest-mlogloss:0.655768\n",
            "[6]\ttrain-mlogloss:0.648764\ttest-mlogloss:0.649848\n",
            "[7]\ttrain-mlogloss:0.642667\ttest-mlogloss:0.643864\n",
            "[8]\ttrain-mlogloss:0.636766\ttest-mlogloss:0.638045\n",
            "[9]\ttrain-mlogloss:0.630887\ttest-mlogloss:0.63231\n",
            "[10]\ttrain-mlogloss:0.625604\ttest-mlogloss:0.627146\n",
            "[11]\ttrain-mlogloss:0.619977\ttest-mlogloss:0.621614\n",
            "[12]\ttrain-mlogloss:0.614647\ttest-mlogloss:0.616429\n",
            "[13]\ttrain-mlogloss:0.609015\ttest-mlogloss:0.610891\n",
            "[14]\ttrain-mlogloss:0.603867\ttest-mlogloss:0.606007\n",
            "[15]\ttrain-mlogloss:0.598587\ttest-mlogloss:0.600805\n",
            "[16]\ttrain-mlogloss:0.593472\ttest-mlogloss:0.595836\n",
            "[17]\ttrain-mlogloss:0.588404\ttest-mlogloss:0.590844\n",
            "[18]\ttrain-mlogloss:0.583567\ttest-mlogloss:0.586032\n",
            "[19]\ttrain-mlogloss:0.578631\ttest-mlogloss:0.581187\n",
            "[20]\ttrain-mlogloss:0.573789\ttest-mlogloss:0.576398\n",
            "[21]\ttrain-mlogloss:0.569132\ttest-mlogloss:0.571801\n",
            "[22]\ttrain-mlogloss:0.564497\ttest-mlogloss:0.567212\n",
            "[23]\ttrain-mlogloss:0.559872\ttest-mlogloss:0.562706\n",
            "[24]\ttrain-mlogloss:0.555577\ttest-mlogloss:0.558486\n",
            "[25]\ttrain-mlogloss:0.5511\ttest-mlogloss:0.554128\n",
            "[26]\ttrain-mlogloss:0.54658\ttest-mlogloss:0.549639\n",
            "[27]\ttrain-mlogloss:0.542154\ttest-mlogloss:0.545231\n",
            "[28]\ttrain-mlogloss:0.537961\ttest-mlogloss:0.541079\n",
            "[29]\ttrain-mlogloss:0.533766\ttest-mlogloss:0.536922\n",
            "[30]\ttrain-mlogloss:0.52949\ttest-mlogloss:0.532651\n",
            "[31]\ttrain-mlogloss:0.525403\ttest-mlogloss:0.52863\n",
            "[32]\ttrain-mlogloss:0.52155\ttest-mlogloss:0.524798\n",
            "[33]\ttrain-mlogloss:0.517742\ttest-mlogloss:0.521099\n",
            "[34]\ttrain-mlogloss:0.513896\ttest-mlogloss:0.517385\n",
            "[35]\ttrain-mlogloss:0.51013\ttest-mlogloss:0.513703\n",
            "[36]\ttrain-mlogloss:0.506201\ttest-mlogloss:0.509853\n",
            "[37]\ttrain-mlogloss:0.50261\ttest-mlogloss:0.50635\n",
            "[38]\ttrain-mlogloss:0.498852\ttest-mlogloss:0.502676\n",
            "[39]\ttrain-mlogloss:0.495088\ttest-mlogloss:0.498928\n",
            "[40]\ttrain-mlogloss:0.491605\ttest-mlogloss:0.4955\n",
            "[41]\ttrain-mlogloss:0.488088\ttest-mlogloss:0.492018\n",
            "[42]\ttrain-mlogloss:0.484836\ttest-mlogloss:0.488818\n",
            "[43]\ttrain-mlogloss:0.481438\ttest-mlogloss:0.485482\n",
            "[44]\ttrain-mlogloss:0.478012\ttest-mlogloss:0.482054\n",
            "[45]\ttrain-mlogloss:0.47483\ttest-mlogloss:0.478974\n",
            "[46]\ttrain-mlogloss:0.471583\ttest-mlogloss:0.475786\n",
            "[47]\ttrain-mlogloss:0.468403\ttest-mlogloss:0.472665\n",
            "[48]\ttrain-mlogloss:0.465242\ttest-mlogloss:0.469568\n",
            "[49]\ttrain-mlogloss:0.462234\ttest-mlogloss:0.466631\n",
            "[50]\ttrain-mlogloss:0.459244\ttest-mlogloss:0.463701\n",
            "[51]\ttrain-mlogloss:0.456196\ttest-mlogloss:0.460633\n",
            "[52]\ttrain-mlogloss:0.453167\ttest-mlogloss:0.457711\n",
            "[53]\ttrain-mlogloss:0.450352\ttest-mlogloss:0.454986\n",
            "[54]\ttrain-mlogloss:0.447552\ttest-mlogloss:0.452215\n",
            "[55]\ttrain-mlogloss:0.444694\ttest-mlogloss:0.44949\n",
            "[56]\ttrain-mlogloss:0.441738\ttest-mlogloss:0.446574\n",
            "[57]\ttrain-mlogloss:0.438792\ttest-mlogloss:0.443634\n",
            "[58]\ttrain-mlogloss:0.436123\ttest-mlogloss:0.440949\n",
            "[59]\ttrain-mlogloss:0.433537\ttest-mlogloss:0.438416\n",
            "[60]\ttrain-mlogloss:0.430773\ttest-mlogloss:0.435737\n",
            "[61]\ttrain-mlogloss:0.428068\ttest-mlogloss:0.433077\n",
            "[62]\ttrain-mlogloss:0.425304\ttest-mlogloss:0.430311\n",
            "[63]\ttrain-mlogloss:0.422728\ttest-mlogloss:0.427742\n",
            "[64]\ttrain-mlogloss:0.420239\ttest-mlogloss:0.425264\n",
            "[65]\ttrain-mlogloss:0.417733\ttest-mlogloss:0.42281\n",
            "[66]\ttrain-mlogloss:0.415281\ttest-mlogloss:0.420437\n",
            "[67]\ttrain-mlogloss:0.412782\ttest-mlogloss:0.41795\n",
            "[68]\ttrain-mlogloss:0.410411\ttest-mlogloss:0.415592\n",
            "[69]\ttrain-mlogloss:0.40798\ttest-mlogloss:0.41318\n",
            "[70]\ttrain-mlogloss:0.405689\ttest-mlogloss:0.410914\n",
            "[71]\ttrain-mlogloss:0.403303\ttest-mlogloss:0.408542\n",
            "[72]\ttrain-mlogloss:0.401019\ttest-mlogloss:0.406288\n",
            "[73]\ttrain-mlogloss:0.398715\ttest-mlogloss:0.404003\n",
            "[74]\ttrain-mlogloss:0.396359\ttest-mlogloss:0.401686\n",
            "[75]\ttrain-mlogloss:0.394125\ttest-mlogloss:0.399519\n",
            "[76]\ttrain-mlogloss:0.391946\ttest-mlogloss:0.397366\n",
            "[77]\ttrain-mlogloss:0.389869\ttest-mlogloss:0.395292\n",
            "[78]\ttrain-mlogloss:0.387896\ttest-mlogloss:0.393433\n",
            "[79]\ttrain-mlogloss:0.385907\ttest-mlogloss:0.391408\n",
            "[80]\ttrain-mlogloss:0.383826\ttest-mlogloss:0.389323\n",
            "[81]\ttrain-mlogloss:0.381829\ttest-mlogloss:0.38734\n",
            "[82]\ttrain-mlogloss:0.37975\ttest-mlogloss:0.385228\n",
            "[83]\ttrain-mlogloss:0.377845\ttest-mlogloss:0.383372\n",
            "[84]\ttrain-mlogloss:0.375856\ttest-mlogloss:0.381388\n",
            "[85]\ttrain-mlogloss:0.373986\ttest-mlogloss:0.379656\n",
            "[86]\ttrain-mlogloss:0.371945\ttest-mlogloss:0.377651\n",
            "[87]\ttrain-mlogloss:0.370051\ttest-mlogloss:0.375811\n",
            "[88]\ttrain-mlogloss:0.368337\ttest-mlogloss:0.374132\n",
            "[89]\ttrain-mlogloss:0.366503\ttest-mlogloss:0.372252\n",
            "[90]\ttrain-mlogloss:0.364741\ttest-mlogloss:0.370494\n",
            "[91]\ttrain-mlogloss:0.362951\ttest-mlogloss:0.36877\n",
            "[92]\ttrain-mlogloss:0.361195\ttest-mlogloss:0.367097\n",
            "[93]\ttrain-mlogloss:0.359426\ttest-mlogloss:0.365424\n",
            "[94]\ttrain-mlogloss:0.357759\ttest-mlogloss:0.363808\n",
            "[95]\ttrain-mlogloss:0.356099\ttest-mlogloss:0.362112\n",
            "[96]\ttrain-mlogloss:0.354543\ttest-mlogloss:0.36063\n",
            "[97]\ttrain-mlogloss:0.353005\ttest-mlogloss:0.359121\n",
            "[98]\ttrain-mlogloss:0.351315\ttest-mlogloss:0.357477\n",
            "[99]\ttrain-mlogloss:0.34962\ttest-mlogloss:0.355769\n",
            "[100]\ttrain-mlogloss:0.347999\ttest-mlogloss:0.35414\n",
            "[101]\ttrain-mlogloss:0.346421\ttest-mlogloss:0.352601\n",
            "[102]\ttrain-mlogloss:0.344823\ttest-mlogloss:0.351096\n",
            "[103]\ttrain-mlogloss:0.343394\ttest-mlogloss:0.349739\n",
            "[104]\ttrain-mlogloss:0.341769\ttest-mlogloss:0.348084\n",
            "[105]\ttrain-mlogloss:0.340233\ttest-mlogloss:0.346609\n",
            "[106]\ttrain-mlogloss:0.338594\ttest-mlogloss:0.34502\n",
            "[107]\ttrain-mlogloss:0.337102\ttest-mlogloss:0.343531\n",
            "[108]\ttrain-mlogloss:0.335633\ttest-mlogloss:0.342161\n",
            "[109]\ttrain-mlogloss:0.334166\ttest-mlogloss:0.340688\n",
            "[110]\ttrain-mlogloss:0.332804\ttest-mlogloss:0.339379\n",
            "[111]\ttrain-mlogloss:0.331364\ttest-mlogloss:0.338046\n",
            "[112]\ttrain-mlogloss:0.330008\ttest-mlogloss:0.336693\n",
            "[113]\ttrain-mlogloss:0.328482\ttest-mlogloss:0.335162\n",
            "[114]\ttrain-mlogloss:0.327015\ttest-mlogloss:0.333647\n",
            "[115]\ttrain-mlogloss:0.325822\ttest-mlogloss:0.332466\n",
            "[116]\ttrain-mlogloss:0.324469\ttest-mlogloss:0.331119\n",
            "[117]\ttrain-mlogloss:0.323096\ttest-mlogloss:0.329726\n",
            "[118]\ttrain-mlogloss:0.321759\ttest-mlogloss:0.328358\n",
            "[119]\ttrain-mlogloss:0.320545\ttest-mlogloss:0.327254\n",
            "[120]\ttrain-mlogloss:0.319256\ttest-mlogloss:0.326024\n",
            "[121]\ttrain-mlogloss:0.317989\ttest-mlogloss:0.324755\n",
            "[122]\ttrain-mlogloss:0.316701\ttest-mlogloss:0.3235\n",
            "[123]\ttrain-mlogloss:0.315436\ttest-mlogloss:0.322289\n",
            "[124]\ttrain-mlogloss:0.314249\ttest-mlogloss:0.321099\n",
            "[125]\ttrain-mlogloss:0.313035\ttest-mlogloss:0.319907\n",
            "[126]\ttrain-mlogloss:0.311906\ttest-mlogloss:0.318796\n",
            "[127]\ttrain-mlogloss:0.310678\ttest-mlogloss:0.317557\n",
            "[128]\ttrain-mlogloss:0.30952\ttest-mlogloss:0.316434\n",
            "[129]\ttrain-mlogloss:0.308234\ttest-mlogloss:0.315195\n",
            "[130]\ttrain-mlogloss:0.307061\ttest-mlogloss:0.31409\n",
            "[131]\ttrain-mlogloss:0.305837\ttest-mlogloss:0.312944\n",
            "[132]\ttrain-mlogloss:0.304661\ttest-mlogloss:0.311765\n",
            "[133]\ttrain-mlogloss:0.303546\ttest-mlogloss:0.310623\n",
            "[134]\ttrain-mlogloss:0.302379\ttest-mlogloss:0.309458\n",
            "[135]\ttrain-mlogloss:0.301205\ttest-mlogloss:0.308294\n",
            "[136]\ttrain-mlogloss:0.300033\ttest-mlogloss:0.307175\n",
            "[137]\ttrain-mlogloss:0.299031\ttest-mlogloss:0.306175\n",
            "[138]\ttrain-mlogloss:0.298016\ttest-mlogloss:0.305163\n",
            "[139]\ttrain-mlogloss:0.29695\ttest-mlogloss:0.304133\n",
            "[140]\ttrain-mlogloss:0.295927\ttest-mlogloss:0.303167\n",
            "[141]\ttrain-mlogloss:0.294832\ttest-mlogloss:0.302069\n",
            "[142]\ttrain-mlogloss:0.293838\ttest-mlogloss:0.301123\n",
            "[143]\ttrain-mlogloss:0.292832\ttest-mlogloss:0.300251\n",
            "[144]\ttrain-mlogloss:0.291864\ttest-mlogloss:0.299321\n",
            "[145]\ttrain-mlogloss:0.290788\ttest-mlogloss:0.298276\n",
            "[146]\ttrain-mlogloss:0.2898\ttest-mlogloss:0.297317\n",
            "[147]\ttrain-mlogloss:0.288857\ttest-mlogloss:0.296355\n",
            "[148]\ttrain-mlogloss:0.287867\ttest-mlogloss:0.295391\n",
            "[149]\ttrain-mlogloss:0.286958\ttest-mlogloss:0.294495\n",
            "[150]\ttrain-mlogloss:0.285955\ttest-mlogloss:0.293505\n",
            "[151]\ttrain-mlogloss:0.284983\ttest-mlogloss:0.292506\n",
            "[152]\ttrain-mlogloss:0.284036\ttest-mlogloss:0.291608\n",
            "[153]\ttrain-mlogloss:0.283098\ttest-mlogloss:0.290699\n",
            "[154]\ttrain-mlogloss:0.282224\ttest-mlogloss:0.289846\n",
            "[155]\ttrain-mlogloss:0.28129\ttest-mlogloss:0.288892\n",
            "[156]\ttrain-mlogloss:0.280436\ttest-mlogloss:0.288041\n",
            "[157]\ttrain-mlogloss:0.279512\ttest-mlogloss:0.287143\n",
            "[158]\ttrain-mlogloss:0.278632\ttest-mlogloss:0.286297\n",
            "[159]\ttrain-mlogloss:0.277918\ttest-mlogloss:0.285641\n",
            "[160]\ttrain-mlogloss:0.276988\ttest-mlogloss:0.28473\n",
            "[161]\ttrain-mlogloss:0.276199\ttest-mlogloss:0.283868\n",
            "[162]\ttrain-mlogloss:0.275409\ttest-mlogloss:0.283065\n",
            "[163]\ttrain-mlogloss:0.274697\ttest-mlogloss:0.282434\n",
            "[164]\ttrain-mlogloss:0.273914\ttest-mlogloss:0.281691\n",
            "[165]\ttrain-mlogloss:0.273127\ttest-mlogloss:0.280916\n",
            "[166]\ttrain-mlogloss:0.27229\ttest-mlogloss:0.280078\n",
            "[167]\ttrain-mlogloss:0.271583\ttest-mlogloss:0.27936\n",
            "[168]\ttrain-mlogloss:0.270815\ttest-mlogloss:0.278584\n",
            "[169]\ttrain-mlogloss:0.270102\ttest-mlogloss:0.277926\n",
            "[170]\ttrain-mlogloss:0.269419\ttest-mlogloss:0.277253\n",
            "[171]\ttrain-mlogloss:0.268684\ttest-mlogloss:0.276558\n",
            "[172]\ttrain-mlogloss:0.268002\ttest-mlogloss:0.275804\n",
            "[173]\ttrain-mlogloss:0.267296\ttest-mlogloss:0.275114\n",
            "[174]\ttrain-mlogloss:0.266635\ttest-mlogloss:0.274453\n",
            "[175]\ttrain-mlogloss:0.26594\ttest-mlogloss:0.273833\n",
            "[176]\ttrain-mlogloss:0.265097\ttest-mlogloss:0.27298\n",
            "[177]\ttrain-mlogloss:0.264463\ttest-mlogloss:0.27242\n",
            "[178]\ttrain-mlogloss:0.263729\ttest-mlogloss:0.271723\n",
            "[179]\ttrain-mlogloss:0.262942\ttest-mlogloss:0.270979\n",
            "[180]\ttrain-mlogloss:0.262226\ttest-mlogloss:0.270278\n",
            "[181]\ttrain-mlogloss:0.261506\ttest-mlogloss:0.269573\n",
            "[182]\ttrain-mlogloss:0.260795\ttest-mlogloss:0.268902\n",
            "[183]\ttrain-mlogloss:0.260166\ttest-mlogloss:0.26826\n",
            "[184]\ttrain-mlogloss:0.259486\ttest-mlogloss:0.267575\n",
            "[185]\ttrain-mlogloss:0.258745\ttest-mlogloss:0.266906\n",
            "[186]\ttrain-mlogloss:0.258131\ttest-mlogloss:0.266281\n",
            "[187]\ttrain-mlogloss:0.257594\ttest-mlogloss:0.265755\n",
            "[188]\ttrain-mlogloss:0.256979\ttest-mlogloss:0.265158\n",
            "[189]\ttrain-mlogloss:0.256409\ttest-mlogloss:0.264579\n",
            "[190]\ttrain-mlogloss:0.255771\ttest-mlogloss:0.263936\n",
            "[191]\ttrain-mlogloss:0.255126\ttest-mlogloss:0.263368\n",
            "[192]\ttrain-mlogloss:0.254505\ttest-mlogloss:0.262795\n",
            "[193]\ttrain-mlogloss:0.253927\ttest-mlogloss:0.262291\n",
            "[194]\ttrain-mlogloss:0.253309\ttest-mlogloss:0.261696\n",
            "[195]\ttrain-mlogloss:0.252708\ttest-mlogloss:0.26112\n",
            "[196]\ttrain-mlogloss:0.25205\ttest-mlogloss:0.260494\n",
            "[197]\ttrain-mlogloss:0.251508\ttest-mlogloss:0.25996\n",
            "[198]\ttrain-mlogloss:0.250964\ttest-mlogloss:0.259415\n",
            "[199]\ttrain-mlogloss:0.250436\ttest-mlogloss:0.258902\n",
            "[200]\ttrain-mlogloss:0.249875\ttest-mlogloss:0.258366\n",
            "[201]\ttrain-mlogloss:0.249274\ttest-mlogloss:0.257831\n",
            "[202]\ttrain-mlogloss:0.248751\ttest-mlogloss:0.25735\n",
            "[203]\ttrain-mlogloss:0.248205\ttest-mlogloss:0.256812\n",
            "[204]\ttrain-mlogloss:0.247656\ttest-mlogloss:0.256279\n",
            "[205]\ttrain-mlogloss:0.247112\ttest-mlogloss:0.255776\n",
            "[206]\ttrain-mlogloss:0.246643\ttest-mlogloss:0.255326\n",
            "[207]\ttrain-mlogloss:0.246118\ttest-mlogloss:0.254855\n",
            "[208]\ttrain-mlogloss:0.245565\ttest-mlogloss:0.254329\n",
            "[209]\ttrain-mlogloss:0.245056\ttest-mlogloss:0.253804\n",
            "[210]\ttrain-mlogloss:0.244636\ttest-mlogloss:0.25346\n",
            "[211]\ttrain-mlogloss:0.244155\ttest-mlogloss:0.252985\n",
            "[212]\ttrain-mlogloss:0.243626\ttest-mlogloss:0.252429\n",
            "[213]\ttrain-mlogloss:0.243184\ttest-mlogloss:0.252\n",
            "[214]\ttrain-mlogloss:0.242712\ttest-mlogloss:0.251543\n",
            "[215]\ttrain-mlogloss:0.242197\ttest-mlogloss:0.251094\n",
            "[216]\ttrain-mlogloss:0.2417\ttest-mlogloss:0.250629\n",
            "[217]\ttrain-mlogloss:0.241301\ttest-mlogloss:0.250264\n",
            "[218]\ttrain-mlogloss:0.240914\ttest-mlogloss:0.249964\n",
            "[219]\ttrain-mlogloss:0.240409\ttest-mlogloss:0.249484\n",
            "[220]\ttrain-mlogloss:0.239975\ttest-mlogloss:0.249068\n",
            "[221]\ttrain-mlogloss:0.239585\ttest-mlogloss:0.248706\n",
            "[222]\ttrain-mlogloss:0.23918\ttest-mlogloss:0.24832\n",
            "[223]\ttrain-mlogloss:0.238721\ttest-mlogloss:0.247888\n",
            "[224]\ttrain-mlogloss:0.238288\ttest-mlogloss:0.247552\n",
            "[225]\ttrain-mlogloss:0.237837\ttest-mlogloss:0.247155\n",
            "[226]\ttrain-mlogloss:0.237327\ttest-mlogloss:0.246725\n",
            "[227]\ttrain-mlogloss:0.236949\ttest-mlogloss:0.246397\n",
            "[228]\ttrain-mlogloss:0.236553\ttest-mlogloss:0.246019\n",
            "[229]\ttrain-mlogloss:0.23618\ttest-mlogloss:0.245658\n",
            "[230]\ttrain-mlogloss:0.235759\ttest-mlogloss:0.24527\n",
            "[231]\ttrain-mlogloss:0.235322\ttest-mlogloss:0.24489\n",
            "[232]\ttrain-mlogloss:0.234894\ttest-mlogloss:0.244455\n",
            "[233]\ttrain-mlogloss:0.23443\ttest-mlogloss:0.244039\n",
            "[234]\ttrain-mlogloss:0.233983\ttest-mlogloss:0.24361\n",
            "[235]\ttrain-mlogloss:0.233558\ttest-mlogloss:0.243199\n",
            "[236]\ttrain-mlogloss:0.233155\ttest-mlogloss:0.242794\n",
            "[237]\ttrain-mlogloss:0.232739\ttest-mlogloss:0.242358\n",
            "[238]\ttrain-mlogloss:0.232307\ttest-mlogloss:0.242015\n",
            "[239]\ttrain-mlogloss:0.231857\ttest-mlogloss:0.241576\n",
            "[240]\ttrain-mlogloss:0.231378\ttest-mlogloss:0.241144\n",
            "[241]\ttrain-mlogloss:0.231036\ttest-mlogloss:0.240841\n",
            "[242]\ttrain-mlogloss:0.230729\ttest-mlogloss:0.240571\n",
            "[243]\ttrain-mlogloss:0.230346\ttest-mlogloss:0.240233\n",
            "[244]\ttrain-mlogloss:0.229974\ttest-mlogloss:0.239827\n",
            "[245]\ttrain-mlogloss:0.229614\ttest-mlogloss:0.239485\n",
            "[246]\ttrain-mlogloss:0.229261\ttest-mlogloss:0.239127\n",
            "[247]\ttrain-mlogloss:0.228894\ttest-mlogloss:0.238828\n",
            "[248]\ttrain-mlogloss:0.228518\ttest-mlogloss:0.238505\n",
            "[249]\ttrain-mlogloss:0.228158\ttest-mlogloss:0.238207\n",
            "[250]\ttrain-mlogloss:0.227749\ttest-mlogloss:0.237869\n",
            "[251]\ttrain-mlogloss:0.227366\ttest-mlogloss:0.237537\n",
            "[252]\ttrain-mlogloss:0.227077\ttest-mlogloss:0.237266\n",
            "[253]\ttrain-mlogloss:0.22665\ttest-mlogloss:0.236857\n",
            "[254]\ttrain-mlogloss:0.226258\ttest-mlogloss:0.236521\n",
            "[255]\ttrain-mlogloss:0.225948\ttest-mlogloss:0.236296\n",
            "[256]\ttrain-mlogloss:0.225605\ttest-mlogloss:0.236018\n",
            "[257]\ttrain-mlogloss:0.225294\ttest-mlogloss:0.235784\n",
            "[258]\ttrain-mlogloss:0.224972\ttest-mlogloss:0.235513\n",
            "[259]\ttrain-mlogloss:0.224712\ttest-mlogloss:0.235272\n",
            "[260]\ttrain-mlogloss:0.224395\ttest-mlogloss:0.234928\n",
            "[261]\ttrain-mlogloss:0.22404\ttest-mlogloss:0.234618\n",
            "[262]\ttrain-mlogloss:0.223728\ttest-mlogloss:0.234335\n",
            "[263]\ttrain-mlogloss:0.223405\ttest-mlogloss:0.234114\n",
            "[264]\ttrain-mlogloss:0.223016\ttest-mlogloss:0.23385\n",
            "[265]\ttrain-mlogloss:0.222765\ttest-mlogloss:0.233596\n",
            "[266]\ttrain-mlogloss:0.222469\ttest-mlogloss:0.233277\n",
            "[267]\ttrain-mlogloss:0.22215\ttest-mlogloss:0.233014\n",
            "[268]\ttrain-mlogloss:0.221835\ttest-mlogloss:0.232738\n",
            "[269]\ttrain-mlogloss:0.221561\ttest-mlogloss:0.23254\n",
            "[270]\ttrain-mlogloss:0.221286\ttest-mlogloss:0.232284\n",
            "[271]\ttrain-mlogloss:0.220984\ttest-mlogloss:0.232017\n",
            "[272]\ttrain-mlogloss:0.220741\ttest-mlogloss:0.231813\n",
            "[273]\ttrain-mlogloss:0.22046\ttest-mlogloss:0.231557\n",
            "[274]\ttrain-mlogloss:0.220194\ttest-mlogloss:0.231346\n",
            "[275]\ttrain-mlogloss:0.219879\ttest-mlogloss:0.231056\n",
            "[276]\ttrain-mlogloss:0.21958\ttest-mlogloss:0.230738\n",
            "[277]\ttrain-mlogloss:0.219309\ttest-mlogloss:0.230559\n",
            "[278]\ttrain-mlogloss:0.219011\ttest-mlogloss:0.23032\n",
            "[279]\ttrain-mlogloss:0.218766\ttest-mlogloss:0.230102\n",
            "[280]\ttrain-mlogloss:0.218502\ttest-mlogloss:0.229841\n",
            "[281]\ttrain-mlogloss:0.218271\ttest-mlogloss:0.229675\n",
            "[282]\ttrain-mlogloss:0.217962\ttest-mlogloss:0.229353\n",
            "[283]\ttrain-mlogloss:0.217729\ttest-mlogloss:0.229215\n",
            "[284]\ttrain-mlogloss:0.217385\ttest-mlogloss:0.228931\n",
            "[285]\ttrain-mlogloss:0.217142\ttest-mlogloss:0.228706\n",
            "[286]\ttrain-mlogloss:0.216855\ttest-mlogloss:0.228462\n",
            "[287]\ttrain-mlogloss:0.216616\ttest-mlogloss:0.228248\n",
            "[288]\ttrain-mlogloss:0.216338\ttest-mlogloss:0.227983\n",
            "[289]\ttrain-mlogloss:0.216009\ttest-mlogloss:0.227729\n",
            "[290]\ttrain-mlogloss:0.215753\ttest-mlogloss:0.227593\n",
            "[291]\ttrain-mlogloss:0.215488\ttest-mlogloss:0.227413\n",
            "[292]\ttrain-mlogloss:0.215182\ttest-mlogloss:0.227144\n",
            "[293]\ttrain-mlogloss:0.214857\ttest-mlogloss:0.226868\n",
            "[294]\ttrain-mlogloss:0.214628\ttest-mlogloss:0.226687\n",
            "[295]\ttrain-mlogloss:0.214387\ttest-mlogloss:0.226492\n",
            "[296]\ttrain-mlogloss:0.214179\ttest-mlogloss:0.226311\n",
            "[297]\ttrain-mlogloss:0.213989\ttest-mlogloss:0.22619\n",
            "[298]\ttrain-mlogloss:0.213783\ttest-mlogloss:0.226022\n",
            "[299]\ttrain-mlogloss:0.213563\ttest-mlogloss:0.225864\n",
            "[300]\ttrain-mlogloss:0.213303\ttest-mlogloss:0.225608\n",
            "[301]\ttrain-mlogloss:0.213081\ttest-mlogloss:0.225371\n",
            "[302]\ttrain-mlogloss:0.212875\ttest-mlogloss:0.225199\n",
            "[303]\ttrain-mlogloss:0.212636\ttest-mlogloss:0.22494\n",
            "[304]\ttrain-mlogloss:0.212427\ttest-mlogloss:0.224822\n",
            "[305]\ttrain-mlogloss:0.212171\ttest-mlogloss:0.22455\n",
            "[306]\ttrain-mlogloss:0.21191\ttest-mlogloss:0.224345\n",
            "[307]\ttrain-mlogloss:0.211675\ttest-mlogloss:0.224156\n",
            "[308]\ttrain-mlogloss:0.21145\ttest-mlogloss:0.223931\n",
            "[309]\ttrain-mlogloss:0.211229\ttest-mlogloss:0.223695\n",
            "[310]\ttrain-mlogloss:0.211016\ttest-mlogloss:0.223504\n",
            "[311]\ttrain-mlogloss:0.210802\ttest-mlogloss:0.22325\n",
            "[312]\ttrain-mlogloss:0.210581\ttest-mlogloss:0.223029\n",
            "[313]\ttrain-mlogloss:0.210403\ttest-mlogloss:0.222868\n",
            "[314]\ttrain-mlogloss:0.210136\ttest-mlogloss:0.222606\n",
            "[315]\ttrain-mlogloss:0.209935\ttest-mlogloss:0.222414\n",
            "[316]\ttrain-mlogloss:0.209671\ttest-mlogloss:0.222202\n",
            "[317]\ttrain-mlogloss:0.209483\ttest-mlogloss:0.222064\n",
            "[318]\ttrain-mlogloss:0.209299\ttest-mlogloss:0.22188\n",
            "[319]\ttrain-mlogloss:0.209078\ttest-mlogloss:0.221672\n",
            "[320]\ttrain-mlogloss:0.208926\ttest-mlogloss:0.221517\n",
            "[321]\ttrain-mlogloss:0.208704\ttest-mlogloss:0.221326\n",
            "[322]\ttrain-mlogloss:0.208514\ttest-mlogloss:0.22115\n",
            "[323]\ttrain-mlogloss:0.208369\ttest-mlogloss:0.221021\n",
            "[324]\ttrain-mlogloss:0.208148\ttest-mlogloss:0.22086\n",
            "[325]\ttrain-mlogloss:0.207945\ttest-mlogloss:0.220758\n",
            "[326]\ttrain-mlogloss:0.207739\ttest-mlogloss:0.220603\n",
            "[327]\ttrain-mlogloss:0.207578\ttest-mlogloss:0.220398\n",
            "[328]\ttrain-mlogloss:0.207401\ttest-mlogloss:0.220266\n",
            "[329]\ttrain-mlogloss:0.207194\ttest-mlogloss:0.220058\n",
            "[330]\ttrain-mlogloss:0.207024\ttest-mlogloss:0.219929\n",
            "[331]\ttrain-mlogloss:0.206843\ttest-mlogloss:0.219779\n",
            "[332]\ttrain-mlogloss:0.206687\ttest-mlogloss:0.219623\n",
            "[333]\ttrain-mlogloss:0.20654\ttest-mlogloss:0.219437\n",
            "[334]\ttrain-mlogloss:0.206436\ttest-mlogloss:0.219343\n",
            "[335]\ttrain-mlogloss:0.206232\ttest-mlogloss:0.219182\n",
            "[336]\ttrain-mlogloss:0.206077\ttest-mlogloss:0.219108\n",
            "[337]\ttrain-mlogloss:0.205942\ttest-mlogloss:0.219028\n",
            "[338]\ttrain-mlogloss:0.205803\ttest-mlogloss:0.21891\n",
            "[339]\ttrain-mlogloss:0.205675\ttest-mlogloss:0.218823\n",
            "[340]\ttrain-mlogloss:0.205533\ttest-mlogloss:0.2187\n",
            "[341]\ttrain-mlogloss:0.2053\ttest-mlogloss:0.218519\n",
            "[342]\ttrain-mlogloss:0.2051\ttest-mlogloss:0.218335\n",
            "[343]\ttrain-mlogloss:0.204896\ttest-mlogloss:0.218219\n",
            "[344]\ttrain-mlogloss:0.204737\ttest-mlogloss:0.218113\n",
            "[345]\ttrain-mlogloss:0.204575\ttest-mlogloss:0.217967\n",
            "[346]\ttrain-mlogloss:0.204412\ttest-mlogloss:0.217817\n",
            "[347]\ttrain-mlogloss:0.204215\ttest-mlogloss:0.217645\n",
            "[348]\ttrain-mlogloss:0.204082\ttest-mlogloss:0.217542\n",
            "[349]\ttrain-mlogloss:0.203947\ttest-mlogloss:0.217457\n",
            "[350]\ttrain-mlogloss:0.203814\ttest-mlogloss:0.217327\n",
            "[351]\ttrain-mlogloss:0.203587\ttest-mlogloss:0.217132\n",
            "[352]\ttrain-mlogloss:0.203409\ttest-mlogloss:0.216969\n",
            "[353]\ttrain-mlogloss:0.203245\ttest-mlogloss:0.216894\n",
            "[354]\ttrain-mlogloss:0.203088\ttest-mlogloss:0.216756\n",
            "[355]\ttrain-mlogloss:0.202923\ttest-mlogloss:0.21669\n",
            "[356]\ttrain-mlogloss:0.202805\ttest-mlogloss:0.216661\n",
            "[357]\ttrain-mlogloss:0.202653\ttest-mlogloss:0.216572\n",
            "[358]\ttrain-mlogloss:0.202544\ttest-mlogloss:0.216496\n",
            "[359]\ttrain-mlogloss:0.202395\ttest-mlogloss:0.216403\n",
            "[360]\ttrain-mlogloss:0.202172\ttest-mlogloss:0.21617\n",
            "[361]\ttrain-mlogloss:0.201981\ttest-mlogloss:0.216031\n",
            "[362]\ttrain-mlogloss:0.201821\ttest-mlogloss:0.215872\n",
            "[363]\ttrain-mlogloss:0.201686\ttest-mlogloss:0.215794\n",
            "[364]\ttrain-mlogloss:0.201571\ttest-mlogloss:0.215701\n",
            "[365]\ttrain-mlogloss:0.201439\ttest-mlogloss:0.215543\n",
            "[366]\ttrain-mlogloss:0.201301\ttest-mlogloss:0.21544\n",
            "[367]\ttrain-mlogloss:0.201135\ttest-mlogloss:0.215307\n",
            "[368]\ttrain-mlogloss:0.200971\ttest-mlogloss:0.215125\n",
            "[369]\ttrain-mlogloss:0.200808\ttest-mlogloss:0.215012\n",
            "[370]\ttrain-mlogloss:0.200686\ttest-mlogloss:0.214899\n",
            "[371]\ttrain-mlogloss:0.200505\ttest-mlogloss:0.214738\n",
            "[372]\ttrain-mlogloss:0.200363\ttest-mlogloss:0.214646\n",
            "[373]\ttrain-mlogloss:0.200247\ttest-mlogloss:0.214549\n",
            "[374]\ttrain-mlogloss:0.200064\ttest-mlogloss:0.214405\n",
            "[375]\ttrain-mlogloss:0.199894\ttest-mlogloss:0.214274\n",
            "[376]\ttrain-mlogloss:0.199803\ttest-mlogloss:0.214211\n",
            "[377]\ttrain-mlogloss:0.199678\ttest-mlogloss:0.214078\n",
            "[378]\ttrain-mlogloss:0.199508\ttest-mlogloss:0.213971\n",
            "[379]\ttrain-mlogloss:0.199327\ttest-mlogloss:0.213808\n",
            "[380]\ttrain-mlogloss:0.199222\ttest-mlogloss:0.213712\n",
            "[381]\ttrain-mlogloss:0.199049\ttest-mlogloss:0.213588\n",
            "[382]\ttrain-mlogloss:0.198942\ttest-mlogloss:0.21357\n",
            "[383]\ttrain-mlogloss:0.198776\ttest-mlogloss:0.213438\n",
            "[384]\ttrain-mlogloss:0.198673\ttest-mlogloss:0.213366\n",
            "[385]\ttrain-mlogloss:0.198566\ttest-mlogloss:0.213269\n",
            "[386]\ttrain-mlogloss:0.198442\ttest-mlogloss:0.213183\n",
            "[387]\ttrain-mlogloss:0.198296\ttest-mlogloss:0.213084\n",
            "[388]\ttrain-mlogloss:0.198127\ttest-mlogloss:0.21296\n",
            "[389]\ttrain-mlogloss:0.197993\ttest-mlogloss:0.212874\n",
            "[390]\ttrain-mlogloss:0.197879\ttest-mlogloss:0.212756\n",
            "[391]\ttrain-mlogloss:0.197783\ttest-mlogloss:0.212705\n",
            "[392]\ttrain-mlogloss:0.197729\ttest-mlogloss:0.212656\n",
            "[393]\ttrain-mlogloss:0.197547\ttest-mlogloss:0.212499\n",
            "[394]\ttrain-mlogloss:0.197397\ttest-mlogloss:0.212404\n",
            "[395]\ttrain-mlogloss:0.197276\ttest-mlogloss:0.212353\n",
            "[396]\ttrain-mlogloss:0.197124\ttest-mlogloss:0.212257\n",
            "[397]\ttrain-mlogloss:0.197009\ttest-mlogloss:0.212139\n",
            "[398]\ttrain-mlogloss:0.196896\ttest-mlogloss:0.212061\n",
            "[399]\ttrain-mlogloss:0.196825\ttest-mlogloss:0.21199\n",
            "[400]\ttrain-mlogloss:0.196685\ttest-mlogloss:0.211914\n",
            "[401]\ttrain-mlogloss:0.196605\ttest-mlogloss:0.211842\n",
            "[402]\ttrain-mlogloss:0.196505\ttest-mlogloss:0.211773\n",
            "[403]\ttrain-mlogloss:0.196392\ttest-mlogloss:0.211648\n",
            "[404]\ttrain-mlogloss:0.196259\ttest-mlogloss:0.211619\n",
            "[405]\ttrain-mlogloss:0.196137\ttest-mlogloss:0.211554\n",
            "[406]\ttrain-mlogloss:0.196021\ttest-mlogloss:0.211497\n",
            "[407]\ttrain-mlogloss:0.19587\ttest-mlogloss:0.211467\n",
            "[408]\ttrain-mlogloss:0.195749\ttest-mlogloss:0.211418\n",
            "[409]\ttrain-mlogloss:0.195678\ttest-mlogloss:0.211347\n",
            "[410]\ttrain-mlogloss:0.195565\ttest-mlogloss:0.211273\n",
            "[411]\ttrain-mlogloss:0.195425\ttest-mlogloss:0.211166\n",
            "[412]\ttrain-mlogloss:0.19528\ttest-mlogloss:0.211095\n",
            "[413]\ttrain-mlogloss:0.195131\ttest-mlogloss:0.210955\n",
            "[414]\ttrain-mlogloss:0.195038\ttest-mlogloss:0.210866\n",
            "[415]\ttrain-mlogloss:0.194923\ttest-mlogloss:0.21081\n",
            "[416]\ttrain-mlogloss:0.194857\ttest-mlogloss:0.210723\n",
            "[417]\ttrain-mlogloss:0.19477\ttest-mlogloss:0.210684\n",
            "[418]\ttrain-mlogloss:0.194651\ttest-mlogloss:0.210576\n",
            "[419]\ttrain-mlogloss:0.194561\ttest-mlogloss:0.210528\n",
            "[420]\ttrain-mlogloss:0.19446\ttest-mlogloss:0.210447\n",
            "[421]\ttrain-mlogloss:0.194297\ttest-mlogloss:0.210359\n",
            "[422]\ttrain-mlogloss:0.194186\ttest-mlogloss:0.210312\n",
            "[423]\ttrain-mlogloss:0.194004\ttest-mlogloss:0.210228\n",
            "[424]\ttrain-mlogloss:0.193894\ttest-mlogloss:0.210142\n",
            "[425]\ttrain-mlogloss:0.1938\ttest-mlogloss:0.210055\n",
            "[426]\ttrain-mlogloss:0.193692\ttest-mlogloss:0.20996\n",
            "[427]\ttrain-mlogloss:0.193608\ttest-mlogloss:0.209863\n",
            "[428]\ttrain-mlogloss:0.19349\ttest-mlogloss:0.209819\n",
            "[429]\ttrain-mlogloss:0.193388\ttest-mlogloss:0.209795\n",
            "[430]\ttrain-mlogloss:0.193298\ttest-mlogloss:0.209735\n",
            "[431]\ttrain-mlogloss:0.193212\ttest-mlogloss:0.209678\n",
            "[432]\ttrain-mlogloss:0.193062\ttest-mlogloss:0.209559\n",
            "[433]\ttrain-mlogloss:0.192994\ttest-mlogloss:0.209507\n",
            "[434]\ttrain-mlogloss:0.192912\ttest-mlogloss:0.209437\n",
            "[435]\ttrain-mlogloss:0.192826\ttest-mlogloss:0.209396\n",
            "[436]\ttrain-mlogloss:0.192738\ttest-mlogloss:0.209367\n",
            "[437]\ttrain-mlogloss:0.192647\ttest-mlogloss:0.209303\n",
            "[438]\ttrain-mlogloss:0.192484\ttest-mlogloss:0.209217\n",
            "[439]\ttrain-mlogloss:0.192369\ttest-mlogloss:0.209086\n",
            "[440]\ttrain-mlogloss:0.192244\ttest-mlogloss:0.208978\n",
            "[441]\ttrain-mlogloss:0.19214\ttest-mlogloss:0.208954\n",
            "[442]\ttrain-mlogloss:0.191986\ttest-mlogloss:0.208861\n",
            "[443]\ttrain-mlogloss:0.191919\ttest-mlogloss:0.208783\n",
            "[444]\ttrain-mlogloss:0.191836\ttest-mlogloss:0.208742\n",
            "[445]\ttrain-mlogloss:0.191724\ttest-mlogloss:0.208677\n",
            "[446]\ttrain-mlogloss:0.191621\ttest-mlogloss:0.208665\n",
            "[447]\ttrain-mlogloss:0.191502\ttest-mlogloss:0.208599\n",
            "[448]\ttrain-mlogloss:0.191408\ttest-mlogloss:0.208593\n",
            "[449]\ttrain-mlogloss:0.19126\ttest-mlogloss:0.208459\n",
            "[450]\ttrain-mlogloss:0.191173\ttest-mlogloss:0.208386\n",
            "[451]\ttrain-mlogloss:0.191062\ttest-mlogloss:0.208354\n",
            "[452]\ttrain-mlogloss:0.19095\ttest-mlogloss:0.208288\n",
            "[453]\ttrain-mlogloss:0.190855\ttest-mlogloss:0.208225\n",
            "[454]\ttrain-mlogloss:0.190786\ttest-mlogloss:0.208211\n",
            "[455]\ttrain-mlogloss:0.190705\ttest-mlogloss:0.208159\n",
            "[456]\ttrain-mlogloss:0.190592\ttest-mlogloss:0.208067\n",
            "[457]\ttrain-mlogloss:0.190518\ttest-mlogloss:0.207987\n",
            "[458]\ttrain-mlogloss:0.190485\ttest-mlogloss:0.207936\n",
            "[459]\ttrain-mlogloss:0.190403\ttest-mlogloss:0.207879\n",
            "[460]\ttrain-mlogloss:0.19028\ttest-mlogloss:0.207727\n",
            "[461]\ttrain-mlogloss:0.190178\ttest-mlogloss:0.207654\n",
            "[462]\ttrain-mlogloss:0.190093\ttest-mlogloss:0.207611\n",
            "[463]\ttrain-mlogloss:0.190039\ttest-mlogloss:0.207586\n",
            "[464]\ttrain-mlogloss:0.190002\ttest-mlogloss:0.207569\n",
            "[465]\ttrain-mlogloss:0.189899\ttest-mlogloss:0.207508\n",
            "[466]\ttrain-mlogloss:0.189812\ttest-mlogloss:0.207446\n",
            "[467]\ttrain-mlogloss:0.189767\ttest-mlogloss:0.207374\n",
            "[468]\ttrain-mlogloss:0.189645\ttest-mlogloss:0.207322\n",
            "[469]\ttrain-mlogloss:0.189526\ttest-mlogloss:0.207265\n",
            "[470]\ttrain-mlogloss:0.189445\ttest-mlogloss:0.207147\n",
            "[471]\ttrain-mlogloss:0.189399\ttest-mlogloss:0.207132\n",
            "[472]\ttrain-mlogloss:0.189311\ttest-mlogloss:0.207067\n",
            "[473]\ttrain-mlogloss:0.18921\ttest-mlogloss:0.206986\n",
            "[474]\ttrain-mlogloss:0.189089\ttest-mlogloss:0.206925\n",
            "[475]\ttrain-mlogloss:0.188968\ttest-mlogloss:0.206789\n",
            "[476]\ttrain-mlogloss:0.188878\ttest-mlogloss:0.206765\n",
            "[477]\ttrain-mlogloss:0.188811\ttest-mlogloss:0.206691\n",
            "[478]\ttrain-mlogloss:0.188722\ttest-mlogloss:0.206679\n",
            "[479]\ttrain-mlogloss:0.18865\ttest-mlogloss:0.206617\n",
            "[480]\ttrain-mlogloss:0.188549\ttest-mlogloss:0.206582\n",
            "[481]\ttrain-mlogloss:0.188461\ttest-mlogloss:0.206532\n",
            "[482]\ttrain-mlogloss:0.188421\ttest-mlogloss:0.206481\n",
            "[483]\ttrain-mlogloss:0.188335\ttest-mlogloss:0.206469\n",
            "[484]\ttrain-mlogloss:0.18823\ttest-mlogloss:0.206437\n",
            "[485]\ttrain-mlogloss:0.188138\ttest-mlogloss:0.206367\n",
            "[486]\ttrain-mlogloss:0.188056\ttest-mlogloss:0.206355\n",
            "[487]\ttrain-mlogloss:0.187941\ttest-mlogloss:0.206307\n",
            "[488]\ttrain-mlogloss:0.187854\ttest-mlogloss:0.206283\n",
            "[489]\ttrain-mlogloss:0.187773\ttest-mlogloss:0.206233\n",
            "[490]\ttrain-mlogloss:0.187672\ttest-mlogloss:0.206175\n",
            "[491]\ttrain-mlogloss:0.187611\ttest-mlogloss:0.206112\n",
            "[492]\ttrain-mlogloss:0.187563\ttest-mlogloss:0.20609\n",
            "[493]\ttrain-mlogloss:0.187494\ttest-mlogloss:0.206043\n",
            "[494]\ttrain-mlogloss:0.187389\ttest-mlogloss:0.205997\n",
            "[495]\ttrain-mlogloss:0.187284\ttest-mlogloss:0.206005\n",
            "[496]\ttrain-mlogloss:0.18721\ttest-mlogloss:0.205975\n",
            "[497]\ttrain-mlogloss:0.187098\ttest-mlogloss:0.205931\n",
            "[498]\ttrain-mlogloss:0.187029\ttest-mlogloss:0.205902\n",
            "[499]\ttrain-mlogloss:0.186917\ttest-mlogloss:0.205824\n",
            "[500]\ttrain-mlogloss:0.186883\ttest-mlogloss:0.205831\n",
            "[501]\ttrain-mlogloss:0.186769\ttest-mlogloss:0.205805\n",
            "[502]\ttrain-mlogloss:0.186666\ttest-mlogloss:0.20576\n",
            "[503]\ttrain-mlogloss:0.186522\ttest-mlogloss:0.20571\n",
            "[504]\ttrain-mlogloss:0.186384\ttest-mlogloss:0.205693\n",
            "[505]\ttrain-mlogloss:0.186336\ttest-mlogloss:0.205709\n",
            "[506]\ttrain-mlogloss:0.186253\ttest-mlogloss:0.205713\n",
            "[507]\ttrain-mlogloss:0.186123\ttest-mlogloss:0.20567\n",
            "[508]\ttrain-mlogloss:0.186036\ttest-mlogloss:0.205622\n",
            "[509]\ttrain-mlogloss:0.185956\ttest-mlogloss:0.205616\n",
            "[510]\ttrain-mlogloss:0.185898\ttest-mlogloss:0.205603\n",
            "[511]\ttrain-mlogloss:0.185811\ttest-mlogloss:0.205598\n",
            "[512]\ttrain-mlogloss:0.185751\ttest-mlogloss:0.205534\n",
            "[513]\ttrain-mlogloss:0.18569\ttest-mlogloss:0.205554\n",
            "[514]\ttrain-mlogloss:0.185592\ttest-mlogloss:0.205552\n",
            "[515]\ttrain-mlogloss:0.18555\ttest-mlogloss:0.205563\n",
            "[516]\ttrain-mlogloss:0.185459\ttest-mlogloss:0.205511\n",
            "[517]\ttrain-mlogloss:0.185353\ttest-mlogloss:0.205472\n",
            "[518]\ttrain-mlogloss:0.185275\ttest-mlogloss:0.205493\n",
            "[519]\ttrain-mlogloss:0.185137\ttest-mlogloss:0.205407\n",
            "[520]\ttrain-mlogloss:0.185071\ttest-mlogloss:0.205351\n",
            "[521]\ttrain-mlogloss:0.184934\ttest-mlogloss:0.205272\n",
            "[522]\ttrain-mlogloss:0.184822\ttest-mlogloss:0.205187\n",
            "[523]\ttrain-mlogloss:0.184734\ttest-mlogloss:0.205158\n",
            "[524]\ttrain-mlogloss:0.184677\ttest-mlogloss:0.205088\n",
            "[525]\ttrain-mlogloss:0.184637\ttest-mlogloss:0.205066\n",
            "[526]\ttrain-mlogloss:0.184484\ttest-mlogloss:0.204996\n",
            "[527]\ttrain-mlogloss:0.184357\ttest-mlogloss:0.204924\n",
            "[528]\ttrain-mlogloss:0.184266\ttest-mlogloss:0.204864\n",
            "[529]\ttrain-mlogloss:0.184206\ttest-mlogloss:0.204863\n",
            "[530]\ttrain-mlogloss:0.184152\ttest-mlogloss:0.204808\n",
            "[531]\ttrain-mlogloss:0.184067\ttest-mlogloss:0.204747\n",
            "[532]\ttrain-mlogloss:0.183959\ttest-mlogloss:0.204676\n",
            "[533]\ttrain-mlogloss:0.183823\ttest-mlogloss:0.204532\n",
            "[534]\ttrain-mlogloss:0.183718\ttest-mlogloss:0.20444\n",
            "[535]\ttrain-mlogloss:0.183634\ttest-mlogloss:0.204437\n",
            "[536]\ttrain-mlogloss:0.183574\ttest-mlogloss:0.204459\n",
            "[537]\ttrain-mlogloss:0.183452\ttest-mlogloss:0.204316\n",
            "[538]\ttrain-mlogloss:0.183387\ttest-mlogloss:0.204262\n",
            "[539]\ttrain-mlogloss:0.183365\ttest-mlogloss:0.204242\n",
            "[540]\ttrain-mlogloss:0.18332\ttest-mlogloss:0.204242\n",
            "[541]\ttrain-mlogloss:0.183204\ttest-mlogloss:0.204168\n",
            "[542]\ttrain-mlogloss:0.183122\ttest-mlogloss:0.204163\n",
            "[543]\ttrain-mlogloss:0.183046\ttest-mlogloss:0.20415\n",
            "[544]\ttrain-mlogloss:0.182938\ttest-mlogloss:0.204117\n",
            "[545]\ttrain-mlogloss:0.18284\ttest-mlogloss:0.204091\n",
            "[546]\ttrain-mlogloss:0.182726\ttest-mlogloss:0.204036\n",
            "[547]\ttrain-mlogloss:0.182631\ttest-mlogloss:0.203996\n",
            "[548]\ttrain-mlogloss:0.182524\ttest-mlogloss:0.203989\n",
            "[549]\ttrain-mlogloss:0.182423\ttest-mlogloss:0.203902\n",
            "[550]\ttrain-mlogloss:0.182337\ttest-mlogloss:0.203891\n",
            "[551]\ttrain-mlogloss:0.182269\ttest-mlogloss:0.203903\n",
            "[552]\ttrain-mlogloss:0.182129\ttest-mlogloss:0.203823\n",
            "[553]\ttrain-mlogloss:0.182083\ttest-mlogloss:0.203841\n",
            "[554]\ttrain-mlogloss:0.181971\ttest-mlogloss:0.203755\n",
            "[555]\ttrain-mlogloss:0.181924\ttest-mlogloss:0.20371\n",
            "[556]\ttrain-mlogloss:0.181825\ttest-mlogloss:0.203686\n",
            "[557]\ttrain-mlogloss:0.181764\ttest-mlogloss:0.203689\n",
            "[558]\ttrain-mlogloss:0.181658\ttest-mlogloss:0.203583\n",
            "[559]\ttrain-mlogloss:0.1816\ttest-mlogloss:0.203522\n",
            "[560]\ttrain-mlogloss:0.181511\ttest-mlogloss:0.203469\n",
            "[561]\ttrain-mlogloss:0.181418\ttest-mlogloss:0.203449\n",
            "[562]\ttrain-mlogloss:0.181352\ttest-mlogloss:0.203438\n",
            "[563]\ttrain-mlogloss:0.181267\ttest-mlogloss:0.203391\n",
            "[564]\ttrain-mlogloss:0.181133\ttest-mlogloss:0.203333\n",
            "[565]\ttrain-mlogloss:0.18102\ttest-mlogloss:0.203301\n",
            "[566]\ttrain-mlogloss:0.180953\ttest-mlogloss:0.203333\n",
            "[567]\ttrain-mlogloss:0.180851\ttest-mlogloss:0.203302\n",
            "[568]\ttrain-mlogloss:0.180747\ttest-mlogloss:0.203236\n",
            "[569]\ttrain-mlogloss:0.180692\ttest-mlogloss:0.203186\n",
            "[570]\ttrain-mlogloss:0.180626\ttest-mlogloss:0.203139\n",
            "[571]\ttrain-mlogloss:0.180588\ttest-mlogloss:0.203109\n",
            "[572]\ttrain-mlogloss:0.180508\ttest-mlogloss:0.203116\n",
            "[573]\ttrain-mlogloss:0.180458\ttest-mlogloss:0.203107\n",
            "[574]\ttrain-mlogloss:0.180386\ttest-mlogloss:0.203084\n",
            "[575]\ttrain-mlogloss:0.180266\ttest-mlogloss:0.203042\n",
            "[576]\ttrain-mlogloss:0.180198\ttest-mlogloss:0.20303\n",
            "[577]\ttrain-mlogloss:0.18009\ttest-mlogloss:0.203024\n",
            "[578]\ttrain-mlogloss:0.180046\ttest-mlogloss:0.203005\n",
            "[579]\ttrain-mlogloss:0.179982\ttest-mlogloss:0.20295\n",
            "[580]\ttrain-mlogloss:0.179894\ttest-mlogloss:0.202889\n",
            "[581]\ttrain-mlogloss:0.179807\ttest-mlogloss:0.202906\n",
            "[582]\ttrain-mlogloss:0.179676\ttest-mlogloss:0.202842\n",
            "[583]\ttrain-mlogloss:0.179587\ttest-mlogloss:0.202819\n",
            "[584]\ttrain-mlogloss:0.179475\ttest-mlogloss:0.202765\n",
            "[585]\ttrain-mlogloss:0.179432\ttest-mlogloss:0.202782\n",
            "[586]\ttrain-mlogloss:0.179392\ttest-mlogloss:0.202781\n",
            "[587]\ttrain-mlogloss:0.179324\ttest-mlogloss:0.202717\n",
            "[588]\ttrain-mlogloss:0.179296\ttest-mlogloss:0.202689\n",
            "[589]\ttrain-mlogloss:0.179252\ttest-mlogloss:0.202712\n",
            "[590]\ttrain-mlogloss:0.179162\ttest-mlogloss:0.202662\n",
            "[591]\ttrain-mlogloss:0.179124\ttest-mlogloss:0.202659\n",
            "[592]\ttrain-mlogloss:0.178973\ttest-mlogloss:0.20259\n",
            "[593]\ttrain-mlogloss:0.178869\ttest-mlogloss:0.202576\n",
            "[594]\ttrain-mlogloss:0.178831\ttest-mlogloss:0.202562\n",
            "[595]\ttrain-mlogloss:0.178765\ttest-mlogloss:0.202543\n",
            "[596]\ttrain-mlogloss:0.178704\ttest-mlogloss:0.202543\n",
            "[597]\ttrain-mlogloss:0.178634\ttest-mlogloss:0.202518\n",
            "[598]\ttrain-mlogloss:0.178553\ttest-mlogloss:0.202489\n",
            "[599]\ttrain-mlogloss:0.178487\ttest-mlogloss:0.202494\n",
            "[600]\ttrain-mlogloss:0.178416\ttest-mlogloss:0.202472\n",
            "[601]\ttrain-mlogloss:0.178361\ttest-mlogloss:0.202509\n",
            "[602]\ttrain-mlogloss:0.178279\ttest-mlogloss:0.202477\n",
            "[603]\ttrain-mlogloss:0.178195\ttest-mlogloss:0.202473\n",
            "[604]\ttrain-mlogloss:0.178151\ttest-mlogloss:0.20243\n",
            "[605]\ttrain-mlogloss:0.178081\ttest-mlogloss:0.202423\n",
            "[606]\ttrain-mlogloss:0.178031\ttest-mlogloss:0.202406\n",
            "[607]\ttrain-mlogloss:0.177921\ttest-mlogloss:0.202352\n",
            "[608]\ttrain-mlogloss:0.17778\ttest-mlogloss:0.202262\n",
            "[609]\ttrain-mlogloss:0.177708\ttest-mlogloss:0.20222\n",
            "[610]\ttrain-mlogloss:0.177641\ttest-mlogloss:0.202204\n",
            "[611]\ttrain-mlogloss:0.177579\ttest-mlogloss:0.202191\n",
            "[612]\ttrain-mlogloss:0.177491\ttest-mlogloss:0.202137\n",
            "[613]\ttrain-mlogloss:0.177397\ttest-mlogloss:0.202113\n",
            "[614]\ttrain-mlogloss:0.177335\ttest-mlogloss:0.202136\n",
            "[615]\ttrain-mlogloss:0.177255\ttest-mlogloss:0.202107\n",
            "[616]\ttrain-mlogloss:0.177148\ttest-mlogloss:0.202042\n",
            "[617]\ttrain-mlogloss:0.177053\ttest-mlogloss:0.20201\n",
            "[618]\ttrain-mlogloss:0.176972\ttest-mlogloss:0.202016\n",
            "[619]\ttrain-mlogloss:0.176931\ttest-mlogloss:0.201977\n",
            "[620]\ttrain-mlogloss:0.176892\ttest-mlogloss:0.201972\n",
            "[621]\ttrain-mlogloss:0.176806\ttest-mlogloss:0.20193\n",
            "[622]\ttrain-mlogloss:0.176752\ttest-mlogloss:0.201913\n",
            "[623]\ttrain-mlogloss:0.176705\ttest-mlogloss:0.201867\n",
            "[624]\ttrain-mlogloss:0.176665\ttest-mlogloss:0.201872\n",
            "[625]\ttrain-mlogloss:0.176595\ttest-mlogloss:0.201861\n",
            "[626]\ttrain-mlogloss:0.176539\ttest-mlogloss:0.201807\n",
            "[627]\ttrain-mlogloss:0.176451\ttest-mlogloss:0.201766\n",
            "[628]\ttrain-mlogloss:0.176417\ttest-mlogloss:0.201775\n",
            "[629]\ttrain-mlogloss:0.176367\ttest-mlogloss:0.201814\n",
            "[630]\ttrain-mlogloss:0.176278\ttest-mlogloss:0.201812\n",
            "[631]\ttrain-mlogloss:0.176253\ttest-mlogloss:0.201813\n",
            "[632]\ttrain-mlogloss:0.176198\ttest-mlogloss:0.201771\n",
            "[633]\ttrain-mlogloss:0.176146\ttest-mlogloss:0.201813\n",
            "[634]\ttrain-mlogloss:0.17609\ttest-mlogloss:0.201796\n",
            "[635]\ttrain-mlogloss:0.176042\ttest-mlogloss:0.201773\n",
            "[636]\ttrain-mlogloss:0.176017\ttest-mlogloss:0.201766\n",
            "[637]\ttrain-mlogloss:0.175924\ttest-mlogloss:0.201748\n",
            "[638]\ttrain-mlogloss:0.175869\ttest-mlogloss:0.201681\n",
            "[639]\ttrain-mlogloss:0.175793\ttest-mlogloss:0.201623\n",
            "[640]\ttrain-mlogloss:0.175762\ttest-mlogloss:0.201609\n",
            "[641]\ttrain-mlogloss:0.175657\ttest-mlogloss:0.201639\n",
            "[642]\ttrain-mlogloss:0.175616\ttest-mlogloss:0.201639\n",
            "[643]\ttrain-mlogloss:0.175539\ttest-mlogloss:0.201602\n",
            "[644]\ttrain-mlogloss:0.175487\ttest-mlogloss:0.201589\n",
            "[645]\ttrain-mlogloss:0.175347\ttest-mlogloss:0.201521\n",
            "[646]\ttrain-mlogloss:0.175331\ttest-mlogloss:0.201516\n",
            "[647]\ttrain-mlogloss:0.175274\ttest-mlogloss:0.201524\n",
            "[648]\ttrain-mlogloss:0.175162\ttest-mlogloss:0.201475\n",
            "[649]\ttrain-mlogloss:0.175084\ttest-mlogloss:0.20146\n",
            "[650]\ttrain-mlogloss:0.174984\ttest-mlogloss:0.201352\n",
            "[651]\ttrain-mlogloss:0.174904\ttest-mlogloss:0.201312\n",
            "[652]\ttrain-mlogloss:0.17475\ttest-mlogloss:0.201287\n",
            "[653]\ttrain-mlogloss:0.174655\ttest-mlogloss:0.20122\n",
            "[654]\ttrain-mlogloss:0.174606\ttest-mlogloss:0.201245\n",
            "[655]\ttrain-mlogloss:0.174542\ttest-mlogloss:0.201218\n",
            "[656]\ttrain-mlogloss:0.17452\ttest-mlogloss:0.201164\n",
            "[657]\ttrain-mlogloss:0.174428\ttest-mlogloss:0.201115\n",
            "[658]\ttrain-mlogloss:0.174373\ttest-mlogloss:0.201084\n",
            "[659]\ttrain-mlogloss:0.174322\ttest-mlogloss:0.201063\n",
            "[660]\ttrain-mlogloss:0.174252\ttest-mlogloss:0.201019\n",
            "[661]\ttrain-mlogloss:0.174178\ttest-mlogloss:0.201024\n",
            "[662]\ttrain-mlogloss:0.174083\ttest-mlogloss:0.200953\n",
            "[663]\ttrain-mlogloss:0.17405\ttest-mlogloss:0.200965\n",
            "[664]\ttrain-mlogloss:0.173967\ttest-mlogloss:0.200916\n",
            "[665]\ttrain-mlogloss:0.173898\ttest-mlogloss:0.200889\n",
            "[666]\ttrain-mlogloss:0.173836\ttest-mlogloss:0.200922\n",
            "[667]\ttrain-mlogloss:0.173743\ttest-mlogloss:0.200904\n",
            "[668]\ttrain-mlogloss:0.173674\ttest-mlogloss:0.200856\n",
            "[669]\ttrain-mlogloss:0.173611\ttest-mlogloss:0.200883\n",
            "[670]\ttrain-mlogloss:0.173475\ttest-mlogloss:0.200819\n",
            "[671]\ttrain-mlogloss:0.173398\ttest-mlogloss:0.200819\n",
            "[672]\ttrain-mlogloss:0.173365\ttest-mlogloss:0.200804\n",
            "[673]\ttrain-mlogloss:0.173296\ttest-mlogloss:0.20079\n",
            "[674]\ttrain-mlogloss:0.173259\ttest-mlogloss:0.200731\n",
            "[675]\ttrain-mlogloss:0.173176\ttest-mlogloss:0.200676\n",
            "[676]\ttrain-mlogloss:0.173122\ttest-mlogloss:0.20063\n",
            "[677]\ttrain-mlogloss:0.173106\ttest-mlogloss:0.200614\n",
            "[678]\ttrain-mlogloss:0.173038\ttest-mlogloss:0.200579\n",
            "[679]\ttrain-mlogloss:0.172915\ttest-mlogloss:0.200498\n",
            "[680]\ttrain-mlogloss:0.172851\ttest-mlogloss:0.200488\n",
            "[681]\ttrain-mlogloss:0.172771\ttest-mlogloss:0.200497\n",
            "[682]\ttrain-mlogloss:0.172726\ttest-mlogloss:0.200482\n",
            "[683]\ttrain-mlogloss:0.172647\ttest-mlogloss:0.200475\n",
            "[684]\ttrain-mlogloss:0.172596\ttest-mlogloss:0.20049\n",
            "[685]\ttrain-mlogloss:0.172507\ttest-mlogloss:0.200484\n",
            "[686]\ttrain-mlogloss:0.172446\ttest-mlogloss:0.200457\n",
            "[687]\ttrain-mlogloss:0.172371\ttest-mlogloss:0.20046\n",
            "[688]\ttrain-mlogloss:0.172311\ttest-mlogloss:0.200418\n",
            "[689]\ttrain-mlogloss:0.172241\ttest-mlogloss:0.2004\n",
            "[690]\ttrain-mlogloss:0.172138\ttest-mlogloss:0.200393\n",
            "[691]\ttrain-mlogloss:0.1721\ttest-mlogloss:0.200365\n",
            "[692]\ttrain-mlogloss:0.172036\ttest-mlogloss:0.200316\n",
            "[693]\ttrain-mlogloss:0.171926\ttest-mlogloss:0.200276\n",
            "[694]\ttrain-mlogloss:0.171817\ttest-mlogloss:0.200282\n",
            "[695]\ttrain-mlogloss:0.171776\ttest-mlogloss:0.200259\n",
            "[696]\ttrain-mlogloss:0.171736\ttest-mlogloss:0.20023\n",
            "[697]\ttrain-mlogloss:0.171655\ttest-mlogloss:0.200232\n",
            "[698]\ttrain-mlogloss:0.171589\ttest-mlogloss:0.200186\n",
            "[699]\ttrain-mlogloss:0.171499\ttest-mlogloss:0.200134\n",
            "[700]\ttrain-mlogloss:0.171456\ttest-mlogloss:0.200126\n",
            "[701]\ttrain-mlogloss:0.171369\ttest-mlogloss:0.200119\n",
            "[702]\ttrain-mlogloss:0.171274\ttest-mlogloss:0.200103\n",
            "[703]\ttrain-mlogloss:0.171215\ttest-mlogloss:0.200099\n",
            "[704]\ttrain-mlogloss:0.171171\ttest-mlogloss:0.200103\n",
            "[705]\ttrain-mlogloss:0.171087\ttest-mlogloss:0.200111\n",
            "[706]\ttrain-mlogloss:0.170937\ttest-mlogloss:0.199999\n",
            "[707]\ttrain-mlogloss:0.170822\ttest-mlogloss:0.199953\n",
            "[708]\ttrain-mlogloss:0.170723\ttest-mlogloss:0.199906\n",
            "[709]\ttrain-mlogloss:0.170633\ttest-mlogloss:0.19988\n",
            "[710]\ttrain-mlogloss:0.170592\ttest-mlogloss:0.199887\n",
            "[711]\ttrain-mlogloss:0.170568\ttest-mlogloss:0.199903\n",
            "[712]\ttrain-mlogloss:0.170509\ttest-mlogloss:0.199918\n",
            "[713]\ttrain-mlogloss:0.170448\ttest-mlogloss:0.199881\n",
            "[714]\ttrain-mlogloss:0.170405\ttest-mlogloss:0.199875\n",
            "[715]\ttrain-mlogloss:0.170369\ttest-mlogloss:0.199827\n",
            "[716]\ttrain-mlogloss:0.170255\ttest-mlogloss:0.199781\n",
            "[717]\ttrain-mlogloss:0.170166\ttest-mlogloss:0.199777\n",
            "[718]\ttrain-mlogloss:0.17007\ttest-mlogloss:0.199724\n",
            "[719]\ttrain-mlogloss:0.17001\ttest-mlogloss:0.199745\n",
            "[720]\ttrain-mlogloss:0.169994\ttest-mlogloss:0.199738\n",
            "[721]\ttrain-mlogloss:0.169944\ttest-mlogloss:0.199685\n",
            "[722]\ttrain-mlogloss:0.169904\ttest-mlogloss:0.1997\n",
            "[723]\ttrain-mlogloss:0.169842\ttest-mlogloss:0.199701\n",
            "[724]\ttrain-mlogloss:0.169798\ttest-mlogloss:0.199694\n",
            "[725]\ttrain-mlogloss:0.169748\ttest-mlogloss:0.19963\n",
            "[726]\ttrain-mlogloss:0.16962\ttest-mlogloss:0.199568\n",
            "[727]\ttrain-mlogloss:0.16957\ttest-mlogloss:0.199514\n",
            "[728]\ttrain-mlogloss:0.169431\ttest-mlogloss:0.199476\n",
            "[729]\ttrain-mlogloss:0.169353\ttest-mlogloss:0.199438\n",
            "[730]\ttrain-mlogloss:0.169277\ttest-mlogloss:0.199407\n",
            "[731]\ttrain-mlogloss:0.169236\ttest-mlogloss:0.199338\n",
            "[732]\ttrain-mlogloss:0.169215\ttest-mlogloss:0.199322\n",
            "[733]\ttrain-mlogloss:0.169154\ttest-mlogloss:0.199297\n",
            "[734]\ttrain-mlogloss:0.169057\ttest-mlogloss:0.199254\n",
            "[735]\ttrain-mlogloss:0.168967\ttest-mlogloss:0.199212\n",
            "[736]\ttrain-mlogloss:0.168904\ttest-mlogloss:0.199197\n",
            "[737]\ttrain-mlogloss:0.168879\ttest-mlogloss:0.199207\n",
            "[738]\ttrain-mlogloss:0.168826\ttest-mlogloss:0.199185\n",
            "[739]\ttrain-mlogloss:0.168764\ttest-mlogloss:0.199219\n",
            "[740]\ttrain-mlogloss:0.168703\ttest-mlogloss:0.1992\n",
            "[741]\ttrain-mlogloss:0.168625\ttest-mlogloss:0.199202\n",
            "[742]\ttrain-mlogloss:0.168534\ttest-mlogloss:0.199185\n",
            "[743]\ttrain-mlogloss:0.168515\ttest-mlogloss:0.199161\n",
            "[744]\ttrain-mlogloss:0.168473\ttest-mlogloss:0.19914\n",
            "[745]\ttrain-mlogloss:0.168398\ttest-mlogloss:0.199084\n",
            "[746]\ttrain-mlogloss:0.168396\ttest-mlogloss:0.199092\n",
            "[747]\ttrain-mlogloss:0.168303\ttest-mlogloss:0.199095\n",
            "[748]\ttrain-mlogloss:0.168221\ttest-mlogloss:0.199088\n",
            "[749]\ttrain-mlogloss:0.168212\ttest-mlogloss:0.199099\n",
            "[750]\ttrain-mlogloss:0.168198\ttest-mlogloss:0.199082\n",
            "[751]\ttrain-mlogloss:0.168143\ttest-mlogloss:0.199022\n",
            "[752]\ttrain-mlogloss:0.168122\ttest-mlogloss:0.199027\n",
            "[753]\ttrain-mlogloss:0.168059\ttest-mlogloss:0.199019\n",
            "[754]\ttrain-mlogloss:0.167976\ttest-mlogloss:0.198967\n",
            "[755]\ttrain-mlogloss:0.167901\ttest-mlogloss:0.198942\n",
            "[756]\ttrain-mlogloss:0.167855\ttest-mlogloss:0.198926\n",
            "[757]\ttrain-mlogloss:0.16779\ttest-mlogloss:0.198904\n",
            "[758]\ttrain-mlogloss:0.167794\ttest-mlogloss:0.198921\n",
            "[759]\ttrain-mlogloss:0.167685\ttest-mlogloss:0.19886\n",
            "[760]\ttrain-mlogloss:0.167609\ttest-mlogloss:0.198867\n",
            "[761]\ttrain-mlogloss:0.167567\ttest-mlogloss:0.19888\n",
            "[762]\ttrain-mlogloss:0.167532\ttest-mlogloss:0.198857\n",
            "[763]\ttrain-mlogloss:0.16743\ttest-mlogloss:0.198778\n",
            "[764]\ttrain-mlogloss:0.167306\ttest-mlogloss:0.198747\n",
            "[765]\ttrain-mlogloss:0.167242\ttest-mlogloss:0.198702\n",
            "[766]\ttrain-mlogloss:0.167213\ttest-mlogloss:0.19871\n",
            "[767]\ttrain-mlogloss:0.167113\ttest-mlogloss:0.198665\n",
            "[768]\ttrain-mlogloss:0.167052\ttest-mlogloss:0.198692\n",
            "[769]\ttrain-mlogloss:0.166992\ttest-mlogloss:0.19869\n",
            "[770]\ttrain-mlogloss:0.166974\ttest-mlogloss:0.198702\n",
            "[771]\ttrain-mlogloss:0.166953\ttest-mlogloss:0.198673\n",
            "[772]\ttrain-mlogloss:0.16688\ttest-mlogloss:0.198607\n",
            "[773]\ttrain-mlogloss:0.166851\ttest-mlogloss:0.198594\n",
            "[774]\ttrain-mlogloss:0.166752\ttest-mlogloss:0.198498\n",
            "[775]\ttrain-mlogloss:0.166733\ttest-mlogloss:0.198527\n",
            "[776]\ttrain-mlogloss:0.166687\ttest-mlogloss:0.198566\n",
            "[777]\ttrain-mlogloss:0.166638\ttest-mlogloss:0.198598\n",
            "[778]\ttrain-mlogloss:0.166585\ttest-mlogloss:0.198602\n",
            "[779]\ttrain-mlogloss:0.166562\ttest-mlogloss:0.198623\n",
            "[780]\ttrain-mlogloss:0.166469\ttest-mlogloss:0.198607\n",
            "[781]\ttrain-mlogloss:0.16635\ttest-mlogloss:0.198539\n",
            "[782]\ttrain-mlogloss:0.166297\ttest-mlogloss:0.198546\n",
            "[783]\ttrain-mlogloss:0.166262\ttest-mlogloss:0.198512\n",
            "[784]\ttrain-mlogloss:0.16626\ttest-mlogloss:0.198509\n",
            "[785]\ttrain-mlogloss:0.166233\ttest-mlogloss:0.198527\n",
            "[786]\ttrain-mlogloss:0.166199\ttest-mlogloss:0.19849\n",
            "[787]\ttrain-mlogloss:0.166182\ttest-mlogloss:0.198452\n",
            "[788]\ttrain-mlogloss:0.166118\ttest-mlogloss:0.198415\n",
            "[789]\ttrain-mlogloss:0.166107\ttest-mlogloss:0.198408\n",
            "[790]\ttrain-mlogloss:0.166061\ttest-mlogloss:0.198424\n",
            "[791]\ttrain-mlogloss:0.165982\ttest-mlogloss:0.198362\n",
            "[792]\ttrain-mlogloss:0.165935\ttest-mlogloss:0.198376\n",
            "[793]\ttrain-mlogloss:0.165867\ttest-mlogloss:0.198346\n",
            "[794]\ttrain-mlogloss:0.165812\ttest-mlogloss:0.198353\n",
            "[795]\ttrain-mlogloss:0.165742\ttest-mlogloss:0.198375\n",
            "[796]\ttrain-mlogloss:0.165659\ttest-mlogloss:0.198344\n",
            "[797]\ttrain-mlogloss:0.165586\ttest-mlogloss:0.198307\n",
            "[798]\ttrain-mlogloss:0.16552\ttest-mlogloss:0.198263\n",
            "[799]\ttrain-mlogloss:0.165425\ttest-mlogloss:0.198241\n",
            "[800]\ttrain-mlogloss:0.165375\ttest-mlogloss:0.19828\n",
            "[801]\ttrain-mlogloss:0.165318\ttest-mlogloss:0.19827\n",
            "[802]\ttrain-mlogloss:0.165279\ttest-mlogloss:0.19828\n",
            "[803]\ttrain-mlogloss:0.165208\ttest-mlogloss:0.198266\n",
            "[804]\ttrain-mlogloss:0.165155\ttest-mlogloss:0.198231\n",
            "[805]\ttrain-mlogloss:0.165063\ttest-mlogloss:0.198211\n",
            "[806]\ttrain-mlogloss:0.165006\ttest-mlogloss:0.198197\n",
            "[807]\ttrain-mlogloss:0.164898\ttest-mlogloss:0.198171\n",
            "[808]\ttrain-mlogloss:0.164872\ttest-mlogloss:0.198169\n",
            "[809]\ttrain-mlogloss:0.164838\ttest-mlogloss:0.198172\n",
            "[810]\ttrain-mlogloss:0.164838\ttest-mlogloss:0.198175\n",
            "[811]\ttrain-mlogloss:0.164802\ttest-mlogloss:0.198209\n",
            "[812]\ttrain-mlogloss:0.164657\ttest-mlogloss:0.198138\n",
            "[813]\ttrain-mlogloss:0.164617\ttest-mlogloss:0.198124\n",
            "[814]\ttrain-mlogloss:0.164557\ttest-mlogloss:0.198123\n",
            "[815]\ttrain-mlogloss:0.164469\ttest-mlogloss:0.198101\n",
            "[816]\ttrain-mlogloss:0.164383\ttest-mlogloss:0.198111\n",
            "[817]\ttrain-mlogloss:0.16432\ttest-mlogloss:0.198142\n",
            "[818]\ttrain-mlogloss:0.164213\ttest-mlogloss:0.198095\n",
            "[819]\ttrain-mlogloss:0.164174\ttest-mlogloss:0.198094\n",
            "[820]\ttrain-mlogloss:0.164093\ttest-mlogloss:0.198057\n",
            "[821]\ttrain-mlogloss:0.16407\ttest-mlogloss:0.198061\n",
            "[822]\ttrain-mlogloss:0.164027\ttest-mlogloss:0.198067\n",
            "[823]\ttrain-mlogloss:0.163942\ttest-mlogloss:0.198015\n",
            "[824]\ttrain-mlogloss:0.163864\ttest-mlogloss:0.197984\n",
            "[825]\ttrain-mlogloss:0.163783\ttest-mlogloss:0.197964\n",
            "[826]\ttrain-mlogloss:0.163756\ttest-mlogloss:0.197954\n",
            "[827]\ttrain-mlogloss:0.163719\ttest-mlogloss:0.19791\n",
            "[828]\ttrain-mlogloss:0.163598\ttest-mlogloss:0.197921\n",
            "[829]\ttrain-mlogloss:0.163559\ttest-mlogloss:0.197916\n",
            "[830]\ttrain-mlogloss:0.163486\ttest-mlogloss:0.19789\n",
            "[831]\ttrain-mlogloss:0.163479\ttest-mlogloss:0.197905\n",
            "[832]\ttrain-mlogloss:0.163392\ttest-mlogloss:0.197935\n",
            "[833]\ttrain-mlogloss:0.163343\ttest-mlogloss:0.197956\n",
            "[834]\ttrain-mlogloss:0.163279\ttest-mlogloss:0.197889\n",
            "[835]\ttrain-mlogloss:0.163264\ttest-mlogloss:0.197886\n",
            "[836]\ttrain-mlogloss:0.163172\ttest-mlogloss:0.197835\n",
            "[837]\ttrain-mlogloss:0.163121\ttest-mlogloss:0.197826\n",
            "[838]\ttrain-mlogloss:0.16308\ttest-mlogloss:0.197831\n",
            "[839]\ttrain-mlogloss:0.163054\ttest-mlogloss:0.197804\n",
            "[840]\ttrain-mlogloss:0.162992\ttest-mlogloss:0.197799\n",
            "[841]\ttrain-mlogloss:0.162935\ttest-mlogloss:0.197825\n",
            "[842]\ttrain-mlogloss:0.162893\ttest-mlogloss:0.197839\n",
            "[843]\ttrain-mlogloss:0.162834\ttest-mlogloss:0.197808\n",
            "[844]\ttrain-mlogloss:0.162773\ttest-mlogloss:0.19777\n",
            "[845]\ttrain-mlogloss:0.162752\ttest-mlogloss:0.19776\n",
            "[846]\ttrain-mlogloss:0.162635\ttest-mlogloss:0.197725\n",
            "[847]\ttrain-mlogloss:0.162564\ttest-mlogloss:0.197728\n",
            "[848]\ttrain-mlogloss:0.162545\ttest-mlogloss:0.197769\n",
            "[849]\ttrain-mlogloss:0.162493\ttest-mlogloss:0.197783\n",
            "[850]\ttrain-mlogloss:0.162448\ttest-mlogloss:0.197813\n",
            "[851]\ttrain-mlogloss:0.162434\ttest-mlogloss:0.197837\n",
            "[852]\ttrain-mlogloss:0.162345\ttest-mlogloss:0.197835\n",
            "[853]\ttrain-mlogloss:0.162291\ttest-mlogloss:0.197841\n",
            "[854]\ttrain-mlogloss:0.162232\ttest-mlogloss:0.197823\n",
            "[855]\ttrain-mlogloss:0.162104\ttest-mlogloss:0.197816\n",
            "[856]\ttrain-mlogloss:0.162095\ttest-mlogloss:0.197885\n",
            "[857]\ttrain-mlogloss:0.162037\ttest-mlogloss:0.197866\n",
            "[858]\ttrain-mlogloss:0.161969\ttest-mlogloss:0.197904\n",
            "[859]\ttrain-mlogloss:0.161908\ttest-mlogloss:0.197908\n",
            "[860]\ttrain-mlogloss:0.161873\ttest-mlogloss:0.197907\n",
            "[861]\ttrain-mlogloss:0.161783\ttest-mlogloss:0.197867\n",
            "[862]\ttrain-mlogloss:0.16173\ttest-mlogloss:0.197907\n",
            "[863]\ttrain-mlogloss:0.161699\ttest-mlogloss:0.19791\n",
            "[864]\ttrain-mlogloss:0.161616\ttest-mlogloss:0.197923\n",
            "[865]\ttrain-mlogloss:0.161602\ttest-mlogloss:0.197977\n",
            "[866]\ttrain-mlogloss:0.161572\ttest-mlogloss:0.198019\n",
            "[867]\ttrain-mlogloss:0.161479\ttest-mlogloss:0.197999\n",
            "[868]\ttrain-mlogloss:0.161356\ttest-mlogloss:0.197947\n",
            "[869]\ttrain-mlogloss:0.161298\ttest-mlogloss:0.197898\n",
            "[870]\ttrain-mlogloss:0.161259\ttest-mlogloss:0.197908\n",
            "[871]\ttrain-mlogloss:0.161203\ttest-mlogloss:0.197883\n",
            "[872]\ttrain-mlogloss:0.161167\ttest-mlogloss:0.197883\n",
            "[873]\ttrain-mlogloss:0.161083\ttest-mlogloss:0.19785\n",
            "[874]\ttrain-mlogloss:0.161032\ttest-mlogloss:0.197825\n",
            "[875]\ttrain-mlogloss:0.160985\ttest-mlogloss:0.197793\n",
            "[876]\ttrain-mlogloss:0.160943\ttest-mlogloss:0.197782\n",
            "[877]\ttrain-mlogloss:0.160911\ttest-mlogloss:0.197767\n",
            "[878]\ttrain-mlogloss:0.16091\ttest-mlogloss:0.197787\n",
            "[879]\ttrain-mlogloss:0.160836\ttest-mlogloss:0.197754\n",
            "[880]\ttrain-mlogloss:0.160825\ttest-mlogloss:0.197768\n",
            "[881]\ttrain-mlogloss:0.16073\ttest-mlogloss:0.197751\n",
            "[882]\ttrain-mlogloss:0.160663\ttest-mlogloss:0.197723\n",
            "[883]\ttrain-mlogloss:0.160572\ttest-mlogloss:0.197747\n",
            "[884]\ttrain-mlogloss:0.160558\ttest-mlogloss:0.197739\n",
            "[885]\ttrain-mlogloss:0.160564\ttest-mlogloss:0.197715\n",
            "[886]\ttrain-mlogloss:0.160536\ttest-mlogloss:0.197719\n",
            "[887]\ttrain-mlogloss:0.160522\ttest-mlogloss:0.197737\n",
            "[888]\ttrain-mlogloss:0.160503\ttest-mlogloss:0.197762\n",
            "[889]\ttrain-mlogloss:0.160434\ttest-mlogloss:0.197765\n",
            "[890]\ttrain-mlogloss:0.160396\ttest-mlogloss:0.197776\n",
            "[891]\ttrain-mlogloss:0.160347\ttest-mlogloss:0.197779\n",
            "[892]\ttrain-mlogloss:0.160334\ttest-mlogloss:0.197804\n",
            "[893]\ttrain-mlogloss:0.160259\ttest-mlogloss:0.197769\n",
            "[894]\ttrain-mlogloss:0.160212\ttest-mlogloss:0.197751\n",
            "[895]\ttrain-mlogloss:0.160107\ttest-mlogloss:0.197691\n",
            "[896]\ttrain-mlogloss:0.160097\ttest-mlogloss:0.197646\n",
            "[897]\ttrain-mlogloss:0.160087\ttest-mlogloss:0.197646\n",
            "[898]\ttrain-mlogloss:0.159993\ttest-mlogloss:0.197611\n",
            "[899]\ttrain-mlogloss:0.15996\ttest-mlogloss:0.197572\n",
            "[900]\ttrain-mlogloss:0.159885\ttest-mlogloss:0.197545\n",
            "[901]\ttrain-mlogloss:0.159821\ttest-mlogloss:0.197509\n",
            "[902]\ttrain-mlogloss:0.159751\ttest-mlogloss:0.197516\n",
            "[903]\ttrain-mlogloss:0.159763\ttest-mlogloss:0.197527\n",
            "[904]\ttrain-mlogloss:0.159658\ttest-mlogloss:0.19749\n",
            "[905]\ttrain-mlogloss:0.159556\ttest-mlogloss:0.197478\n",
            "[906]\ttrain-mlogloss:0.159465\ttest-mlogloss:0.197421\n",
            "[907]\ttrain-mlogloss:0.159413\ttest-mlogloss:0.197426\n",
            "[908]\ttrain-mlogloss:0.159384\ttest-mlogloss:0.197386\n",
            "[909]\ttrain-mlogloss:0.159311\ttest-mlogloss:0.197395\n",
            "[910]\ttrain-mlogloss:0.15926\ttest-mlogloss:0.197379\n",
            "[911]\ttrain-mlogloss:0.159204\ttest-mlogloss:0.19734\n",
            "[912]\ttrain-mlogloss:0.159165\ttest-mlogloss:0.197315\n",
            "[913]\ttrain-mlogloss:0.159106\ttest-mlogloss:0.197261\n",
            "[914]\ttrain-mlogloss:0.159066\ttest-mlogloss:0.197268\n",
            "[915]\ttrain-mlogloss:0.158989\ttest-mlogloss:0.197261\n",
            "[916]\ttrain-mlogloss:0.158911\ttest-mlogloss:0.19727\n",
            "[917]\ttrain-mlogloss:0.158862\ttest-mlogloss:0.197318\n",
            "[918]\ttrain-mlogloss:0.158838\ttest-mlogloss:0.19733\n",
            "[919]\ttrain-mlogloss:0.158785\ttest-mlogloss:0.197296\n",
            "[920]\ttrain-mlogloss:0.158742\ttest-mlogloss:0.197265\n",
            "[921]\ttrain-mlogloss:0.158678\ttest-mlogloss:0.197265\n",
            "[922]\ttrain-mlogloss:0.158625\ttest-mlogloss:0.197251\n",
            "[923]\ttrain-mlogloss:0.158598\ttest-mlogloss:0.197281\n",
            "[924]\ttrain-mlogloss:0.158546\ttest-mlogloss:0.197291\n",
            "[925]\ttrain-mlogloss:0.158498\ttest-mlogloss:0.197323\n",
            "[926]\ttrain-mlogloss:0.158456\ttest-mlogloss:0.197308\n",
            "[927]\ttrain-mlogloss:0.158437\ttest-mlogloss:0.197296\n",
            "[928]\ttrain-mlogloss:0.158418\ttest-mlogloss:0.197299\n",
            "[929]\ttrain-mlogloss:0.158379\ttest-mlogloss:0.197298\n",
            "[930]\ttrain-mlogloss:0.158334\ttest-mlogloss:0.197323\n",
            "[931]\ttrain-mlogloss:0.158331\ttest-mlogloss:0.197336\n",
            "[932]\ttrain-mlogloss:0.158316\ttest-mlogloss:0.197348\n",
            "[933]\ttrain-mlogloss:0.158288\ttest-mlogloss:0.197294\n",
            "[934]\ttrain-mlogloss:0.158255\ttest-mlogloss:0.19729\n",
            "[935]\ttrain-mlogloss:0.158184\ttest-mlogloss:0.19728\n",
            "[936]\ttrain-mlogloss:0.158127\ttest-mlogloss:0.19723\n",
            "[937]\ttrain-mlogloss:0.158098\ttest-mlogloss:0.197216\n",
            "[938]\ttrain-mlogloss:0.158054\ttest-mlogloss:0.197225\n",
            "[939]\ttrain-mlogloss:0.157986\ttest-mlogloss:0.197218\n",
            "[940]\ttrain-mlogloss:0.157908\ttest-mlogloss:0.197183\n",
            "[941]\ttrain-mlogloss:0.157852\ttest-mlogloss:0.197171\n",
            "[942]\ttrain-mlogloss:0.157806\ttest-mlogloss:0.197168\n",
            "[943]\ttrain-mlogloss:0.157723\ttest-mlogloss:0.197197\n",
            "[944]\ttrain-mlogloss:0.157661\ttest-mlogloss:0.197154\n",
            "[945]\ttrain-mlogloss:0.157627\ttest-mlogloss:0.197117\n",
            "[946]\ttrain-mlogloss:0.157608\ttest-mlogloss:0.197119\n",
            "[947]\ttrain-mlogloss:0.15754\ttest-mlogloss:0.197082\n",
            "[948]\ttrain-mlogloss:0.157565\ttest-mlogloss:0.197109\n",
            "[949]\ttrain-mlogloss:0.157567\ttest-mlogloss:0.197122\n",
            "[950]\ttrain-mlogloss:0.157484\ttest-mlogloss:0.197066\n",
            "[951]\ttrain-mlogloss:0.157452\ttest-mlogloss:0.197066\n",
            "[952]\ttrain-mlogloss:0.15743\ttest-mlogloss:0.197067\n",
            "[953]\ttrain-mlogloss:0.157363\ttest-mlogloss:0.197038\n",
            "[954]\ttrain-mlogloss:0.157325\ttest-mlogloss:0.197049\n",
            "[955]\ttrain-mlogloss:0.15727\ttest-mlogloss:0.197037\n",
            "[956]\ttrain-mlogloss:0.157221\ttest-mlogloss:0.197052\n",
            "[957]\ttrain-mlogloss:0.157185\ttest-mlogloss:0.197003\n",
            "[958]\ttrain-mlogloss:0.157101\ttest-mlogloss:0.196978\n",
            "[959]\ttrain-mlogloss:0.157038\ttest-mlogloss:0.196971\n",
            "[960]\ttrain-mlogloss:0.157009\ttest-mlogloss:0.196963\n",
            "[961]\ttrain-mlogloss:0.156977\ttest-mlogloss:0.196958\n",
            "[962]\ttrain-mlogloss:0.156899\ttest-mlogloss:0.196946\n",
            "[963]\ttrain-mlogloss:0.156841\ttest-mlogloss:0.196931\n",
            "[964]\ttrain-mlogloss:0.156755\ttest-mlogloss:0.196903\n",
            "[965]\ttrain-mlogloss:0.15671\ttest-mlogloss:0.196905\n",
            "[966]\ttrain-mlogloss:0.156639\ttest-mlogloss:0.196843\n",
            "[967]\ttrain-mlogloss:0.156595\ttest-mlogloss:0.196827\n",
            "[968]\ttrain-mlogloss:0.156562\ttest-mlogloss:0.196794\n",
            "[969]\ttrain-mlogloss:0.156462\ttest-mlogloss:0.196766\n",
            "[970]\ttrain-mlogloss:0.1564\ttest-mlogloss:0.196734\n",
            "[971]\ttrain-mlogloss:0.15634\ttest-mlogloss:0.196691\n",
            "[972]\ttrain-mlogloss:0.156281\ttest-mlogloss:0.196719\n",
            "[973]\ttrain-mlogloss:0.156218\ttest-mlogloss:0.196735\n",
            "[974]\ttrain-mlogloss:0.156134\ttest-mlogloss:0.196681\n",
            "[975]\ttrain-mlogloss:0.156079\ttest-mlogloss:0.196684\n",
            "[976]\ttrain-mlogloss:0.15604\ttest-mlogloss:0.196713\n",
            "[977]\ttrain-mlogloss:0.155995\ttest-mlogloss:0.196691\n",
            "[978]\ttrain-mlogloss:0.155958\ttest-mlogloss:0.196707\n",
            "[979]\ttrain-mlogloss:0.15594\ttest-mlogloss:0.196738\n",
            "[980]\ttrain-mlogloss:0.155877\ttest-mlogloss:0.196731\n",
            "[981]\ttrain-mlogloss:0.15583\ttest-mlogloss:0.196691\n",
            "[982]\ttrain-mlogloss:0.155783\ttest-mlogloss:0.1967\n",
            "[983]\ttrain-mlogloss:0.155723\ttest-mlogloss:0.196674\n",
            "[984]\ttrain-mlogloss:0.155715\ttest-mlogloss:0.19665\n",
            "[985]\ttrain-mlogloss:0.15564\ttest-mlogloss:0.196646\n",
            "[986]\ttrain-mlogloss:0.155601\ttest-mlogloss:0.196654\n",
            "[987]\ttrain-mlogloss:0.155559\ttest-mlogloss:0.196641\n",
            "[988]\ttrain-mlogloss:0.15546\ttest-mlogloss:0.196625\n",
            "[989]\ttrain-mlogloss:0.155438\ttest-mlogloss:0.196626\n",
            "[990]\ttrain-mlogloss:0.155356\ttest-mlogloss:0.196627\n",
            "[991]\ttrain-mlogloss:0.155331\ttest-mlogloss:0.196615\n",
            "[992]\ttrain-mlogloss:0.15533\ttest-mlogloss:0.196647\n",
            "[993]\ttrain-mlogloss:0.155288\ttest-mlogloss:0.196572\n",
            "[994]\ttrain-mlogloss:0.155254\ttest-mlogloss:0.196536\n",
            "[995]\ttrain-mlogloss:0.155211\ttest-mlogloss:0.196528\n",
            "[996]\ttrain-mlogloss:0.155172\ttest-mlogloss:0.196521\n",
            "[997]\ttrain-mlogloss:0.155119\ttest-mlogloss:0.196497\n",
            "[998]\ttrain-mlogloss:0.155111\ttest-mlogloss:0.196541\n",
            "[999]\ttrain-mlogloss:0.155022\ttest-mlogloss:0.196492\n",
            "[[20060   427]\n",
            " [ 1517  1036]]\n",
            "{'Accuracy': 0.915625, 'Precision': 0.7081339712918661, 'Sensitivity_recall': 0.4057971014492754, 'Specificity': 0.9791575145214039, 'F1_score': 0.5159362549800797}\n",
            "Normalized confusion matrix\n",
            "[[0.97915751 0.02084249]\n",
            " [0.5942029  0.4057971 ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HnUGISACgBpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLxIEB3LYXry"
      },
      "source": [
        "# WARNING\n",
        "\n",
        "Do not continue running the cells below until you follow the steps in the next cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lPwUmniYB2k"
      },
      "source": [
        "## Change the names of the files in the deployed_model folder\n",
        "\n",
        "Directions: \n",
        "1. After running the above training code, grab the GAN, CNN, and XGB checkpoint models you desire. (Grab all 3 of the index, meta, and .data extension files for the desired model. For XGBBoost Model look for the **clf.pkl** file in the models folder. Move/Copy this to the deployed_model folder so that it is prepared for making predictions)\n",
        "2. Place/Copy the model (with its 3 files) into the folder called deployed_model (this folder should be in the directory that googlepath points to)\n",
        "3. This script will rename the model so that it runs for the **'Get the Predictions'** section below.\n",
        "4. After running script wait 10-15 seconds to see it update in the google driver folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzFuIWXtYCOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bd111e-e472-4041-a429-599171e9358f"
      },
      "source": [
        "import os\n",
        "\n",
        "# Below Code Gets Rid of Copy Of when making a copy of the models.\n",
        "for filename in os.listdir(f'{googlepath}deployed_model/'):\n",
        "    split = filename.split(\" \")\n",
        "    if len(split) == 3:\n",
        "        os.rename(f'{googlepath}deployed_model/{filename}', f'{googlepath}deployed_model/{split[2]}')\n",
        "    else:\n",
        "        print(\"Nothing to rename.\")\n",
        "\n",
        "# Below Code Gets Rid of Checkpoint Number in Name\n",
        "for filename in os.listdir(f'{googlepath}deployed_model/'):\n",
        "    split = filename.split(\".\")\n",
        "    if len(split) == 3:\n",
        "        prefix, _, suffix = split\n",
        "        new_name = prefix + \".\" + suffix\n",
        "        os.rename(f'{googlepath}deployed_model/{filename}', f'{googlepath}deployed_model/{new_name}')\n",
        "    elif filename == \"clf.pkl\":\n",
        "        os.rename(f'{googlepath}deployed_model/{filename}', f'{googlepath}deployed_model/xgb')\n",
        "    else:\n",
        "        print(\"Nothing to rename.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nothing to rename.\n",
            "Nothing to rename.\n",
            "Nothing to rename.\n",
            "Nothing to rename.\n",
            "Nothing to rename.\n",
            "Nothing to rename.\n",
            "Nothing to rename.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXGRuO7zw7JI"
      },
      "source": [
        "# Get the Predictions\n",
        "\n",
        "After following the above instruction and copying over the models to the deployed_model folder, run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmv5jskCw54L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e938b07-38e0-4989-dd0a-b056a577869b"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import xgboost as xgb\n",
        "#from sklearn.externals import joblib\n",
        "import joblib\n",
        "\n",
        "class Predict:\n",
        "\n",
        "    def __init__(self, num_historical_days=20, days=10, pct_change=0, \n",
        "                 gan_model=f'{googlepath}deployed_model/gan', \n",
        "                 xgb_model=f'{googlepath}deployed_model/xgb'):\n",
        "        self.data = []\n",
        "        self.num_historical_days = num_historical_days\n",
        "        self.gan_model = gan_model\n",
        "        self.xgb_model = xgb_model\n",
        "\n",
        "        files = [os.path.join(f'{googlepath}stock_data', f) for f in os.listdir(f'{googlepath}stock_data')]\n",
        "        for file in files:\n",
        "            print(file)\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df.sort_index(ascending=False)\n",
        "            df = df[['open', 'high', 'low', 'close', 'volume']]\n",
        "            df = ((df -\n",
        "                  df.rolling(num_historical_days).mean().shift(-num_historical_days)) /\n",
        "                  (df.rolling(num_historical_days).max().shift(-num_historical_days) -\n",
        "                  df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "            df = df.dropna()\n",
        "\n",
        "            \"\"\"\n",
        "            file.split --> is the symbol of the current file. Append a tuple of\n",
        "            that symbol and the dataframe index[0] which is the timestamp, and\n",
        "            thirdly append the data for 200 to 200 + num_historical_days values\n",
        "            (open, high, low, close, volume). For each symbol we have, we are\n",
        "            predicting based on the df[200:200+num_historical_days].values...\n",
        "            \"\"\"\n",
        "            self.data.append((file.split('/')[-1], df.index[0], df[200:200+num_historical_days].values))\n",
        "            \n",
        "    def gan_predict(self):\n",
        "        # clears the default graph stack and resets the global default graph.\n",
        "\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "        gan = GAN(num_features=5, num_historical_days=self.num_historical_days,\n",
        "                  generator_input_size=200, is_train=False)\n",
        "        # A class for running Tensorflow operations. A session object\n",
        "        # encapsulates the environment in which Operation objects are executed,\n",
        "        # and Tensor objects are evaluated. A session may own resources, such as\n",
        "        # tf.Variable, tf.QueueBase and tf.ReaderBase. It is important to\n",
        "        # release these resources when they are no longer required. Invoke\n",
        "        # tf.Session.close method on the session or use the session as a context\n",
        "        # manager. \n",
        "        # with tf.Session() as sess:\n",
        "        #   sess.run(...)\n",
        "        # or\n",
        "        # sess = tf.Session()\n",
        "        # sess.run(...)\n",
        "        # sess.close()     \n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver = tf.train.Saver()\n",
        "            saver.restore(sess, self.gan_model)\n",
        "            # Reconstruct a Python object from a file persisted with joblib.dump\n",
        "            clf = joblib.load(self.xgb_model)\n",
        "            for sym, date, data in self.data:\n",
        "                # run takes in feed_dict=None, session=None. A feed_dict is a\n",
        "                # dictionary that maps Tensor objects to feed values. In this\n",
        "                # case, I believe we are doing run( fetches, feed_dict=None...)\n",
        "                # case where the fetches is gan.features and the feed_dict\n",
        "                # points to the gan.X dictionary which points to data. The\n",
        "                # fetches argument may be a single graph element, or an\n",
        "                # arbitrarily nested list, tuple, namedtuple, dict, or\n",
        "                # OrderedDict containing graph elements at its leaves.\n",
        "#                 print(\"data: \", data)\n",
        "#                 print(\"data length: \", len(data))\n",
        "#                 print(\"gan features: \", gan.features)\n",
        "                try:\n",
        "\n",
        "                  features = sess.run(gan.features, feed_dict={gan.X:[data]})\n",
        "                  # Value returned by run() has the same shape as the fetches\n",
        "                  # argument, where the leaves are replaced by the corresponding\n",
        "                  # values returned by TensorFlow.  \n",
        "                  \n",
        "                  # xgb.DMatrix, construct one from either a dense matrix, a\n",
        "                  # sparse matrix, or a local file. Supported input file formats\n",
        "                  # are either a libsvm text file or a binary file that was\n",
        "                  # created previously by xgb.DMatrix.save. Internal data\n",
        "                  # structure that is used by XGBoost which is optimized for both\n",
        "                  # memory efficiency and training speed.\n",
        "                  features = xgb.DMatrix(features)\n",
        "                \n",
        "\n",
        "                  # The clf predict is the xgb classifier that is used on the gan\n",
        "                  # features (the flattened last layer of the convolutional neural\n",
        "                  # network, that is the discriminator). As far as I can tell, we\n",
        "                  # are using the GAN on the past 20 days to come up with some\n",
        "                  # features. Then these features are plugged into the XGBoost\n",
        "                  # Classifier. Then the XGBoost Classifier makes a prediction for\n",
        "                  # the stock (going Up or Down).\n",
        "                  print('{} {} {}'.format(str(date).split(' ')[0], sym, clf.predict(features)[0][1] > 0.5))\n",
        "                except Exception as e:\n",
        "                  print(Exception)\n",
        "\n",
        "\n",
        "\n",
        "p = Predict(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\n",
        "p.gan_predict()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AAPL.csv\n",
            "2023-01-04 AAPL.csv False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kofPWQhcCHWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}