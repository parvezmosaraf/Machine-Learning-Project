{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV9MfNTw1TRV",
        "outputId": "cb625b94-246b-4d88-eaac-4a2f131b808a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # keep using v1 syntax \n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7x8lBRR1UsP",
        "outputId": "3a9a8936-85d3-448c-f4b3-8514e622835b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "googlepath = \"/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/\"\n",
        "ALPHA_VANTAGE_KEY = \"YFUTWYHB7TY1CD74\" # This is a backup key.\n",
        "\n",
        "# Setting the Training Amount\n",
        "TRAINING_AMOUNT = 50000 # Number of steps to train the models\n",
        "SAVE_STEPS_AMOUNT = 10000 # Frequency of which to save the models\n",
        "PCT_CHANGE_AMOUNT = 5 # Percent Change Amount\n",
        "HISTORICAL_DAYS_AMOUNT = 20 # Window to use for historical dayss\n",
        "DAYS_AHEAD = 5 # How many days ahead to look"
      ],
      "metadata": {
        "id": "dFukafxb1UvG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Downloads stock data from alphavantage\n",
        "'''\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import urllib\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "import io\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "#Note should have companylist.csv in the directory with this file.\n",
        "\n",
        "# Add stock_data folder\n",
        "if not os.path.exists(f'{googlepath}stock_data'):\n",
        "    os.makedirs(f'{googlepath}stock_data')\n",
        "\n",
        "'''\n",
        "Saves data to a file\n",
        "'''\n",
        "def save(googlepath, stock_csv, output_dir, filename):\n",
        "    filepath = os.path.join(googlepath, output_dir, filename)\n",
        "    try:\n",
        "        df = stock_csv\n",
        "        df.to_csv(filepath, index=False)\n",
        "    except Exception as ex:\n",
        "        print('Could not open file {} to write data'.format(filepath))\n",
        "        print(ex)\n",
        "\n",
        "\n",
        "def try_download(symbol):\n",
        "    try:\n",
        "        # Keep call frequency below threshold \n",
        "        time.sleep(15) # (5 per minute allowed for free service)\n",
        "        url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&apikey={}&datatype=csv&outputsize=full'.format(symbol, ALPHA_VANTAGE_KEY)\n",
        "        df = pd.read_csv(url)\n",
        "        df = df.drop(['split_coefficient', 'dividend_amount', 'adjusted_close'], axis=1)\n",
        "        return df, True\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "#Given a stock symbol (aka 'tsla') will download and save the data to the\n",
        "#output dir as a csv \n",
        "def download_symbol(symbol, output_dir, retry_count=4):\n",
        "\n",
        "    stock_csv, didPass = try_download(symbol)\n",
        "    if didPass:\n",
        "        save(googlepath, stock_csv, output_dir, '{}.csv'.format(symbol))\n",
        "    else:\n",
        "        print('Failed to download {}'.format(symbol))\n",
        "\n",
        "df = pd.read_csv(f\"{googlepath}companylist.csv\")\n",
        "for symbol in df.Symbol:\n",
        "    my_file = Path(f\"{googlepath}stock_data/{symbol}.csv\")  # check if already downloaded\n",
        "    if not my_file.exists():\n",
        "        print('Downloading {}'.format(symbol))\n",
        "        download_symbol(symbol, 'stock_data')\n",
        "    else:\n",
        "        print(f\"Already downloaded {symbol}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcaOWmEI1Uxm",
        "outputId": "8a8d6191-9f47-4f02-9db8-1a61f7670fc7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MMM\n",
            "Downloading AOS\n",
            "Downloading ABT\n",
            "Downloading ABBV\n",
            "Downloading ACN\n",
            "Downloading ATVI\n",
            "Downloading AYI\n",
            "Downloading ADBE\n",
            "Downloading AAP\n",
            "Downloading AMD\n",
            "Downloading AES\n",
            "Downloading AET\n",
            "Downloading AMG\n",
            "Downloading AFL\n",
            "Downloading A\n",
            "Downloading APD\n",
            "Downloading AKAM\n",
            "Downloading ALK\n",
            "Downloading ALB\n",
            "Downloading ARE\n",
            "Downloading ALXN\n",
            "Downloading ALGN\n",
            "Downloading ALLE\n",
            "Downloading AGN\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download AGN\n",
            "Downloading ADS\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download ADS\n",
            "Downloading LNT\n",
            "Downloading ALL\n",
            "Downloading GOOGL\n",
            "Downloading GOOG\n",
            "Downloading MO\n",
            "Downloading AMZN\n",
            "Downloading AEE\n",
            "Downloading AAL\n",
            "Downloading AEP\n",
            "Downloading AXP\n",
            "Downloading AIG\n",
            "Downloading AMT\n",
            "Downloading AWK\n",
            "Downloading AMP\n",
            "Downloading ABC\n",
            "Downloading AME\n",
            "Downloading AMGN\n",
            "Downloading APH\n",
            "Downloading APC\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download APC\n",
            "Downloading ADI\n",
            "Downloading ANDV\n",
            "Downloading ANSS\n",
            "Downloading ANTM\n",
            "\"['split_coefficient' 'dividend_amount' 'adjusted_close'] not found in axis\"\n",
            "Failed to download ANTM\n",
            "Downloading AON\n",
            "Downloading APA\n",
            "Downloading AIV\n",
            "Downloading AAPL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "s91gnxs81U0D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "\n",
        "class GAN():\n",
        "\n",
        "    def sample_Z(self, batch_size, n):\n",
        "        return np.random.uniform(-1., 1., size=(batch_size, n))\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, generator_input_size=200, is_train=True):\n",
        "        def get_batch_norm_with_global_normalization_vars(size):\n",
        "            v = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            m = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            beta = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            gamma = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            return v, m, beta, gamma\n",
        "\n",
        "        self.X = tf.compat.v1.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Z = tf.compat.v1.placeholder(tf.float32, shape=[None, generator_input_size])\n",
        "\n",
        "        generator_output_size = num_features*num_historical_days\n",
        "        with tf.variable_scope(\"generator\"):\n",
        "            W1 = tf.Variable(tf.truncated_normal([generator_input_size, generator_output_size*10]))\n",
        "            b1 = tf.Variable(tf.truncated_normal([generator_output_size*10]))\n",
        "\n",
        "            h1 = tf.nn.sigmoid(tf.matmul(self.Z, W1) + b1)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([generator_output_size*10, generator_output_size*5]))\n",
        "            b2 = tf.Variable(tf.truncated_normal([generator_output_size*5]))\n",
        "\n",
        "            h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)\n",
        "\n",
        "            W3 = tf.Variable(tf.truncated_normal([generator_output_size*5, generator_output_size]))\n",
        "            b3 = tf.Variable(tf.truncated_normal([generator_output_size]))\n",
        "\n",
        "            g_log_prob = tf.matmul(h2, W3) + b3\n",
        "            g_log_prob = tf.reshape(g_log_prob, [-1, num_historical_days, 1, num_features])\n",
        "            self.gen_data = tf.reshape(g_log_prob, [-1, num_historical_days, num_features])\n",
        "\n",
        "            theta_G = [W1, b1, W2, b2, W3, b3]\n",
        "\n",
        "\n",
        "\n",
        "        with tf.variable_scope(\"discriminator\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "\n",
        "            v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(32)\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "\n",
        "            v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(64)\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 64, 128],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([128], dtype=tf.float32))\n",
        "\n",
        "            v3, m3, beta3, gamma3 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*128, 128]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([128]))\n",
        "\n",
        "            v4, m4, beta4, gamma4 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([128, 1]))\n",
        "\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]\n",
        "\n",
        "        def discriminator(X):\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            pool = relu\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            pool = relu\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=0.8)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=0.8)\n",
        "\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "            D_logit = tf.matmul(h1, W2)\n",
        "            D_prob = tf.nn.sigmoid(D_logit)\n",
        "            return D_prob, D_logit, features\n",
        "\n",
        "        D_real, D_logit_real, self.features = discriminator(X)\n",
        "        D_fake, D_logit_fake, _ = discriminator(g_log_prob)\n",
        "\n",
        "\n",
        "        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
        "        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
        "        self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n",
        "        self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n",
        "        self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n",
        "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n",
        "\n",
        "\n",
        "        self.D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.D_loss, var_list=theta_D)\n",
        "        self.G_solver = tf.train.AdamOptimizer(learning_rate=0.000055).minimize(self.G_loss, var_list=theta_G)\n"
      ],
      "metadata": {
        "id": "aUqCmuJa1U4n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "random.seed(42)\n",
        "class TrainGan:\n",
        "\n",
        "    def __init__(self, num_historical_days, batch_size=128):\n",
        "        self.batch_size = batch_size\n",
        "        self.data = []\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n",
        "      \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            #Read in file -- note that parse_dates will be need later\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df[['open','high','low','close','volume']]\n",
        "\n",
        "            #Normilize using a of size num_historical_days\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "\n",
        "            #Drop days that we don't have data for\n",
        "            df = df.dropna()\n",
        "            \n",
        "            #Hold out 500 days for testing\n",
        "            df = df[500:]\n",
        "\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                self.data.append(df.values[i-num_historical_days:i])\n",
        "\n",
        "        self.gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200)\n",
        "\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        while True:\n",
        "            batch.append(random.choice(self.data))\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "    def train(self, print_steps=100, display_data=100, save_steps=SAVE_STEPS_AMOUNT):\n",
        "        if not os.path.exists(f'{googlepath}models'):\n",
        "            os.makedirs(f'{googlepath}models')\n",
        "        sess = tf.Session()\n",
        "        \n",
        "        G_loss = 0\n",
        "        D_loss = 0\n",
        "        G_l2_loss = 0\n",
        "        D_l2_loss = 0\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        g_loss_array = []\n",
        "        d_loss_array = []\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}models/checkpoint'):\n",
        "                with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, X in enumerate(self.random_batch(self.batch_size)):\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "            if i % 1 == 0:\n",
        "                _, D_loss_curr, D_l2_loss_curr = sess.run([self.gan.D_solver, self.gan.D_loss, self.gan.D_l2_loss], feed_dict=\n",
        "                        {self.gan.X:X, self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                D_loss += D_loss_curr\n",
        "                D_l2_loss += D_l2_loss_curr\n",
        "            if i % 1 == 0:\n",
        "                _, G_loss_curr, G_l2_loss_curr = sess.run([self.gan.G_solver, self.gan.G_loss, self.gan.G_l2_loss],\n",
        "                        feed_dict={self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                G_loss += G_loss_curr\n",
        "                G_l2_loss += G_l2_loss_curr\n",
        "                \n",
        "            g_loss_array.append(G_loss_curr - G_l2_loss)\n",
        "            d_loss_array.append(D_loss_curr - D_l2_loss)\n",
        "            \n",
        "            \n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} D_loss={}, G_loss={}'.format(i + int(currentStep), D_loss/print_steps - D_l2_loss/print_steps, G_loss/print_steps - G_l2_loss/print_steps))\n",
        "                G_loss = 0\n",
        "                D_loss = 0\n",
        "                G_l2_loss = 0\n",
        "                D_l2_loss = 0\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess, f'{googlepath}/models/gan.ckpt', i + int(currentStep))\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if ((i + int(currentStep)) > TRAINING_AMOUNT):\n",
        "                \n",
        "                print(\"Reached {} epochs for GAN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                \n",
        "                axisX = np.arange(0,len(g_loss_array),1)\n",
        "                plt.plot(axisX, g_loss_array, label='generator loss')\n",
        "                plt.plot(axisX, d_loss_array, label='discriminator loss')\n",
        "                plt.legend()\n",
        "                plt.title('generator and discriminator loss')\n",
        "                plt.show()\n",
        "                \n",
        "                break\n",
        "\n",
        "            # if (i+1) % display_data == 0:\n",
        "            #     print('Generated Data')\n",
        "            #     print(sess.run(self.gan.gen_data, feed_dict={self.gan.Z:self.gan.sample_Z(1, 200)}))\n",
        "            #     print('Real Data')\n",
        "            #     print(X[0])\n",
        "\n",
        "\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "gan = TrainGan(HISTORICAL_DAYS_AMOUNT, 128)\n",
        "gan.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8JpzaKEW1U7O",
        "outputId": "2124a3b3-9007-42f3-d85e-af523ed53bc5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/MMM.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AOS.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ABT.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ABBV.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ACN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ATVI.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AYI.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ADBE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AAP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMD.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AES.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AET.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMG.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AFL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/A.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/APD.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AKAM.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALK.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALB.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ARE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALXN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALLE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/LNT.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/GOOGL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/GOOG.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/MO.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMZN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AEE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AAL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AEP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AXP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AIG.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMT.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AWK.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ABC.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AME.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/APH.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ADI.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ANDV.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ANSS.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AON.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/APA.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AIV.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AAPL.csv\n",
            "Step=99 D_loss=18.26511024475098, G_loss=738.6002155894041\n",
            "Step=199 D_loss=3.7253373324871064, G_loss=789.1806599375606\n",
            "Step=299 D_loss=2.053569413423538, G_loss=755.8169658255578\n",
            "Step=399 D_loss=1.643549338579178, G_loss=738.883019644022\n",
            "Step=499 D_loss=1.1810955488681796, G_loss=719.4403593716025\n",
            "Step=599 D_loss=0.9266499888896944, G_loss=693.4276154616475\n",
            "Step=699 D_loss=0.9197992682456972, G_loss=701.4697079974413\n",
            "Step=799 D_loss=0.5728976988792418, G_loss=679.0368849825859\n",
            "Step=899 D_loss=0.5569627201557159, G_loss=667.6771029201151\n",
            "Step=999 D_loss=0.3857983529567719, G_loss=675.643302912116\n",
            "Step=1099 D_loss=0.382789237499237, G_loss=675.0443686944246\n",
            "Step=1199 D_loss=0.33692786216735837, G_loss=672.4471126207709\n",
            "Step=1299 D_loss=0.25271836638450607, G_loss=665.397859905064\n",
            "Step=1399 D_loss=0.2712855339050293, G_loss=670.0858827820421\n",
            "Step=1499 D_loss=0.2987022626399993, G_loss=680.4937644329666\n",
            "Step=1599 D_loss=0.2866641426086425, G_loss=670.2530804097653\n",
            "Step=1699 D_loss=0.1779555594921114, G_loss=634.6299470424651\n",
            "Step=1799 D_loss=0.23684638619422915, G_loss=649.109184575975\n",
            "Step=1899 D_loss=0.2494929242134094, G_loss=622.9541376721859\n",
            "Step=1999 D_loss=0.13694852232933052, G_loss=636.0895228233933\n",
            "Step=2099 D_loss=0.20442241787910453, G_loss=623.0120568192004\n",
            "Step=2199 D_loss=0.0760408473014833, G_loss=588.1485428968072\n",
            "Step=2299 D_loss=0.10534237742424013, G_loss=569.2540158459544\n",
            "Step=2399 D_loss=0.06595147609710694, G_loss=548.0601454508304\n",
            "Step=2499 D_loss=0.19439715504646293, G_loss=512.4151074123382\n",
            "Step=2599 D_loss=0.1081986594200135, G_loss=478.7901231917739\n",
            "Step=2699 D_loss=0.09643510699272162, G_loss=467.44710769742727\n",
            "Step=2799 D_loss=0.09099211454391476, G_loss=433.1853869044781\n",
            "Step=2899 D_loss=0.11428378701210007, G_loss=402.80322148710485\n",
            "Step=2999 D_loss=0.06789359450340271, G_loss=390.57541564762596\n",
            "Step=3099 D_loss=0.059392930269241306, G_loss=386.54161371141674\n",
            "Step=3199 D_loss=0.04192877411842355, G_loss=379.19086085617545\n",
            "Step=3299 D_loss=0.05590666890144336, G_loss=353.92388420730833\n",
            "Step=3399 D_loss=0.047255206108093306, G_loss=344.23448211103675\n",
            "Step=3499 D_loss=0.049410891532897905, G_loss=308.19610284090044\n",
            "Step=3599 D_loss=0.03669391393661492, G_loss=275.79615204274654\n",
            "Step=3699 D_loss=0.056589028835296595, G_loss=249.93185597956182\n",
            "Step=3799 D_loss=0.03440751671791076, G_loss=230.75364443272352\n",
            "Step=3899 D_loss=0.07977161526679999, G_loss=210.10655096560717\n",
            "Step=3999 D_loss=0.05955416083335874, G_loss=193.1284634590149\n",
            "Step=4099 D_loss=0.09957322120666512, G_loss=186.85996596187354\n",
            "Step=4199 D_loss=0.11822810649871829, G_loss=172.25904980242254\n",
            "Step=4299 D_loss=0.21411822199821473, G_loss=150.30678665220736\n",
            "Step=4399 D_loss=0.4759066283702851, G_loss=148.75292757242917\n",
            "Step=4499 D_loss=0.7105576467514039, G_loss=122.04084377765656\n",
            "Step=4599 D_loss=0.8983703339099887, G_loss=83.85977302461863\n",
            "Step=4699 D_loss=0.9937821578979493, G_loss=57.10339155286551\n",
            "Step=4799 D_loss=1.2438342797756194, G_loss=48.21430800229311\n",
            "Step=4899 D_loss=0.8375151848793028, G_loss=39.70368255794048\n",
            "Step=4999 D_loss=0.9179114341735841, G_loss=35.33349889069795\n",
            "Step=5099 D_loss=1.1691505658626555, G_loss=27.36541807204485\n",
            "Step=5199 D_loss=0.946547886133194, G_loss=23.72929663538933\n",
            "Step=5299 D_loss=0.4887988364696503, G_loss=21.90235437631607\n",
            "Step=5399 D_loss=1.0994825220108033, G_loss=21.50166136831045\n",
            "Step=5499 D_loss=0.7271427536010742, G_loss=16.421001477837564\n",
            "Step=5599 D_loss=0.5799384069442748, G_loss=15.940403268039228\n",
            "Step=5699 D_loss=0.8948023986816407, G_loss=14.7869685074687\n",
            "Step=5799 D_loss=0.9760220372676849, G_loss=13.053541005551816\n",
            "Step=5899 D_loss=0.8941349256038666, G_loss=11.314882120490074\n",
            "Step=5999 D_loss=0.8152556610107422, G_loss=10.608698294162751\n",
            "Step=6099 D_loss=0.9195863807201385, G_loss=10.189728098511697\n",
            "Step=6199 D_loss=0.7675648796558379, G_loss=10.774634121656419\n",
            "Step=6299 D_loss=0.8343440628051755, G_loss=10.857816175520421\n",
            "Step=6399 D_loss=0.8147185420989989, G_loss=8.125942980051041\n",
            "Step=6499 D_loss=0.726359233856201, G_loss=9.693751980662347\n",
            "Step=6599 D_loss=0.9290557849407195, G_loss=8.90819594115019\n",
            "Step=6699 D_loss=0.8215488696098328, G_loss=7.677602026760578\n",
            "Step=6799 D_loss=0.7438711082935334, G_loss=9.13839485168457\n",
            "Step=6899 D_loss=1.3827980303764342, G_loss=8.416328567266465\n",
            "Step=6999 D_loss=1.0928487432003022, G_loss=7.055683563053608\n",
            "Step=7099 D_loss=1.1524068081378935, G_loss=5.522212435901165\n",
            "Step=7199 D_loss=1.1134549713134767, G_loss=5.561373353302479\n",
            "Step=7299 D_loss=0.727007907629013, G_loss=6.255508043766022\n",
            "Step=7399 D_loss=1.5094114470481874, G_loss=5.75128573089838\n",
            "Step=7499 D_loss=0.5898052024841307, G_loss=6.789080854356289\n",
            "Step=7599 D_loss=1.2139902973175047, G_loss=6.19734972178936\n",
            "Step=7699 D_loss=0.8997017955780031, G_loss=5.531723924577236\n",
            "Step=7799 D_loss=0.6905064511299133, G_loss=6.256796082258225\n",
            "Step=7899 D_loss=1.017025557756424, G_loss=4.122744510471821\n",
            "Step=7999 D_loss=1.0751161789894106, G_loss=3.8947887048125263\n",
            "Step=8099 D_loss=0.9529518306255338, G_loss=4.216325988769531\n",
            "Step=8199 D_loss=0.5389473497867585, G_loss=5.326961851119996\n",
            "Step=8299 D_loss=1.2074114680290224, G_loss=3.988481554985046\n",
            "Step=8399 D_loss=1.2679794394969939, G_loss=3.2841316321492195\n",
            "Step=8499 D_loss=1.0076055204868315, G_loss=3.071962770819664\n",
            "Step=8599 D_loss=1.0083077013492585, G_loss=3.093772152364254\n",
            "Step=8699 D_loss=0.8966771352291107, G_loss=2.7567569148540496\n",
            "Step=8799 D_loss=0.8445049190521239, G_loss=2.9402984511852264\n",
            "Step=8899 D_loss=0.870165935754776, G_loss=2.922746589779854\n",
            "Step=8999 D_loss=0.8538187992572785, G_loss=2.6691754618287087\n",
            "Step=9099 D_loss=1.2705020368099214, G_loss=3.7717099153995512\n",
            "Step=9199 D_loss=1.2408394730091095, G_loss=2.918529157936573\n",
            "Step=9299 D_loss=1.128074117898941, G_loss=2.4974565270543096\n",
            "Step=9399 D_loss=1.0938910090923308, G_loss=2.7759109777212143\n",
            "Step=9499 D_loss=1.07500492811203, G_loss=2.9404566648602484\n",
            "Step=9599 D_loss=0.8859353673458101, G_loss=2.308061963915825\n",
            "Step=9699 D_loss=0.981680737733841, G_loss=3.3227991890907287\n",
            "Step=9799 D_loss=0.8524326169490815, G_loss=3.508165412545204\n",
            "Step=9899 D_loss=1.5214676678180696, G_loss=2.8107794791460035\n",
            "Step=9999 D_loss=0.9945526051521298, G_loss=3.2528937566280365\n",
            "Step=10099 D_loss=1.2811443996429444, G_loss=2.07901522308588\n",
            "Step=10199 D_loss=1.0904301738739013, G_loss=3.5760220327973364\n",
            "Step=10299 D_loss=0.9810243296623229, G_loss=3.4967382761836054\n",
            "Step=10399 D_loss=1.0020013570785522, G_loss=3.525615718066692\n",
            "Step=10499 D_loss=0.8069870495796203, G_loss=3.8261384174227717\n",
            "Step=10599 D_loss=1.0493361222743989, G_loss=2.542258343696594\n",
            "Step=10699 D_loss=1.4712843406200409, G_loss=1.8466371738910674\n",
            "Step=10799 D_loss=0.9816660141944886, G_loss=1.7792465364933012\n",
            "Step=10899 D_loss=0.5090074968338014, G_loss=2.12782745718956\n",
            "Step=10999 D_loss=0.9686456787586211, G_loss=1.838643784224987\n",
            "Step=11099 D_loss=1.2418195068836215, G_loss=3.3228688767552375\n",
            "Step=11199 D_loss=1.1434635400772095, G_loss=2.5867293742299076\n",
            "Step=11299 D_loss=0.7594726395606994, G_loss=3.8476394307613373\n",
            "Step=11399 D_loss=1.236464959383011, G_loss=2.0671769240498543\n",
            "Step=11499 D_loss=0.7584072613716126, G_loss=2.9974327009916304\n",
            "Step=11599 D_loss=1.073323916196823, G_loss=1.786433844268322\n",
            "Step=11699 D_loss=1.027787232398987, G_loss=2.151916019618511\n",
            "Step=11799 D_loss=0.5700975918769837, G_loss=2.997923555672169\n",
            "Step=11899 D_loss=1.5061966478824615, G_loss=1.5470053422451018\n",
            "Step=11999 D_loss=0.8849495649337769, G_loss=2.1966116240620615\n",
            "Step=12099 D_loss=1.095312478542328, G_loss=2.104230138063431\n",
            "Step=12199 D_loss=1.0791617608070372, G_loss=1.6516736081242562\n",
            "Step=12299 D_loss=0.7702204060554505, G_loss=2.175386663675308\n",
            "Step=12399 D_loss=1.039527906179428, G_loss=2.446155242025852\n",
            "Step=12499 D_loss=1.151026837825775, G_loss=1.4651623862981795\n",
            "Step=12599 D_loss=0.9073134744167328, G_loss=1.8546585351228713\n",
            "Step=12699 D_loss=1.2438872122764586, G_loss=1.915414822101593\n",
            "Step=12799 D_loss=0.7883958160877229, G_loss=1.8132914462685588\n",
            "Step=12899 D_loss=0.8275360858440399, G_loss=2.6158925175666807\n",
            "Step=12999 D_loss=1.1796466970443724, G_loss=1.646492556929588\n",
            "Step=13099 D_loss=1.1546545279026033, G_loss=3.7195825049281117\n",
            "Step=13199 D_loss=1.1108686256408693, G_loss=1.850120171904564\n",
            "Step=13299 D_loss=0.6627791047096252, G_loss=2.830357362627983\n",
            "Step=13399 D_loss=1.4958945083618165, G_loss=1.524430987536907\n",
            "Step=13499 D_loss=1.047605644464493, G_loss=1.8464745235443116\n",
            "Step=13599 D_loss=1.1165410411357881, G_loss=1.59807866692543\n",
            "Step=13699 D_loss=0.7489629602432251, G_loss=2.608050157725811\n",
            "Step=13799 D_loss=1.204077900648117, G_loss=2.226155413389206\n",
            "Step=13899 D_loss=1.2003167402744295, G_loss=3.0552782902121542\n",
            "Step=13999 D_loss=0.6394527399539949, G_loss=3.9630760502815248\n",
            "Step=14099 D_loss=0.9843442106246949, G_loss=1.9701830944418905\n",
            "Step=14199 D_loss=0.5349624717235566, G_loss=5.753187628686428\n",
            "Step=14299 D_loss=0.9717796456813812, G_loss=4.307591493725777\n",
            "Step=14399 D_loss=1.1312905061244964, G_loss=1.706232046186924\n",
            "Step=14499 D_loss=0.7627225947380065, G_loss=1.941151459515095\n",
            "Step=14599 D_loss=1.057996029853821, G_loss=1.9844007718563081\n",
            "Step=14699 D_loss=0.8133841288089751, G_loss=3.265668778717518\n",
            "Step=14799 D_loss=1.024671813249588, G_loss=1.8218683266639708\n",
            "Step=14899 D_loss=1.4970595753192901, G_loss=1.284956488609314\n",
            "Step=14999 D_loss=0.8247311043739318, G_loss=1.4029419884085654\n",
            "Step=15099 D_loss=0.8722844684123994, G_loss=1.4859207546710969\n",
            "Step=15199 D_loss=0.9037335574626921, G_loss=2.0652430742979053\n",
            "Step=15299 D_loss=0.8364248585700989, G_loss=2.8297726792097095\n",
            "Step=15399 D_loss=1.305998457670212, G_loss=1.9462100359797476\n",
            "Step=15499 D_loss=0.6277065777778625, G_loss=1.9748379206657407\n",
            "Step=15599 D_loss=1.0554633069038388, G_loss=2.2319616717100144\n",
            "Step=15699 D_loss=0.35390192508697527, G_loss=4.144630470275879\n",
            "Step=15799 D_loss=0.9561717176437379, G_loss=1.547033477127552\n",
            "Step=15899 D_loss=0.8379797303676604, G_loss=1.9898214107751846\n",
            "Step=15999 D_loss=0.6888591396808623, G_loss=1.685575394630432\n",
            "Step=16099 D_loss=0.876761530637741, G_loss=1.884682776629925\n",
            "Step=16199 D_loss=0.7134269750118254, G_loss=2.4248829847574234\n",
            "Step=16299 D_loss=1.3768219625949858, G_loss=1.6197656783461571\n",
            "Step=16399 D_loss=0.6959551149606704, G_loss=2.8172462770342825\n",
            "Step=16499 D_loss=1.3625213801860807, G_loss=1.463252059519291\n",
            "Step=16599 D_loss=0.8546609759330749, G_loss=1.9655986678600312\n",
            "Step=16699 D_loss=0.7902403849363328, G_loss=1.607412535250187\n",
            "Step=16799 D_loss=1.04885869204998, G_loss=2.046980670392513\n",
            "Step=16899 D_loss=0.7251857888698579, G_loss=3.369540411233902\n",
            "Step=16999 D_loss=1.6815840697288513, G_loss=1.3062149202823639\n",
            "Step=17099 D_loss=0.9311856180429459, G_loss=1.7983736011385918\n",
            "Step=17199 D_loss=0.7375284039974213, G_loss=2.3341564846038816\n",
            "Step=17299 D_loss=0.5234794467687607, G_loss=4.274057039618492\n",
            "Step=17399 D_loss=0.6789957255125046, G_loss=3.0622887733578685\n",
            "Step=17499 D_loss=0.8514504057168961, G_loss=1.486859719455242\n",
            "Step=17599 D_loss=0.5645334470272065, G_loss=2.7886101359128954\n",
            "Step=17699 D_loss=0.5052067410945892, G_loss=3.589019455611706\n",
            "Step=17799 D_loss=1.1545867109298706, G_loss=1.8103505861759184\n",
            "Step=17899 D_loss=0.6948768329620363, G_loss=2.8104254716634753\n",
            "Step=17999 D_loss=0.9683947092294694, G_loss=1.58498300164938\n",
            "Step=18099 D_loss=0.7710484826564789, G_loss=4.332929706573486\n",
            "Step=18199 D_loss=1.289767359495163, G_loss=2.3165047374367713\n",
            "Step=18299 D_loss=0.9701112055778502, G_loss=4.345360099971295\n",
            "Step=18399 D_loss=1.1106589329242706, G_loss=3.588658137619495\n",
            "Step=18499 D_loss=1.217295415997505, G_loss=1.9837963891029358\n",
            "Step=18599 D_loss=0.7518515384197235, G_loss=2.737518683671951\n",
            "Step=18699 D_loss=0.7641553449630737, G_loss=1.8646187967061998\n",
            "Step=18799 D_loss=1.0510084301233291, G_loss=1.407833756506443\n",
            "Step=18899 D_loss=0.7999082517623901, G_loss=2.9916988584399222\n",
            "Step=18999 D_loss=0.705515352487564, G_loss=2.2817106774449347\n",
            "Step=19099 D_loss=1.2179121732711793, G_loss=1.8514622956514357\n",
            "Step=19199 D_loss=0.6585760939121247, G_loss=2.556979879140854\n",
            "Step=19299 D_loss=0.7514535015821457, G_loss=1.7742386347055432\n",
            "Step=19399 D_loss=1.1215306049585343, G_loss=1.6360829406976698\n",
            "Step=19499 D_loss=0.6083725553750992, G_loss=1.8211615604162217\n",
            "Step=19599 D_loss=0.7214494198560714, G_loss=2.5797833889722823\n",
            "Step=19699 D_loss=0.6977885091304779, G_loss=3.3003513476252557\n",
            "Step=19799 D_loss=1.0424271750450136, G_loss=2.8355203539133074\n",
            "Step=19899 D_loss=0.954539789557457, G_loss=6.786070820391178\n",
            "Step=19999 D_loss=1.5029190319776533, G_loss=2.096917087137699\n",
            "Step=20099 D_loss=0.7535306280851365, G_loss=1.6643053129315377\n",
            "Step=20199 D_loss=0.6924369359016418, G_loss=3.2603539812564852\n",
            "Step=20299 D_loss=0.8362129843235016, G_loss=2.3038646948337558\n",
            "Step=20399 D_loss=1.0450270158052444, G_loss=1.9647437989711762\n",
            "Step=20499 D_loss=0.6421868884563446, G_loss=2.28671467512846\n",
            "Step=20599 D_loss=0.6632497304677962, G_loss=2.066525102555752\n",
            "Step=20699 D_loss=0.5774309253692627, G_loss=2.291823724806309\n",
            "Step=20799 D_loss=1.023337385058403, G_loss=1.6649372839927674\n",
            "Step=20899 D_loss=1.4500560265779496, G_loss=1.3759723350405695\n",
            "Step=20999 D_loss=0.8964291435480118, G_loss=3.4724813404679296\n",
            "Step=21099 D_loss=0.3884729170799255, G_loss=4.089949658513069\n",
            "Step=21199 D_loss=0.6941331535577775, G_loss=1.9846257743239404\n",
            "Step=21299 D_loss=0.6071125704050064, G_loss=3.7623738774657247\n",
            "Step=21399 D_loss=1.2312469905614853, G_loss=2.0019346100091933\n",
            "Step=21499 D_loss=0.8332167679071425, G_loss=2.0216807669401167\n",
            "Step=21599 D_loss=1.0672550791501998, G_loss=2.9506275177001955\n",
            "Step=21699 D_loss=0.6242988783121108, G_loss=2.165883874595165\n",
            "Step=21799 D_loss=0.4951976937055589, G_loss=2.063778372108936\n",
            "Step=21899 D_loss=0.4888369977474212, G_loss=2.0414380645751953\n",
            "Step=21999 D_loss=2.058709908127785, G_loss=5.651546187400818\n",
            "Step=22099 D_loss=0.9868871217966081, G_loss=4.272902266085148\n",
            "Step=22199 D_loss=1.5059019392728805, G_loss=2.0114455723762514\n",
            "Step=22299 D_loss=0.8969088011980058, G_loss=3.214435448646545\n",
            "Step=22399 D_loss=0.6471892267465591, G_loss=1.9153009471297264\n",
            "Step=22499 D_loss=1.1950823068618772, G_loss=2.1277979990839957\n",
            "Step=22599 D_loss=1.113996832370758, G_loss=2.3477147608995437\n",
            "Step=22699 D_loss=0.9403811949491502, G_loss=2.7005618932843207\n",
            "Step=22799 D_loss=0.8744738221168518, G_loss=1.519299417734146\n",
            "Step=22899 D_loss=0.7461291545629501, G_loss=2.0479223352670672\n",
            "Step=22999 D_loss=1.0441544359922408, G_loss=4.314331410825252\n",
            "Step=23099 D_loss=1.2610952025651931, G_loss=1.6811593627929688\n",
            "Step=23199 D_loss=0.5968580228090286, G_loss=1.7101614779233931\n",
            "Step=23299 D_loss=0.713188943862915, G_loss=2.7274097102880477\n",
            "Step=23399 D_loss=0.9350287175178527, G_loss=1.6038099122047424\n",
            "Step=23499 D_loss=0.9162761932611465, G_loss=1.7212894123792648\n",
            "Step=23599 D_loss=0.7257600569725037, G_loss=5.348562952280044\n",
            "Step=23699 D_loss=1.2579533123970035, G_loss=1.7703622153401377\n",
            "Step=23799 D_loss=0.6428735041618348, G_loss=1.6587942564487457\n",
            "Step=23899 D_loss=0.8616320478916167, G_loss=4.817999349832535\n",
            "Step=23999 D_loss=0.8417548280954362, G_loss=1.8916814148426055\n",
            "Step=24099 D_loss=0.6649915045499802, G_loss=2.265258955955505\n",
            "Step=24199 D_loss=0.7658072841167451, G_loss=1.8116799750924113\n",
            "Step=24299 D_loss=0.7125862157344818, G_loss=2.796671114265919\n",
            "Step=24399 D_loss=0.40193115532398227, G_loss=4.799632989764214\n",
            "Step=24499 D_loss=1.0090408849716188, G_loss=1.6369989436864854\n",
            "Step=24599 D_loss=1.0488107073307038, G_loss=2.0563880029320716\n",
            "Step=24699 D_loss=0.9841994041204453, G_loss=1.4214343795180322\n",
            "Step=24799 D_loss=0.998482518196106, G_loss=1.5505625477433205\n",
            "Step=24899 D_loss=0.8157775646448137, G_loss=1.5640452516078949\n",
            "Step=24999 D_loss=1.3238611388206483, G_loss=3.411248942017555\n",
            "Step=25099 D_loss=0.8904224163293838, G_loss=1.9247685882449153\n",
            "Step=25199 D_loss=0.8042164504528045, G_loss=1.5624262556433677\n",
            "Step=25299 D_loss=1.2243581664562226, G_loss=1.3305490571260452\n",
            "Step=25399 D_loss=1.249988169670105, G_loss=2.4184432035684584\n",
            "Step=25499 D_loss=1.070493068099022, G_loss=1.3832688796520234\n",
            "Step=25599 D_loss=0.7469986307621002, G_loss=2.1633669829368594\n",
            "Step=25699 D_loss=0.6275952821969986, G_loss=1.6065584486722946\n",
            "Step=25799 D_loss=0.9056683909893036, G_loss=1.4508136516809462\n",
            "Step=25899 D_loss=0.7356805962324142, G_loss=1.772218095064163\n",
            "Step=25999 D_loss=0.6928160136938096, G_loss=1.9588476592302322\n",
            "Step=26099 D_loss=1.1764237868785858, G_loss=1.5114396816492082\n",
            "Step=26199 D_loss=0.8536401176452637, G_loss=3.6059779614210123\n",
            "Step=26299 D_loss=1.1944866889715193, G_loss=1.8663826048374177\n",
            "Step=26399 D_loss=1.0402605813741683, G_loss=2.0571427193284033\n",
            "Step=26499 D_loss=1.0626991188526154, G_loss=1.3939412569999694\n",
            "Step=26599 D_loss=0.9097448164224624, G_loss=1.3148490080237387\n",
            "Step=26699 D_loss=0.7379088789224624, G_loss=1.4372286677360533\n",
            "Step=26799 D_loss=1.2536132991313933, G_loss=2.583183468878269\n",
            "Step=26899 D_loss=0.8902798521518708, G_loss=1.892052040100098\n",
            "Step=26999 D_loss=1.1514233112335206, G_loss=1.5414087122678757\n",
            "Step=27099 D_loss=0.9257600593566895, G_loss=1.6734035515785215\n",
            "Step=27199 D_loss=1.360231502652168, G_loss=2.2550608330965045\n",
            "Step=27299 D_loss=0.9476117664575576, G_loss=4.706349499821663\n",
            "Step=27399 D_loss=1.0474089473485946, G_loss=1.5356440597772598\n",
            "Step=27499 D_loss=1.2672601050138472, G_loss=1.5575818184018135\n",
            "Step=27599 D_loss=0.9266493922471999, G_loss=1.4374954888224603\n",
            "Step=27699 D_loss=1.0552034956216811, G_loss=2.510086472928524\n",
            "Step=27799 D_loss=0.7415593516826631, G_loss=3.03433351367712\n",
            "Step=27899 D_loss=1.1842138558626174, G_loss=2.7653344994783398\n",
            "Step=27999 D_loss=1.237012082338333, G_loss=1.7801344600319864\n",
            "Step=28099 D_loss=0.777842842936516, G_loss=1.5472258460521697\n",
            "Step=28199 D_loss=0.6392571324110032, G_loss=2.3462545159459114\n",
            "Step=28299 D_loss=1.0620420372486117, G_loss=2.1281424340605737\n",
            "Step=28399 D_loss=0.9544404357671739, G_loss=1.790768361389637\n",
            "Step=28499 D_loss=0.8191814303398133, G_loss=2.723492552340031\n",
            "Step=28599 D_loss=1.3158055418729782, G_loss=1.2606456044316292\n",
            "Step=28699 D_loss=0.9689471006393432, G_loss=1.2733310359716417\n",
            "Step=28799 D_loss=0.8511036229133606, G_loss=1.808566417992115\n",
            "Step=28899 D_loss=0.9679620587825776, G_loss=1.42045109719038\n",
            "Step=28999 D_loss=0.8152886110544204, G_loss=2.015772170126438\n",
            "Step=29099 D_loss=0.9316292500495911, G_loss=2.3045988184213635\n",
            "Step=29199 D_loss=1.4101718860864638, G_loss=1.8537337458133698\n",
            "Step=29299 D_loss=0.8712628817558289, G_loss=1.6064492982625962\n",
            "Step=29399 D_loss=1.1926153647899627, G_loss=2.189361430108547\n",
            "Step=29499 D_loss=1.0895125937461854, G_loss=1.8763393220305442\n",
            "Step=29599 D_loss=0.8063687002658844, G_loss=1.3988911977410314\n",
            "Step=29699 D_loss=0.7059730082750321, G_loss=1.577501827776432\n",
            "Step=29799 D_loss=1.4710045456886292, G_loss=4.7291960474848755\n",
            "Step=29899 D_loss=0.4828968650102615, G_loss=6.600506670176983\n",
            "Step=29999 D_loss=0.9889604955911636, G_loss=1.6700017949938772\n",
            "Step=30099 D_loss=0.7636422348022462, G_loss=1.8090412569046022\n",
            "Step=30199 D_loss=0.7483697003126145, G_loss=2.0655231043696403\n",
            "Step=30299 D_loss=0.8654060417413711, G_loss=4.2820976260304455\n",
            "Step=30399 D_loss=1.2889583671092988, G_loss=1.2444188693165779\n",
            "Step=30499 D_loss=0.7882310581207275, G_loss=1.6455678209662439\n",
            "Step=30599 D_loss=0.7684460592269897, G_loss=1.4994499263167382\n",
            "Step=30699 D_loss=0.7718495666980743, G_loss=2.092229760587215\n",
            "Step=30799 D_loss=0.3644505190849303, G_loss=4.579979390501976\n",
            "Step=30899 D_loss=1.221505515575409, G_loss=1.5175321310758592\n",
            "Step=30999 D_loss=0.752110698223114, G_loss=1.6812855967879297\n",
            "Step=31099 D_loss=1.2183324760198593, G_loss=1.5862859502434732\n",
            "Step=31199 D_loss=0.6357620453834534, G_loss=2.756304989159107\n",
            "Step=31299 D_loss=0.6160457861423493, G_loss=1.6854275754094123\n",
            "Step=31399 D_loss=1.4117329472303388, G_loss=1.5703261348605158\n",
            "Step=31499 D_loss=1.3451229339838027, G_loss=1.2208747527003287\n",
            "Step=31599 D_loss=1.2524982720613478, G_loss=2.1622397637367246\n",
            "Step=31699 D_loss=0.9129380959272384, G_loss=1.4905491280555725\n",
            "Step=31799 D_loss=1.1280921882390975, G_loss=1.0377450370788575\n",
            "Step=31899 D_loss=0.7651443082094193, G_loss=1.45393695384264\n",
            "Step=31999 D_loss=0.649376620054245, G_loss=2.0680135387182235\n",
            "Step=32099 D_loss=1.6462218803167343, G_loss=1.9545458012819292\n",
            "Step=32199 D_loss=0.8566053897142409, G_loss=1.4863638722896575\n",
            "Step=32299 D_loss=0.8313307362794876, G_loss=2.600561039745808\n",
            "Step=32399 D_loss=0.7950392627716064, G_loss=2.8846772682666777\n",
            "Step=32499 D_loss=1.100989370942116, G_loss=1.3869941031932829\n",
            "Step=32599 D_loss=0.4952925539016725, G_loss=4.1977025431394575\n",
            "Step=32699 D_loss=0.8156240022182464, G_loss=2.0796081948280336\n",
            "Step=32799 D_loss=0.6835906225442886, G_loss=1.657087065279484\n",
            "Step=32899 D_loss=1.1170137131214142, G_loss=2.585944652855396\n",
            "Step=32999 D_loss=0.9355512678623199, G_loss=3.4587010788917545\n",
            "Step=33099 D_loss=0.8993191611766815, G_loss=1.5741411739587785\n",
            "Step=33199 D_loss=0.7228780323266983, G_loss=2.071535177230835\n",
            "Step=33299 D_loss=1.079900582432747, G_loss=1.6205530610680583\n",
            "Step=33399 D_loss=0.6151816356182097, G_loss=2.3220407915115358\n",
            "Step=33499 D_loss=0.8405524736642837, G_loss=1.4009712493419646\n",
            "Step=33599 D_loss=0.90712635576725, G_loss=1.639676983356476\n",
            "Step=33699 D_loss=0.7325990235805513, G_loss=2.0344595685601234\n",
            "Step=33799 D_loss=0.9894666028022766, G_loss=1.4142007488012314\n",
            "Step=33899 D_loss=0.6783649134635925, G_loss=2.0950610551238062\n",
            "Step=33999 D_loss=0.8528120017051698, G_loss=4.848370672762393\n",
            "Step=34099 D_loss=0.893094533085823, G_loss=1.9460288178920746\n",
            "Step=34199 D_loss=0.9928212982416154, G_loss=1.534702836573124\n",
            "Step=34299 D_loss=0.5060938858985901, G_loss=2.442183491587639\n",
            "Step=34399 D_loss=1.5055777645111086, G_loss=1.2650719052553177\n",
            "Step=34499 D_loss=0.5994209730625153, G_loss=3.0109758338332178\n",
            "Step=34599 D_loss=1.0009251135587691, G_loss=1.183814600110054\n",
            "Step=34699 D_loss=0.799258286356926, G_loss=5.086350508332252\n",
            "Step=34799 D_loss=0.5393723767995835, G_loss=2.857442990541458\n",
            "Step=34899 D_loss=1.2311219263076782, G_loss=1.5340172252058983\n",
            "Step=34999 D_loss=1.1009771060943603, G_loss=1.5822720062732696\n",
            "Step=35099 D_loss=1.0356926584243775, G_loss=2.314858221113682\n",
            "Step=35199 D_loss=0.9385127186775206, G_loss=2.401063586473465\n",
            "Step=35299 D_loss=1.4944088649749754, G_loss=4.813123594224453\n",
            "Step=35399 D_loss=1.113135359287262, G_loss=2.8874186486005784\n",
            "Step=35499 D_loss=1.0538482576608659, G_loss=0.9848480254411697\n",
            "Step=35599 D_loss=0.8629033958911897, G_loss=1.7068170639872549\n",
            "Step=35699 D_loss=0.7712264776229858, G_loss=1.3939278772473336\n",
            "Step=35799 D_loss=1.788707120418549, G_loss=3.8804152235388756\n",
            "Step=35899 D_loss=0.4034803307056427, G_loss=3.6255408149957655\n",
            "Step=35999 D_loss=1.0299939572811128, G_loss=1.1869511809945108\n",
            "Step=36099 D_loss=1.1658165460824965, G_loss=2.1714118272066116\n",
            "Step=36199 D_loss=0.8254822069406509, G_loss=2.3835232454538344\n",
            "Step=36299 D_loss=1.2143727058172225, G_loss=1.6893528133630755\n",
            "Step=36399 D_loss=1.072686586380005, G_loss=2.563712770342827\n",
            "Step=36499 D_loss=0.853492090702057, G_loss=1.5815721493959427\n",
            "Step=36599 D_loss=0.6150112414360047, G_loss=2.412549351453781\n",
            "Step=36699 D_loss=0.6568736433982849, G_loss=1.8821376338601112\n",
            "Step=36799 D_loss=1.7203302204608915, G_loss=2.5153393730521203\n",
            "Step=36899 D_loss=0.9079511797428133, G_loss=2.1040790069103243\n",
            "Step=36999 D_loss=0.7447597295045852, G_loss=1.3857933753728866\n",
            "Step=37099 D_loss=0.8937727612257003, G_loss=2.875155990719795\n",
            "Step=37199 D_loss=1.2243904715776446, G_loss=1.7758887034654618\n",
            "Step=37299 D_loss=0.9986088365316391, G_loss=5.384553908407688\n",
            "Step=37399 D_loss=2.03382603764534, G_loss=2.4138383921980857\n",
            "Step=37499 D_loss=1.2139616805315017, G_loss=1.63527250289917\n",
            "Step=37599 D_loss=0.6387811249494553, G_loss=4.041519892811775\n",
            "Step=37699 D_loss=1.0638474613428115, G_loss=1.2886973822116852\n",
            "Step=37799 D_loss=0.7954481762647627, G_loss=5.043507691323757\n",
            "Step=37899 D_loss=0.8567421036958693, G_loss=2.517626519203186\n",
            "Step=37999 D_loss=1.2643409532308578, G_loss=1.32785447627306\n",
            "Step=38099 D_loss=1.0297778028249742, G_loss=1.3397107276320457\n",
            "Step=38199 D_loss=0.664942288994789, G_loss=1.6062809675931928\n",
            "Step=38299 D_loss=1.008697425723076, G_loss=1.169496761262417\n",
            "Step=38399 D_loss=0.580541832447052, G_loss=1.8428406026959419\n",
            "Step=38499 D_loss=1.057764273881912, G_loss=1.41161965996027\n",
            "Step=38599 D_loss=0.63409323990345, G_loss=2.8981090369820595\n",
            "Step=38699 D_loss=0.9359118103981018, G_loss=1.5373794919252395\n",
            "Step=38799 D_loss=0.768748934864998, G_loss=1.6820163205265999\n",
            "Step=38899 D_loss=0.806770339012146, G_loss=3.44560887157917\n",
            "Step=38999 D_loss=2.092156336903572, G_loss=1.8679233738780023\n",
            "Step=39099 D_loss=1.1639916962385177, G_loss=1.4256127867102624\n",
            "Step=39199 D_loss=0.9456009590625762, G_loss=1.3701696574687958\n",
            "Step=39299 D_loss=0.8836582750082016, G_loss=2.9287665313482285\n",
            "Step=39399 D_loss=1.0383137530088424, G_loss=1.8162994608283043\n",
            "Step=39499 D_loss=0.8971257066726684, G_loss=1.8015294498205183\n",
            "Step=39599 D_loss=0.8337503206729889, G_loss=3.1699584314227103\n",
            "Step=39699 D_loss=0.7459827435016632, G_loss=2.0156079816818235\n",
            "Step=39799 D_loss=0.8939935874938966, G_loss=1.4600869521498678\n",
            "Step=39899 D_loss=0.8013496404886246, G_loss=1.7123196050524712\n",
            "Step=39999 D_loss=0.9564992797374726, G_loss=1.8533881226181983\n",
            "Step=40099 D_loss=0.861746461391449, G_loss=1.5298211002349855\n",
            "Step=40199 D_loss=0.897169548869133, G_loss=2.5946909365057946\n",
            "Step=40299 D_loss=0.8646296721696852, G_loss=2.383036107122898\n",
            "Step=40399 D_loss=1.1603791481256485, G_loss=1.7381627056002618\n",
            "Step=40499 D_loss=1.3054043793678283, G_loss=1.8053043410181997\n",
            "Step=40599 D_loss=1.0295925194025042, G_loss=1.1128642624616623\n",
            "Step=40699 D_loss=0.8530395865440369, G_loss=1.3162088707089423\n",
            "Step=40799 D_loss=0.7359909129142761, G_loss=1.5712322545051574\n",
            "Step=40899 D_loss=0.5887621599435807, G_loss=3.7267714592814443\n",
            "Step=40999 D_loss=0.7327393174171448, G_loss=2.304783409237862\n",
            "Step=41099 D_loss=1.390041064620018, G_loss=1.2552779936790466\n",
            "Step=41199 D_loss=0.8332447773218156, G_loss=2.3229252737760544\n",
            "Step=41299 D_loss=1.243833955526352, G_loss=2.294324652850628\n",
            "Step=41399 D_loss=1.1899965554475784, G_loss=1.4739312425255775\n",
            "Step=41499 D_loss=1.153568703532219, G_loss=1.2384725511074066\n",
            "Step=41599 D_loss=1.0420050781965253, G_loss=2.8088722243905067\n",
            "Step=41699 D_loss=0.9842705708742141, G_loss=1.1814903259277343\n",
            "Step=41799 D_loss=0.7157603842020035, G_loss=1.4782540905475616\n",
            "Step=41899 D_loss=0.8684980708360672, G_loss=1.3800962394475937\n",
            "Step=41999 D_loss=0.8090336221456528, G_loss=2.575740876197815\n",
            "Step=42099 D_loss=1.345361270904541, G_loss=1.2712700870633125\n",
            "Step=42199 D_loss=0.78775723695755, G_loss=3.506705751419067\n",
            "Step=42299 D_loss=1.192924767136574, G_loss=1.3155296543240547\n",
            "Step=42399 D_loss=0.6993428474664688, G_loss=1.6971792125701906\n",
            "Step=42499 D_loss=1.0289071249961852, G_loss=2.604907481968403\n",
            "Step=42599 D_loss=1.0841707146167756, G_loss=1.6898941221833228\n",
            "Step=42699 D_loss=0.7797900426387787, G_loss=1.7642846250534057\n",
            "Step=42799 D_loss=1.2411345332860946, G_loss=2.0200211048126224\n",
            "Step=42899 D_loss=0.9072164100408555, G_loss=2.292200152873993\n",
            "Step=42999 D_loss=1.0972332495450974, G_loss=2.327510384321213\n",
            "Step=43099 D_loss=1.2998047423362733, G_loss=1.0740450415015221\n",
            "Step=43199 D_loss=0.8325772899389267, G_loss=3.196887936294079\n",
            "Step=43299 D_loss=1.0545042580366135, G_loss=1.2762517434358598\n",
            "Step=43399 D_loss=1.0809319537878037, G_loss=1.3423088043928146\n",
            "Step=43499 D_loss=0.6885880649089814, G_loss=1.7859888759255407\n",
            "Step=43599 D_loss=1.347161311507225, G_loss=1.734975130259991\n",
            "Step=43699 D_loss=1.048700643181801, G_loss=1.6068694111704827\n",
            "Step=43799 D_loss=0.754862756729126, G_loss=1.6757366940379144\n",
            "Step=43899 D_loss=0.8561198544502259, G_loss=1.859056702852249\n",
            "Step=43999 D_loss=1.9647593104839323, G_loss=4.486983199715614\n",
            "Step=44099 D_loss=1.0432172632217407, G_loss=1.6145359924435614\n",
            "Step=44199 D_loss=0.8276261001825334, G_loss=1.285897415280342\n",
            "Step=44299 D_loss=0.5928317487239837, G_loss=2.3069540125131605\n",
            "Step=44399 D_loss=0.8966613790392874, G_loss=1.4848190194368363\n",
            "Step=44499 D_loss=1.2214346799254416, G_loss=1.76602231323719\n",
            "Step=44599 D_loss=1.1249301469326018, G_loss=1.9832095280289648\n",
            "Step=44699 D_loss=0.8518777313828467, G_loss=3.112573420405388\n",
            "Step=44799 D_loss=0.6651717329025268, G_loss=1.8087820172309876\n",
            "Step=44899 D_loss=0.9688409516215324, G_loss=1.4496558871865273\n",
            "Step=44999 D_loss=0.9329997503757477, G_loss=1.7693066883087156\n",
            "Step=45099 D_loss=0.9141239562630654, G_loss=1.7535647299885748\n",
            "Step=45199 D_loss=1.1125891235470773, G_loss=1.7631227776408196\n",
            "Step=45299 D_loss=0.6691436848044394, G_loss=2.100789841115475\n",
            "Step=45399 D_loss=1.3887115630507467, G_loss=1.4078358075022697\n",
            "Step=45499 D_loss=1.0097269052267075, G_loss=1.3343560782074928\n",
            "Step=45599 D_loss=1.4324741950631141, G_loss=2.7352751791477203\n",
            "Step=45699 D_loss=0.4632279667258262, G_loss=2.7094235381484033\n",
            "Step=45799 D_loss=1.1063616660237312, G_loss=1.4564603671431542\n",
            "Step=45899 D_loss=0.7803857287764548, G_loss=3.1578551298379898\n",
            "Step=45999 D_loss=1.0213762485980988, G_loss=1.2636464527249336\n",
            "Step=46099 D_loss=1.2174485769867898, G_loss=1.5909192577004432\n",
            "Step=46199 D_loss=0.825303571820259, G_loss=2.0936202284693715\n",
            "Step=46299 D_loss=1.0980650675296784, G_loss=1.2569222870469092\n",
            "Step=46399 D_loss=0.9602635154128075, G_loss=1.9742955577373507\n",
            "Step=46499 D_loss=0.6006548541784286, G_loss=3.059295784831047\n",
            "Step=46599 D_loss=0.9725794196128844, G_loss=1.601907655596733\n",
            "Step=46699 D_loss=0.9043310210108758, G_loss=2.848196310997009\n",
            "Step=46799 D_loss=0.9754574623703958, G_loss=1.3140905505418776\n",
            "Step=46899 D_loss=0.7905722841620445, G_loss=2.2540253260731697\n",
            "Step=46999 D_loss=1.1059087291359901, G_loss=2.4731907030940055\n",
            "Step=47099 D_loss=1.019550779759884, G_loss=1.0547564354538919\n",
            "Step=47199 D_loss=0.904906224310398, G_loss=2.683472515642643\n",
            "Step=47299 D_loss=0.5861735942959785, G_loss=3.086876938045025\n",
            "Step=47399 D_loss=1.4297654110193254, G_loss=1.269879820048809\n",
            "Step=47499 D_loss=0.9209176132082938, G_loss=1.576533930301666\n",
            "Step=47599 D_loss=0.9392458659410478, G_loss=1.9419393056631091\n",
            "Step=47699 D_loss=0.9407272258400916, G_loss=1.3937384349107744\n",
            "Step=47799 D_loss=0.8965540215373039, G_loss=1.6784165516495706\n",
            "Step=47899 D_loss=1.312704822421074, G_loss=1.3902896884083749\n",
            "Step=47999 D_loss=0.9574449980258942, G_loss=1.2968155509233474\n",
            "Step=48099 D_loss=0.7044364324212076, G_loss=2.8136624825000762\n",
            "Step=48199 D_loss=0.5929881036281586, G_loss=3.6642689931392667\n",
            "Step=48299 D_loss=0.9445606058835985, G_loss=1.6826285263895986\n",
            "Step=48399 D_loss=0.7002934670448304, G_loss=1.6667355769872667\n",
            "Step=48499 D_loss=0.4383808141946792, G_loss=11.206783331930637\n",
            "Step=48599 D_loss=1.1497968903183935, G_loss=3.7367355686426165\n",
            "Step=48699 D_loss=0.6249680316448212, G_loss=1.9749037435650827\n",
            "Step=48799 D_loss=1.066809577047825, G_loss=1.4089801368117332\n",
            "Step=48899 D_loss=0.975041831433773, G_loss=1.6969284063577652\n",
            "Step=48999 D_loss=0.7370370140671729, G_loss=1.639527561068535\n",
            "Step=49099 D_loss=0.4973659619688988, G_loss=1.7967593970894815\n",
            "Step=49199 D_loss=0.9765631091594695, G_loss=2.5024530047178266\n",
            "Step=49299 D_loss=0.7585206511616708, G_loss=1.7448334115743638\n",
            "Step=49399 D_loss=1.087300733923912, G_loss=2.4602912569046023\n",
            "Step=49499 D_loss=0.6609710359573364, G_loss=1.7751926591992375\n",
            "Step=49599 D_loss=0.7820606091618538, G_loss=1.3215527889132501\n",
            "Step=49699 D_loss=0.7953218096494675, G_loss=1.6025560632348061\n",
            "Step=49799 D_loss=0.6573956316709517, G_loss=1.731367602944374\n",
            "Step=49899 D_loss=0.9574440923333167, G_loss=2.5784486186504365\n",
            "Step=49999 D_loss=0.9090494316816329, G_loss=1.4362658298015596\n",
            "Reached 50001 epochs for GAN\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9WTttgD3YHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AdaBOOST**"
      ],
      "metadata": {
        "id": "Ep4kORXs8D_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#from sklearn.externals import joblib\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "class TrainAdaBoost:\n",
        "\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "        \n",
        "        assert os.path.exists(f'{googlepath}models/checkpoint')\n",
        "        gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200, is_train=False)\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver = tf.train.Saver()\n",
        "            if os.path.exists(f'{googlepath}models/checkpoint'):\n",
        "                    \n",
        "                    with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "                        model_name = next(f).split('\"'.encode())[1]\n",
        "                    model_name_string = model_name.decode()\n",
        "                    model_name_string = model_name_string.replace(\"//\", \"/\")\n",
        "                    #filename = \"{}models/{}\".format(googlepath, model_name_string )\n",
        "                    currentStep = model_name_string.split(\"-\")[1]\n",
        "                    new_saver = tf.train.import_meta_graph('{}.meta'.format(model_name_string))\n",
        "                    new_saver.restore(sess, \"{}\".format(model_name_string))\n",
        "            files = [os.path.join(f'{googlepath}stock_data', f) for f in os.listdir(f'{googlepath}/stock_data')]\n",
        "            for file in files:\n",
        "                print(file)\n",
        "                #Read in file -- note that parse_dates will be need later\n",
        "                df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "                df = df[['open','high','low','close','volume']]\n",
        "\n",
        "                #Normilize using a of size num_historical_days\n",
        "                labels = df.close.pct_change(days).map(lambda x: int(x > pct_change/100.0))\n",
        "                df = ((df -\n",
        "                df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "                /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "                -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "                df['labels'] = labels\n",
        "\n",
        "                df = df.dropna()\n",
        "\n",
        "                #Hold out the testing data\n",
        "                test_df = df[:500]\n",
        "                df = df[500:]\n",
        "\n",
        "                data = df[['open','high','low','close','volume']].values\n",
        "                labels = df['labels'].values\n",
        "                for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.data.append(features[0])\n",
        "#                     print(features[0])\n",
        "                    self.labels.append(labels[i-1])\n",
        "                data = test_df[['open','high','low','close','volume']].values\n",
        "                labels = test_df['labels'].values\n",
        "                for i in range(num_historical_days, len(test_df), 1):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.test_data.append(features[0])\n",
        "                    self.test_labels.append(labels[i-1])\n",
        "\n",
        "    def train(self):\n",
        "          clf = AdaBoostClassifier()\n",
        "          train = self.data\n",
        "          test = self.test_data\n",
        "          \n",
        "          clf.fit(train, self.labels)\n",
        "          joblib.dump(clf, f'{googlepath}models/clf.pkl')\n",
        "\n",
        "          cm = confusion_matrix(self.test_labels, clf.predict(test))\n",
        "          print(cm)\n",
        "          Accuracy = accuracy_score(self.test_labels, clf.predict(test))\n",
        "          Precision = precision_score(self.test_labels, clf.predict(test))\n",
        "          Sensitivity_recall = recall_score(self.test_labels, clf.predict(test))\n",
        "          Specificity = recall_score(self.test_labels, clf.predict(test), pos_label=0)\n",
        "          F1_score = f1_score(self.test_labels, clf.predict(test))\n",
        "          print({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})\n",
        "          #plot_confusion_matrix(cm, ['Down', 'Up'], normalize=True, title=\"Confusion Matrix\")\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "boost_model = TrainAdaBoost(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\n",
        "boost_model.train() \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87tswgEh1VAF",
        "outputId": "08d9974d-acad-4497-dc9e-28d54997797d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/MMM.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AOS.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ABT.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ABBV.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ACN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ATVI.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AYI.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ADBE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AAP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMD.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AES.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AET.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMG.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AFL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/A.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/APD.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AKAM.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALK.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALB.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ARE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALXN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALLE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/LNT.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ALL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/GOOGL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/GOOG.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/MO.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMZN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AEE.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AAL.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AEP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AXP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AIG.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMT.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AWK.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMP.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ABC.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AME.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AMGN.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/APH.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ADI.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ANDV.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/ANSS.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AON.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/APA.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AIV.csv\n",
            "/content/drive/MyDrive/M.Tech Project /Code/23rdDec2022/stock_data/AAPL.csv\n",
            "[[19636   851]\n",
            " [ 1092  1461]]\n",
            "{'Accuracy': 0.9156684027777777, 'Precision': 0.6319204152249135, 'Sensitivity_recall': 0.5722679200940071, 'Specificity': 0.9584614633670132, 'F1_score': 0.6006166495375128}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwL7CI5I1VCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQEr1U5R1VFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhrWYmsR1VIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2w_dkth1VKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syoNKV8y1VM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqrcQgJz1VPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3b644ars1VRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "397P78h61VUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0lS-twx1VW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSmVij5m1VZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xEWjx_be1VbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}